[["index.html", "R for Data Analytics Preface", " R for Data Analytics Abhay Singh 2021-10-25 Preface Follow rforresearch This is compilation of notes for R for Data Analytics. There are three parts Part-I covers the following topics Getting Started R Data Types &amp; Data Structures A Short Introduction to R Programming Data Exploration: Preprocessing, Transformation Graphics in R Part-II covers topics in Financial Modelling Linear Regression Panel Regression Technical Analysis VaR Forecasting and GARCH models Part-III covers topics in Machine Learning Introduction to Sampling and Resampling methods Introduction to Logistic Regression and K-Nearest Neighbour Machine Learning: CART (Classification &amp; Regression Trees) Text Mining example The data files used in these notes are available here https://github.com/singh1985/rforanalytics/tree/master/data "],["1-what-is-r.html", "Topic 1 What is R?", " Topic 1 What is R? According to the official webpage: R is a language and environment for statistical computing and graphics. It is a GNU project which is similar to the S language and environment which was developed at Bell Laboratories (formerly AT&amp;T, now Lucent Technologies) by John Chambers and colleagues. R can be considered as a different implementation of S. There are some important differences, but much code written for S runs unaltered under R. http://www.r-project.org/about.html According to Wikipedia R is a free software programming language and a software environment for statistical computing and graphics. The R language is widely used among statisticians and data miners for developing statistical software and data analysis. Polls and surveys of data miners are showing Rs popularity has increased substantially in recent years. To Summarise R is the most amazing free statistical software ever! This recent video by Revolution Analytics does a great job in summarizing R ! "],["1.1-why-should-we-learn-r.html", "1.1 Why should we learn R?", " 1.1 Why should we learn R? R follows a type inference1 coding structure and provides a wide variety of statistical and graphical techniques, including; Linear and non-linear modelling Univariate &amp; Multivariate Statistics Classical statistical tests Time-series analysis/ Econometrics Simulation and Modelling Datamining-classification, clustering etc. For computationally intensive tasks, C, C++, and Fortran code can be linked and called at run time. R is easily extensible through functions and extensions, and the R community is noted for its active contributions in terms of packages. # Number of R Packages length(available.packages(repos = &quot;http://cran.us.r-project.org&quot;)[, 1]) [1] 18274 Total 18274 packages and counting Type inference refers to the automatic deduction of the type of an expression in a programming language. "],["1.2-Rinstall.html", "1.2 Installing R and RStudio on Windows", " 1.2 Installing R and RStudio on Windows The latest version of R can be download from the R homepage. R download page: http://www.cran.r-project.org/bin/windows/base/ The page also provides some instructions and FAQs on R installation. RStudio IDE ( IDE: Integrated Development Environment) is a powerful and productive user interface for R. Its free and open source, and works great on Windows, Mac, and Linux "],["1.3-rstudio-guiide.html", "1.3 RStudio GUI/IDE", " 1.3 RStudio GUI/IDE RStudio GUI is composed of 4 panes which can be rearranged according to the requirements. There are a lot of short introductions to RStudio available online so we will not go into more details. Download Rstudio from here https://rstudio.com/products/rstudio/download/#download The figure below gives the snapshot of RStudio GUI. Figure 1.1: RStudio IDE A short intro to RStudio ! RStudio Overview - 1:30 from RStudio, Inc. on Vimeo. "],["1.4-installing-packages.html", "1.4 Installing Packages", " 1.4 Installing Packages The easiest way to install packages is to do it via R console. The command install.packages(package name) installs R packages directly from internet. Other options to install various dependencies to a package can be easily specified when calling this function. A call to this function asks the user to chose a CRAN mirror at the first instance. Run the following to install Quantreg package on R. Also use the help function to get the details. # Install a package using RStudio Console install.packages(&quot;quantreg&quot;, dependencies = c(&quot;Depends&quot;, &quot;Suggests&quot;)) install.packages(c(&quot;zoo&quot;, &quot;reshape2&quot;, &quot;quantreg&quot;, &quot;e1071&quot;, &quot;foreign&quot;, &quot;psych&quot;, &quot;pastecs&quot;, &quot;ggplot2&quot;, &quot;stargazer&quot;, &quot;formatR&quot;, &quot;plm&quot;, &quot;xts&quot;, &quot;tseries&quot;, &quot;fArma&quot;), dependencies = TRUE) # to be updated "],["1.5-getting-help.html", "1.5 Getting Help", " 1.5 Getting Help As R is constantly evolving and new functions/packages are introduced every day it is good to know sources of help. The most basic help one can get is via the help() function. This function shows the help file for a function which has been created by package managers. help(&quot;function name&quot;) The following can be used to search for a function etc. #Replace the &#39;search string&#39; with the expression you want to search ??search string All the R packages (with few exceptions) have a users manual listing the functions in a package. This can be downloaded in PDF format from the R package download page2. R also provides some search tools given at http://cran.r-project.org/search.html The R Site search is helpful in searching for topics related to problem in hand. Other than these various good R related blogs are on the internet which can be really helpful. A combined upto date view of 452 contributed blogs can be found at R-bloggers3. Over all there quite a big community of R Users and help can be found for most of the topics. For example reference manual for quantreg package is at http://cran.r-project.org/web/packages/quantreg/quantreg.pdf Go to [www.r-bloggers.com]: www.r-bloggers.com "],["1.6-task-views-in-r-introduction-installation.html", "1.6 Task Views in R-Introduction &amp; Installation", " 1.6 Task Views in R-Introduction &amp; Installation Task Views in R provide packages grouped together according to a generalized task they are used for. Table below gives the name of task views available4. The following commands install the package ctv and then Finance task view. # install package task views install.packages(&quot;ctv&quot;) library(&quot;ctv&quot;) #R function library() is used to call a package # install Finance task view install.views(&quot;Finance&quot;) Figure 1.2: Task Views This list of available task views can be found at http://cran.r-project.org/web/views/ "],["1.7-r-core-packages.html", "1.7 R core packages", " 1.7 R core packages R comes with few bundled core packages which provide various data analytics/statistical capabilities to R. The base package in R has basic functions and operators which are required for analytical programming, stats is another example of core R packages. # List of R core packages row.names(installed.packages(priority = &quot;base&quot;)) [1] &quot;base&quot; &quot;compiler&quot; &quot;datasets&quot; &quot;graphics&quot; &quot;grDevices&quot; &quot;grid&quot; [7] &quot;methods&quot; &quot;parallel&quot; &quot;splines&quot; &quot;stats&quot; &quot;stats4&quot; &quot;tcltk&quot; [13] &quot;tools&quot; &quot;utils&quot; "],["1.8-example-1-hello-r.html", "1.8 Example-1 Hello R!", " 1.8 Example-1 Hello R! message(&quot;Hello R!&quot;) #use to display messages print(&quot;Hello R!&quot;) #use to display variables/messages [1] &quot;Hello R!&quot; msg = &quot;Hello R!&quot; #type inference no need to define strings! print(msg) [1] &quot;Hello R!&quot; R packages not just come with demo programs but the help files for the functions in R packages mostly have example codes for the particular function. R function example() is helpful in running the example code for a function. For running example code for in quantreg package type example(rq, package=quantreg) "],["2-r-data-types-and-data-structures.html", "Topic 2 R Data Types and Data Structures", " Topic 2 R Data Types and Data Structures When human judgement and big data intersect there are some funny things that happen. -Nate Silver "],["2.1-data-types.html", "2.1 Data Types", " 2.1 Data Types As per Rs official language definitions; in every computer language variables provide a means of accessing the data stored in memory. R does not provide direct access to the computers memory but rather provides a number of specialized data structures we will refer to as objects. These objects are referred to through symbols or variables. 2.1.1 Double Doubles are numbers like 5.0, 5.5, 10.999 etc. They may or may not include decimal places. Doubles are mostly used to represent a continuous variable like serial number, weight, age etc. x = 8.5 is.double(x) #to check if the data type is double [1] TRUE 2.1.2 Integer Integers are natural numbers. x = 9 typeof(x) [1] &quot;double&quot; # The following specifically assigns an integer to x x = as.integer(9) typeof(x) [1] &quot;integer&quot; 2.1.3 Logical A variable of data type logical has the value TRUE or FALSE. To perform calculation on logical objects in R the FALSE is replaced by a zero and TRUE is replaced by 1. x = 11 y = 10 a = x &gt; y a [1] TRUE typeof(a) [1] &quot;logical&quot; 2.1.4 Character Characters represent the string values in R. An object of type character can have alphanumeric strings. Character objects are specified by assigning a string or collection of characters between double quotes ( string) . Everything in a double quote is considered a string in R. 2.1.5 Factor -Factor is an important data type to represent categorical data. This also comes handy when dealing with Panel or Longitudinal data. Example of factors are Blood type (A , B, AB, O), Sex (Male or Female). Factor objects can be created from character object or from numeric object. -The operator c is used to create a vector of values which can be of any data type. b.type = c(&quot;A&quot;, &quot;AB&quot;, &quot;B&quot;, &quot;O&quot;) #character object # use factor function to convert to factor object b.type = factor(b.type) b.type [1] A AB B O Levels: A AB B O # to get individual elements (levels) in factor object levels(b.type) [1] &quot;A&quot; &quot;AB&quot; &quot;B&quot; &quot;O&quot; 2.1.6 Date &amp; Time -R is capable of dealing calendar dates and times. It is an important object when dealing with time series models. The function as.Date can be used to create an object of class Date. - see help(as.Date) for more details about the format of dates. date1 = &quot;31-01-2012&quot; date1 = as.Date(date1, &quot;%d-%m-%Y&quot;) date1 [1] &quot;2012-01-31&quot; data.class(date1) [1] &quot;Date&quot; # The date and time are internally interpreted as Double so the function typeof # will return the type Double typeof(date1) [1] &quot;double&quot; "],["2.2-data-structures-in-r.html", "2.2 Data Structures in R", " 2.2 Data Structures in R Every data analysis requires the data to be structured in a well defined way. These coherent ways to put together data forms some basic data structures in R. Every data set intended for analysis has to be imported in R environment as a data structure. R has the following basic data structures:  Vector  Matrix  Array  Data Frame  Lists 2.2.1 Vector Vectors are group of values having same data types. There can be numeric vectors, character vector and so on. Vectors are mostly used to represent a single variable in a data set. A vector is constructed using the function c. The same function c can be used to combine different vectors of same data type. vec1 = c(1, 2, 3, 4, 5) vec1 [1] 1 2 3 4 5 The str function can be used to view the data structure of an object 2.2.2 Matrices A matrix is a collection of data elements arranged in a two-dimensional rectangular layout. Like vectors all the elements in a matrix are of same data type. \\[\\left[\\begin{array}{cc} 1 &amp; 2\\\\ 3 &amp; 4\\\\ 5 &amp; 6 \\end{array}\\right]\\] The function \\(\\mathtt{matrix}\\) is used to create matrices in R. Note that all the elements in a matrix object are of same basic type. Lets create the matrix in the example above. m1 = matrix(c(1, 2, 3, 4, 5, 6), nrow = 3, ncol = 2, byrow = TRUE) # nrow-specify number of rows, ncol-specify number of columns, byrow-fill the # matrix in rows with the data supplied m1 #print the matrix [,1] [,2] [1,] 1 2 [2,] 3 4 [3,] 5 6 A vector can be converted to matrix using \\(\\mathtt{dim}\\) function, e.g: m2 = c(1, 2, 3, 4, 5, 6) dim(m2) = c(3, 2) #the matrix will be filled by columns m2 [,1] [,2] [1,] 1 4 [2,] 2 5 [3,] 3 6 # use dim to get the dimension (#rows and #columns) of a matrix dim(m1) [1] 3 2 ** Matrix Manipulations ** For calculations on matrices; all the mathematical functions available for vectors are applicable on a matrix. All operations are applied on each element in a matrix, e.g. m3 = m1 * 2 # all elements will be multiplied by 2 individually m3 [,1] [,2] [1,] 2 4 [2,] 6 8 [3,] 10 12 A matrix can be multiplied with a vector as long as the length of the vector is a multiple of length of the matrix. Try different combinations of matrix and vector arithmetic to see the results and errors. Mathematical matrix operations are also available for matrices in R. For instance \\(\\mathtt{\\%*\\%}\\) is used for matrix multiplication, the matrices must agree dimensionally for matrix multiplication. Note the use of \\(\\mathtt{:}\\) operator to create a sequence. dim(m1) # 3 rows and 2 columns [1] 3 2 # create another matrix with 2 rows and 3 columns m3 = matrix(c(1:6), ncol = 3) m1 %*% m3 [,1] [,2] [,3] [1,] 5 11 17 [2,] 11 25 39 [3,] 17 39 61 R facilitates various matrix specific operations. Table 1 gives most of the available functions and operators. Use \\(\\mathtt{help()}\\) or \\(\\mathtt{?}\\) followed by function name to get more details about the operators and functions. Table : Functions and operators for matrices Operator or Function Description X * Y Element-wise multiplication X %*% Y Matrix multiplication Y %o% X Outer product. XB crossprod(X,Y) XY crossprod(X) XX t(X) Transpose diag(x) Creates diagonal matrix with elements of x in the principal diagonal diag(X) Returns a vector containing the elements of the principal diagonal diag(k) If k is a scalar, this creates a k x k identity matrix. Go figure. solve(X, b) Returns vector x in the equation b = Xx (i.e., X-1b) solve(X) Inverse of X where X is a square matrix. y=eigen(X) y$val are the eigenvalues of X y$vec are the eigenvectors of X y=svd(X) Singular value decomposition of X. R = chol(X) Choleski factorization of X. Returns the upper triangular factor, such that RR = X. y = qr(X) QR decomposition of X. cbind(X,Y,) Combine matrices(vectors) horizontally. Returns a matrix. rbind(X,Y,) Combine matrices(vectors) vertically. Returns a matrix. rowMeans(X) Returns vector of row means. rowSums(X) Returns vector of row sums. colMeans(X) Returns vector of column means. colSums(X) Returns vector of column means. 2.2.3 Arrays Arrays are the generalisation of vectors and matrices. A vector in R is a one dimensional array and a matrix a two dimensional array. An array is a multiply subscripted collection of data entries of the same data type. Arrays can be constructed using the function \\(\\mathtt{array}\\), for example5 z = c(1:24) #vector of length 24 # constructing a 3 by 4 by 2 array a1 = array(z, dim = c(3, 4, 2)) a1 , , 1 [,1] [,2] [,3] [,4] [1,] 1 4 7 10 [2,] 2 5 8 11 [3,] 3 6 9 12 , , 2 [,1] [,2] [,3] [,4] [1,] 13 16 19 22 [2,] 14 17 20 23 [3,] 15 18 21 24 Individual elements of an array are accessed by referring them by their index. This is done by giving the name of the array followed by the subscript (index) in this square bracket separated by commas. We try to access the element [1,3,1] of array a1 in the following example # element in the row 1 and column 3 in the first subset a1[1, 3, 1] [1] 7 2.2.4 Data Frames Data frame forms the most convenient data structures in R to represent tabular data. In quantitative research data is often in the form of data tables. These data tables have multiple rows and can have multiple columns with each column representing a different variable (quantity). A data frame in R is the most natural way to represent these data sets as it can have different data type in the data frame object. Most statistical routines in R require a data frame as input. The following example uses an important function \\(\\mathtt{str}\\) on Rs inbuilt data frame swiss. \\(\\mathtt{str}\\) function is used to see the internal structure of an object in R. options(str = list(vec.len = 2)) # swiss dataframe has standardized fertility measure and socio-economic # indicators for each of 47 French-speaking provinces of Switzerland at about # 1888. data(swiss) str(swiss) &#39;data.frame&#39;: 47 obs. of 6 variables: $ Fertility : num 80.2 83.1 92.5 85.8 76.9 ... $ Agriculture : num 17 45.1 39.7 36.5 43.5 ... $ Examination : int 15 6 5 12 17 ... $ Education : int 12 9 5 7 15 ... $ Catholic : num 9.96 84.84 ... $ Infant.Mortality: num 22.2 22.2 20.2 20.3 20.6 ... Data frames have two attributes namely; \\(\\mathtt{names}\\) and \\(\\mathtt{row.names}\\), these two contains the column names and row names respectively. The data in the named column can be accessed by the \\(\\mathtt{\\$}\\) operator. # using names and row.names names(swiss) #name of the columns (can also use colnames) [1] &quot;Fertility&quot; &quot;Agriculture&quot; &quot;Examination&quot; &quot;Education&quot; [5] &quot;Catholic&quot; &quot;Infant.Mortality&quot; colnames(swiss) [1] &quot;Fertility&quot; &quot;Agriculture&quot; &quot;Examination&quot; &quot;Education&quot; [5] &quot;Catholic&quot; &quot;Infant.Mortality&quot; row.names(swiss) #name of the rows [1] &quot;Courtelary&quot; &quot;Delemont&quot; &quot;Franches-Mnt&quot; &quot;Moutier&quot; &quot;Neuveville&quot; [6] &quot;Porrentruy&quot; &quot;Broye&quot; &quot;Glane&quot; &quot;Gruyere&quot; &quot;Sarine&quot; [11] &quot;Veveyse&quot; &quot;Aigle&quot; &quot;Aubonne&quot; &quot;Avenches&quot; &quot;Cossonay&quot; [16] &quot;Echallens&quot; &quot;Grandson&quot; &quot;Lausanne&quot; &quot;La Vallee&quot; &quot;Lavaux&quot; [21] &quot;Morges&quot; &quot;Moudon&quot; &quot;Nyone&quot; &quot;Orbe&quot; &quot;Oron&quot; [26] &quot;Payerne&quot; &quot;Paysd&#39;enhaut&quot; &quot;Rolle&quot; &quot;Vevey&quot; &quot;Yverdon&quot; [31] &quot;Conthey&quot; &quot;Entremont&quot; &quot;Herens&quot; &quot;Martigwy&quot; &quot;Monthey&quot; [36] &quot;St Maurice&quot; &quot;Sierre&quot; &quot;Sion&quot; &quot;Boudry&quot; &quot;La Chauxdfnd&quot; [41] &quot;Le Locle&quot; &quot;Neuchatel&quot; &quot;Val de Ruz&quot; &quot;ValdeTravers&quot; &quot;V. De Geneve&quot; [46] &quot;Rive Droite&quot; &quot;Rive Gauche&quot; swiss$Fertility #returns the vector of data in the column Fertility [1] 80.2 83.1 92.5 85.8 76.9 76.1 83.8 92.4 82.4 82.9 87.1 64.1 66.9 68.9 61.7 [16] 68.3 71.7 55.7 54.3 65.1 65.5 65.0 56.6 57.4 72.5 74.2 72.0 60.5 58.3 65.4 [31] 75.5 69.3 77.3 70.5 79.4 65.0 92.2 79.3 70.4 65.7 72.7 64.4 77.6 67.6 35.0 [46] 44.7 42.8 Data frames are constructed using the function \\(\\mathtt{data.frame}\\). For example following creates a data frame of a character and numeric vector. num1 = seq(1:5) ch1 = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;) df1 = data.frame(ch1, num1) df1 ch1 num1 1 A 1 2 B 2 3 C 3 4 D 4 5 E 5 2.2.5 Lists A list is like generic vector containing other objects. Lists can have numerous elements any type and structure they can also be of different lengths A list can contain another list and therefore it can be used to construct arbitrary data structures. A list can be constructed using the \\(\\mathtt{list}\\) function, for example e1 = c(2, 3, 5) #element-1 e2 = c(&quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;, &quot;dd&quot;, &quot;ee&quot;) #element-2 e3 = c(TRUE, FALSE, TRUE, FALSE, FALSE) #element-3 e4 = df1 #element-4 (previously constructed data frame) lst1 = list(e1, e2, e3, e4) # lst contains copies of e1,e2,e3,e4 str(lst1) #show the structure of lst1 List of 4 $ : num [1:3] 2 3 5 $ : chr [1:5] &quot;aa&quot; &quot;bb&quot; ... $ : logi [1:5] TRUE FALSE TRUE ... $ :&#39;data.frame&#39;: 5 obs. of 2 variables: ..$ ch1 : chr [1:5] &quot;A&quot; &quot;B&quot; ... ..$ num1: int [1:5] 1 2 3 4 5 Components are always numbered and may always be referred to as such. Figure 2.1: Lists Thus if lst1 is the name of a list with four components, these may be individually referred to as lst1[[1]], lst1[[2]], lst1[[3]] and lst1[[4]]. Note: When a single square bracket is used the component of a list is returned as a list while the double square bracket returns the component itself # first element of lst1 lst1[[1]] [1] 2 3 5 lst1[1] [[1]] [1] 2 3 5 The elements in a list can also be named using the function and these elements can be referred individually via there names. names(lst1) = c(&quot;e1&quot;, &quot;e2&quot;, &quot;e3&quot;, &quot;e4&quot;) names(lst1) #name of the elements [1] &quot;e1&quot; &quot;e2&quot; &quot;e3&quot; &quot;e4&quot; lst1$e1 #using $operator to refer the element [1] 2 3 5 This section provided an overview of various data types and data structures in R. The next section will discuss how to deal with external data souces with flat data. Function \\(\\mathtt{dim}\\) can also be used to define an array by assigning dimensions to a vector. "],["2.3-data-importexport-in-r.html", "2.3 Data Import/Export in R", " 2.3 Data Import/Export in R 2.3.1 Reading Data from a Text File The easiest way to import data into Rs statistical system is to do in a tabular format saved in a text/ file. To import tabular data from a text file, R provides the function \\(\\mathtt{read.table()}\\). \\(\\mathtt{read.table()}\\) is the most convenient function to import tabular data from text files and can be easily used for data files of small or moderate size having data in a rectangular format. The arguments which can be passed to \\(\\mathtt{read.table()}\\) are given below. args(read.table) function (file, header = FALSE, sep = &quot;&quot;, quote = &quot;\\&quot;&#39;&quot;, dec = &quot;.&quot;, numerals = c(&quot;allow.loss&quot;, &quot;warn.loss&quot;, &quot;no.loss&quot;), row.names, col.names, as.is = !stringsAsFactors, na.strings = &quot;NA&quot;, colClasses = NA, nrows = -1, skip = 0, check.names = TRUE, fill = !blank.lines.skip, strip.white = FALSE, blank.lines.skip = TRUE, comment.char = &quot;#&quot;, allowEscapes = FALSE, flush = FALSE, stringsAsFactors = FALSE, fileEncoding = &quot;&quot;, encoding = &quot;unknown&quot;, text, skipNul = FALSE) NULL Some of the important arguments for the function \\(\\mathtt{read.table}\\) are discussed below, for the rest see the help file using \\(\\mathtt{help(read.table)}\\). Argument Description file The name of the tabular (text) file to import along with the full path header A logical argument to specify if the names of the variables are available in the first row sep Character to specify the seperator type, default   takes any white space as a separator quote To specify if the character vectors in the data are in quotes, this shuold specify the type of quotes as.is To specify if the character vectors should be converted to factors. The default behaviour is to read characters as characters and not factors strip.white A logical value to specify if the extra leading and trailing white spaces have to be removed from the character fiels. This is used when sep !=\". fill Logical value to specify if the blank fields in a row should be filled. The example below imports a tab delimited text file. Note the use of  in the sep argument for tab delimited data . The header argument is also TRUE here as our dataset has variable names in the first row. Note that in the example below, the working directory for the RStudio session has already been set to the destination files directory (data folder). If the working directory is different from the location of the data file then either the working directory should be changed using \\(\\mathtt{setwd}\\) or RStudios GUI or full path for the files location should be provided with the file name. data_readtable = read.table(&quot;data/demo_data.txt&quot;, sep = &quot;\\t&quot;, header = TRUE) head(data_readtable) Date AAPL MSFT 1 2/01/1998 4.06 16.39 2 5/01/1998 3.97 16.30 3 6/01/1998 4.73 16.39 4 7/01/1998 4.38 16.20 5 8/01/1998 4.55 16.31 6 9/01/1998 4.55 15.88 This data can be now saved into .Rdata format after importing from a text file using \\(\\mathtt{save}\\) or can be written to another text file using \\(\\mathtt{write.table}\\) as shown below: # saving data as an object in .Rdata format save(data_readtable, file = &quot;data/data1.Rdata&quot;) # saving data into another text file write.table(data_readtable, file = &quot;data/data1.txt&quot;) Another convenient way to store the data is to store in RDS format. saveRDS(data_readtable, file = &quot;data/data1_rds.Rds&quot;) These data files can then be loaded using load and readRDS functions load(&quot;data/data1.Rdata&quot;) head(data_readtable) Date AAPL MSFT 1 2/01/1998 4.06 16.39 2 5/01/1998 3.97 16.30 3 6/01/1998 4.73 16.39 4 7/01/1998 4.38 16.20 5 8/01/1998 4.55 16.31 6 9/01/1998 4.55 15.88 Rds format can be loaded into a different object data_readtable2 = readRDS(&quot;data/data1_rds.Rds&quot;) str(data_readtable2) &#39;data.frame&#39;: 3936 obs. of 3 variables: $ Date: chr &quot;2/01/1998&quot; &quot;5/01/1998&quot; ... $ AAPL: num 4.06 3.97 4.73 4.38 4.55 ... $ MSFT: num 16.4 16.3 ... 2.3.2 Reading Data from CSV files Reading data from a CSV file is made easy by the \\(\\mathtt{read.csv}\\) function. \\(\\mathtt{read.csv}\\) function is an extension of \\(\\mathtt{read.table}\\). It facilitates direct import of data from CSV files. \\(\\mathtt{read.csv}\\) function takes the following arguments The following example imports a CSV file with the same data as previously imported from a text file. # Check the working directory before importing else provide full path data_readcsv = read.csv(&quot;data/demo_data.csv&quot;) head(data_readcsv) Date AAPL MSFT 1 2/01/1998 4.06 16.39 2 5/01/1998 3.97 16.30 3 6/01/1998 4.73 16.39 4 7/01/1998 4.38 16.20 5 8/01/1998 4.55 16.31 6 9/01/1998 4.55 15.88 Similar to \\(\\mathtt{write.table}\\) data can also be written to an external csv file using \\(\\mathtt{write.csv}\\). The following example uses an inbuilt data set in R and exports it to a CSV. Notice the use of row.names=FALSE to avoid creating one more column in the CSV file with row numbers data(iris) #R inbuilt dataset head(iris) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 1 5.1 3.5 1.4 0.2 setosa 2 4.9 3.0 1.4 0.2 setosa 3 4.7 3.2 1.3 0.2 setosa 4 4.6 3.1 1.5 0.2 setosa 5 5.0 3.6 1.4 0.2 setosa 6 5.4 3.9 1.7 0.4 setosa write.csv(iris, &quot;data/data_iris.csv&quot;, row.names = FALSE) 2.3.3 Reading from Excel Files R does provide methods to import data from excel file with the help of external packages. There are methods provided by packages like readxl, \\(\\mathtt{gdata}\\), \\(\\mathtt{XLConnet}\\), \\(\\mathtt{xlsx}\\). 2.3.4 Reading from Data Files from other Statistical Systems When migrating from software like SPSS, Stata, Matlab users might want to use there old datasets generated from these systems in R. This requires methods for importing these datasets into R. There are packages like \\(\\mathtt{haven}\\), \\(\\mathtt{foreign}\\) and \\(\\mathtt{R.matlab}\\) which provide these functionality. 2.3.5 Importing Data using RStudio To import data click on Import Dataset \\(\\rightarrow\\) From Excel.. \\(\\rightarrow\\) for the file to import. Remember the file should be in a tabular format, a text file or a csv are the best options. On clicking Import the data will be imported in a Data Frame and will be made visible by RStudio. This will also generate basic data import command used for importing and viewing the file in the RStudio console as shown in the figure below. Note that the path in the command as shown in the console has been scrambled as it will be different for every computer Figure 2.2: Import Dataset wizard in RStudio "],["3-r-programming---short-introduction.html", "Topic 3 R Programming - Short Introduction", " Topic 3 R Programming - Short Introduction you might not think that programmers are artists, but programming is an extremely creative profession. Its logic-based creativity. -John Romero Here we will cover the basic structures of R programming, including control flow (if else) and loops (iteration routines) followed by writing our first function. We will confine our discussion to the beginners level. "],["3.1-programming-control-flow.html", "3.1 Programming Control Flow", " 3.1 Programming Control Flow Control flow (or flow control) is a well defined sequence of conditional statements, loops and statements which directs the R script (or code in generalised sense) to execute one thing or the other based on the conditions written in the program. 3.1.1 if-else Conditional Statements We use if-else conditional statements when we want the R program to branch out in different directions based on a logical condition. The following example compares the mean of two stocks and assigns a variable with the greater mean. data_stocks = read.csv(&quot;data/us_stocks.csv&quot;) # remove NAs from the data data_stocks = na.omit(data_stocks) m_msft = mean(data_stocks$MSFT) m_aapl = mean(data_stocks$AAPL) if (m_msft &gt; m_aapl) { g_mean = m_msft message(&quot;Msft mean is higher&quot;) } else { g_mean = m_aapl message(&quot;Aapl mean is higher&quot;) } g_mean #print greater mean [1] 207.7967 The if-else also works as a function call, the if-else call in the example above can be reduced to one line as follows. Note that the curly brackets in case of just one statement are optional. They are required in case of a block operation. Its easy to just use them to avoid confusion. R also has a function \\(\\mathtt{ifelse}\\) which does the same operation as in example above. See \\(\\mathtt{help(ifelse)}\\) for more details # arguments to ifelse args(ifelse) function (test, yes, no) NULL g_mean = ifelse(m_msft &gt; m_aapl, m_msft, m_aapl) g_mean [1] 207.7967 3.1.2 Loops Loops are the common feature in almost all the programming languages. R provides three basic loops using \\(\\mathtt{for}\\), \\(\\mathtt{while}\\) and \\(\\mathtt{repeat}\\). # construct the loop j = 0 for (i in 1:15) { j = j + i #add i to j print(j) #print the sequential sum } [1] 1 [1] 3 [1] 6 [1] 10 [1] 15 [1] 21 [1] 28 [1] 36 [1] 45 [1] 55 [1] 66 [1] 78 [1] 91 [1] 105 [1] 120 3.1.3 \\(\\mathtt{while}\\) loop \\(\\mathtt{while}\\) loop evaluates an expression or a function while a condition is TRUE. Lets repeat the above example using \\(\\mathtt{while}\\) loop # intialise j and i j = 0 i = 1 # one can also use i&lt;=15 while (i &lt; 16) { j = j + i i = i + 1 print(j) } [1] 1 [1] 3 [1] 6 [1] 10 [1] 15 [1] 21 [1] 28 [1] 36 [1] 45 [1] 55 [1] 66 [1] 78 [1] 91 [1] 105 [1] 120 3.1.4 \\(\\mathtt{repeat}\\) loop \\(\\mathtt{repeat}\\) repeats the same expression till it is broken due to a condition. # intialize j = 0 i = 1 repeat { j = j + i i = i + 1 print(j) if (i &gt; 15) break } [1] 1 [1] 3 [1] 6 [1] 10 [1] 15 [1] 21 [1] 28 [1] 36 [1] 45 [1] 55 [1] 66 [1] 78 [1] 91 [1] 105 [1] 120 Together all these three loops can be used for iterative operations. The loops come handy when you have to iterate medium to large data by row or column.6 Using simple loops can be resource intensive for large datasets or operations. There are other approaches such as using iterative functions like \\(\\mathtt{lapply, sapply}\\) etc or parallel computing methods to get better results in such cases. "],["3.2-functions-in-r.html", "3.2 Functions in R", " 3.2 Functions in R In all research fields there are few statistical (or otherwise) calculations which are used frequently by the users, for example calculation of returns in finance research. R provides the facility of creating specific functions to evaluate a set of arguments and return an output value which are stored as R objects. The functions in R are created by the keyword \\(\\mathtt{function}\\) which takes the following syntax. \\[\\mathtt{function(arguments)}body\\] The arguments are the values/defaults/variables which are used in the body of the function to evaluate an expression. The body of the function is enclosed in curly braces. The function is assigned to a named object and called by passing arguments to the object. The following example illustrates by creating a function to calculate mean for all the columns in the data set \\(\\mathtt{data\\_stocks}\\) # the following function takes 2 arguments, x a data frame, dates to indicate # if there are dates in the first column cal_mean = function(x, dates = TRUE) { num_cols = ncol(x) #calculate the number of columns # num_cols=ifelse(dates==TRUE,num_cals-1,num_cals) lets use a list and a # loop to refresh our concepts m_stocks = list() #creating an empty list # use for loop assign the starting value based on the dates column,we skip # dates column if they are present (dates are basically row names to more # generalised version will be to check for row names) l = ifelse(dates == TRUE, 2, 1) j = 1 #starting point in the list m_stocks for (i in l:num_cols) { m_stocks[[j]] = mean(x[, i]) j = j + 1 } names(m_stocks) = colnames(x[, l:num_cols]) return(m_stocks) } # lets call the function cal_mean (output not shown) cal_mean(data_stocks, TRUE) $MSFT [1] 26.91177 $IBM [1] 122.3303 $AAPL [1] 207.7967 $MCD [1] 58.95141 $PG [1] 61.32512 $GOOG [1] 469.9453 # lets call the function with no dates column cal_mean(data_stocks[, 2:ncol(data_stocks)], FALSE) $MSFT [1] 26.91177 $IBM [1] 122.3303 $AAPL [1] 207.7967 $MCD [1] 58.95141 $PG [1] 61.32512 $GOOG [1] 469.9453 "],["4-data-exploration.html", "Topic 4 Data Exploration", " Topic 4 Data Exploration The good news is that these descriptive statistics give us a manageable and meaningful summary of the underlying phenomenon. Thats what this chapter is about. The bad news is that any simplification invites abuse. Descriptive statistics can be like online dating profiles: technically accurate and yet pretty darn misleading. -Charles Wheelan "],["4.1-data-preprocessing.html", "4.1 Data Preprocessing", " 4.1 Data Preprocessing -We will now discuss some methods for data manipulation to clean a dataset, combine various datasets or extract a variable from a data frame before we jump into some programming basics. 4.1.1 Extracting Data -Data frames are the most used data structures in R as they offer more flexibility in the way they can handle data. Lets see some methods to extract data from a data frame. We will use the example dataset called \\(\\mathtt{us\\_stocks.csv}\\). Lets import it using \\(\\mathtt{read.csv}\\) data_stocks = read.csv(file = &quot;data/us_stocks.csv&quot;, header = TRUE) head(data_stocks) Date MSFT IBM AAPL MCD PG GOOG 1 2/01/2002 33.52 121.50 11.65 26.49 40.00 NA 2 3/01/2002 34.62 123.66 11.79 26.79 39.62 NA 3 4/01/2002 34.45 125.60 11.84 26.99 39.22 NA 4 7/01/2002 34.28 124.05 11.45 27.20 38.78 NA 5 8/01/2002 34.69 124.70 11.30 27.36 38.88 NA 6 9/01/2002 34.36 124.49 10.82 26.88 38.60 NA The function \\(\\mathtt{names}\\) or \\(\\mathtt{colnames}\\) are used to access the names of the columns (or variables) in the data set as shows below. The function \\(\\mathtt{row.names}\\) can be used to access row names (if any) from a dataset names(data_stocks) [1] &quot;Date&quot; &quot;MSFT&quot; &quot;IBM&quot; &quot;AAPL&quot; &quot;MCD&quot; &quot;PG&quot; &quot;GOOG&quot; colnames(data_stocks) [1] &quot;Date&quot; &quot;MSFT&quot; &quot;IBM&quot; &quot;AAPL&quot; &quot;MCD&quot; &quot;PG&quot; &quot;GOOG&quot; A specific data variable can be accessed using its name or index (column number) in the data frame. To select any column use \\(\\mathtt{\\$}\\) symbol followed by the column name or its name in square brackets as shown in the example below msft_prices1 = data_stocks$MSFT #the data is returned as a vector head(msft_prices1) [1] 33.52 34.62 34.45 34.28 34.69 34.36 msft_prices2 = data_stocks[[&quot;MSFT&quot;]] #the data is returned as a vector head(msft_prices2) [1] 33.52 34.62 34.45 34.28 34.69 34.36 # the following returns data as a data frame msft_prices3 = data_stocks[&quot;MSFT&quot;] #can also be used to access multiple columns head(msft_prices3) MSFT 1 33.52 2 34.62 3 34.45 4 34.28 5 34.69 6 34.36 These data columns can also be accessed like a matrix, using a matrix index. This method can return a complete row, a complete column or just an element from the dataset. # MSFT is in the second column and leaving the row index blank returns all the # rows for the particular column msft_prices4 = data_stocks[, 2] head(msft_prices4) [1] 33.52 34.62 34.45 34.28 34.69 34.36 # all the elements in row 4 data_stocks[4, ] Date MSFT IBM AAPL MCD PG GOOG 4 7/01/2002 34.28 124.05 11.45 27.2 38.78 NA 4.1.2 Combining Data Frames It may be required to combine two data frames during a data processing. This can be done by stacking them row by row or combining them by columns using \\(\\mathtt{rbind}\\) and \\(\\mathtt{cbind}\\) respectively. When using \\(\\mathtt{cbind}\\) the number of rows in the columns combined must be of equal length likewise in \\(\\mathtt{rbind}\\) the number of columns of the datasets combined should be equal. Lets see an example # First create a vector having the returns for msft msft_ret = 100 * diff(log(data_stocks$MSFT)) # combine the vector with the data data_stocks_r = cbind(data_stocks, MSFT_RET = msft_ret) #this will generate an error message Error in data.frame(..., check.names = FALSE): arguments imply differing number of rows: 2784, 2783 # different length length(msft_ret) [1] 2783 length(data_stocks$MSFT) [1] 2784 # add one more value to vector msft_ret msft_ret = c(0, msft_ret) # check the length length(msft_ret) [1] 2784 # lets combine now (it should work) data_stocks_r = cbind(data_stocks, MSFT_RET = msft_ret) head(data_stocks_r) #shows one more column added to the data Date MSFT IBM AAPL MCD PG GOOG MSFT_RET 1 2/01/2002 33.52 121.50 11.65 26.49 40.00 NA 0.0000000 2 3/01/2002 34.62 123.66 11.79 26.79 39.62 NA 3.2289274 3 4/01/2002 34.45 125.60 11.84 26.99 39.22 NA -0.4922552 4 7/01/2002 34.28 124.05 11.45 27.20 38.78 NA -0.4946904 5 8/01/2002 34.69 124.70 11.30 27.36 38.88 NA 1.1889367 6 9/01/2002 34.36 124.49 10.82 26.88 38.60 NA -0.9558364 The following example adds a row to the data frame. # create two dataframes from data_stocks data_r1 = data_stocks[1:10, ] #first 10 rows data_r2 = data_stocks[2775:2784, ] #last 10 rows data_stocks_rbind = rbind(data_r1, data_r2) print(data_stocks_rbind) Date MSFT IBM AAPL MCD PG GOOG 1 2/01/2002 33.52 121.50 11.65 26.49 40.00 NA 2 3/01/2002 34.62 123.66 11.79 26.79 39.62 NA 3 4/01/2002 34.45 125.60 11.84 26.99 39.22 NA 4 7/01/2002 34.28 124.05 11.45 27.20 38.78 NA 5 8/01/2002 34.69 124.70 11.30 27.36 38.88 NA 6 9/01/2002 34.36 124.49 10.82 26.88 38.60 NA 7 10/01/2002 34.64 122.14 10.62 26.81 38.46 NA 8 11/01/2002 34.30 120.31 10.52 26.34 38.60 NA 9 14/01/2002 34.24 118.05 10.58 26.02 39.35 NA 10 15/01/2002 34.78 118.85 10.85 26.20 39.82 NA 2775 17/12/2012 27.10 193.62 518.83 89.91 69.93 720.78 2776 18/12/2012 27.56 195.69 533.90 90.52 69.97 721.07 2777 19/12/2012 27.31 195.08 526.31 89.71 69.34 720.11 2778 20/12/2012 27.68 194.77 521.73 90.04 69.82 722.36 2779 21/12/2012 27.45 193.42 519.33 90.18 68.72 715.63 2780 24/12/2012 27.06 192.40 520.17 89.29 68.52 709.50 2781 26/12/2012 26.86 191.95 513.00 88.74 68.00 708.87 2782 27/12/2012 26.96 192.71 515.06 88.72 67.97 706.29 2783 28/12/2012 26.55 189.83 509.59 87.58 67.15 700.01 2784 31/12/2012 26.71 191.55 532.17 88.21 67.89 707.38 4.1.3 Sub setting and Logical Data Selection Suppose we want to extract data with particular characteristics like values ranges etc. This can be accomplished using logical statements in bracket notations. The following example illustrates. See \\(\\mathtt{help(&quot;&gt;&quot;)}\\) to see more comparison operators. # select all rows with Apple prices above 100 data_aaplgr100 = data_stocks[data_stocks$AAPL &gt; 100, ] head(data_aaplgr100) Date MSFT IBM AAPL MCD PG GOOG 1342 2/05/2007 30.61 102.22 100.39 50.02 62.37 465.78 1343 3/05/2007 30.97 102.80 100.40 49.91 62.00 473.23 1344 4/05/2007 30.56 102.96 100.81 49.92 62.41 471.12 1345 7/05/2007 30.71 103.16 103.92 49.50 62.18 467.27 1346 8/05/2007 30.75 103.29 105.06 49.32 61.75 466.81 1347 9/05/2007 30.78 104.38 106.88 49.84 62.01 469.25 min(data_aaplgr100$AAPL) #check if the prices are above 100 [1] NA # this give NA as the minimum which indicates that data frame has NA lets # remove NAs from data_aaplgr100 using na.omit function data_aaplgr100 = na.omit(data_aaplgr100) # now check the minimum again min(data_aaplgr100$AAPL) [1] 100.06 The \\(\\mathtt{na.omit}\\) function used in the example above can be used to remove all the empty values in the dataset. head(data_stocks) #notice NAs in GOOG Date MSFT IBM AAPL MCD PG GOOG 1 2/01/2002 33.52 121.50 11.65 26.49 40.00 NA 2 3/01/2002 34.62 123.66 11.79 26.79 39.62 NA 3 4/01/2002 34.45 125.60 11.84 26.99 39.22 NA 4 7/01/2002 34.28 124.05 11.45 27.20 38.78 NA 5 8/01/2002 34.69 124.70 11.30 27.36 38.88 NA 6 9/01/2002 34.36 124.49 10.82 26.88 38.60 NA data_stocks_googlena = data_stocks[!is.na(data_stocks$GOOG), ] head(data_stocks_googlena) #after removing NAs Date MSFT IBM AAPL MCD PG GOOG 663 19/08/2004 27.12 84.89 15.36 26.60 54.48 100.34 664 20/08/2004 27.20 85.25 15.40 27.07 54.85 108.31 665 23/08/2004 27.24 84.65 15.54 26.64 54.75 109.40 666 24/08/2004 27.24 84.71 15.98 26.87 54.95 104.87 667 25/08/2004 27.55 85.07 16.52 26.95 55.30 106.00 668 26/08/2004 27.44 84.69 17.33 27.10 55.70 107.91 # the above can still leave NAs in other columns use na.omit to remove all the # blank data data_stocks_naomit = na.omit(data_stocks) There can be a requirement in data pre processing where one might have to select data in a range. The following example selects data where MSFT prices lie between 20 and 30. \\(\\mathtt{\\&amp;}\\) is a Logic operator in R see help(&amp;) to see more details and other Logic operators. data_msft = data_stocks_naomit[data_stocks_naomit$MSFT &lt;= 30 &amp; data_stocks_naomit$MSFT &gt; 20, ] min(data_msft$MSFT) #check [1] 20.06 These selections can also be performed using the function \\(\\mathtt{subset}\\). The following example uses \\(\\mathtt{subset}\\) function to select rows with AAPL&gt;100. The arguments to the function are also shows in the example args(subset.data.frame) function (x, subset, select, drop = FALSE, ...) NULL aaplgr100 = subset(data_stocks_naomit, AAPL &gt; 100) head(aaplgr100) Date MSFT IBM AAPL MCD PG GOOG 1342 2/05/2007 30.61 102.22 100.39 50.02 62.37 465.78 1343 3/05/2007 30.97 102.80 100.40 49.91 62.00 473.23 1344 4/05/2007 30.56 102.96 100.81 49.92 62.41 471.12 1345 7/05/2007 30.71 103.16 103.92 49.50 62.18 467.27 1346 8/05/2007 30.75 103.29 105.06 49.32 61.75 466.81 1347 9/05/2007 30.78 104.38 106.88 49.84 62.01 469.25 min(aaplgr100$AAPL) [1] 100.06 "],["4.2-data-transformation-from-wide-to-long-or-vice-versa.html", "4.2 Data Transformation from Wide to Long (or vice versa)", " 4.2 Data Transformation from Wide to Long (or vice versa) Sometimes its required to transform wide format data to long, which is often required to work with ggplot2 package (discussed in the graphics section) R package tidyr provides two functions pivot_longer() and pivot_wider() to transform the data into long or wide format. Lets convert the stocks data to the long format library(tidyr) FinData_long = pivot_longer(data = data_stocks, cols = -Date, names_to = &quot;Stock&quot;, values_to = &quot;Price&quot;) head(FinData_long) # A tibble: 6 x 3 Date Stock Price &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 2/01/2002 MSFT 33.5 2 2/01/2002 IBM 122. 3 2/01/2002 AAPL 11.6 4 2/01/2002 MCD 26.5 5 2/01/2002 PG 40 6 2/01/2002 GOOG NA A reverse operation can be conducted using pivot_wider() FinData_wide = pivot_wider(FinData_long, names_from = Stock, values_from = Price) head(FinData_wide) # A tibble: 6 x 7 Date MSFT IBM AAPL MCD PG GOOG &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 2/01/2002 33.5 122. 11.6 26.5 40 NA 2 3/01/2002 34.6 124. 11.8 26.8 39.6 NA 3 4/01/2002 34.4 126. 11.8 27.0 39.2 NA 4 7/01/2002 34.3 124. 11.4 27.2 38.8 NA 5 8/01/2002 34.7 125. 11.3 27.4 38.9 NA 6 9/01/2002 34.4 124. 10.8 26.9 38.6 NA "],["4.3-summary-statistics.html", "4.3 Summary Statistics", " 4.3 Summary Statistics The good news is that these descriptive statistics give us a manageable and meaningful summary of the underlying phenomenon. Thats what this chapter is about. The bad news is that any simplification invites abuse. Descriptive statistics can be like online dating profiles: technically accurate and yet pretty darn misleading.-Charles Wheelan It is simple to calculate basic summary statistics in R, most of the functions are named according to what they do. For instance \\(\\mathtt{mean}\\) calculates the mean of a single variable, \\(\\mathtt{sd}\\) calculates the standard deviation. Table: Basic statistics functions in R Statistics R-Function Arithmetic mean mean( x ) Geometric mean exp( mean( log( x ) ) ) median median( x ) Range range( x ) variance var( x ) standard deviation sd( x ) Interquantile Range IQR( x ) Other quantiles quantile( x ) Skewness skewness( x ) Kurtosis kurtosis( x ) The following example demonstrates how to calculate the statistics measures in table-1 for Dow Jones prices in data file data_fin.csv # change the working directory to the folder containing data_fin.csv or provide # the full path with the filename data_stocks = read.csv(&quot;data/data_fin.csv&quot;) #import data head(data_stocks) Date DJI AXP MMM ATT BA CAT CISCO DD XOM GE 1 3/01/2000 11357.5 45.82 47.19 47.19 40.12 24.31 54.05 65.00 39.09 49.95 2 4/01/2000 10997.9 44.09 45.31 44.25 40.12 24.00 51.00 65.00 38.41 48.06 3 5/01/2000 11122.7 42.96 46.62 44.94 42.62 24.56 51.19 67.75 40.50 47.70 4 6/01/2000 11253.3 43.78 50.62 43.75 43.06 25.81 50.00 71.50 42.59 48.51 5 7/01/2000 11522.6 44.42 51.47 44.12 44.12 26.66 52.94 71.62 42.31 50.28 6 10/01/2000 11572.2 45.04 51.12 44.75 43.69 25.78 54.91 70.00 41.88 50.37 GS HD IBM INTC JNJ JPM MRK MCD MSFT NKE 1 88.31 65.50 115.56 43.47 46.09 48.69 64.04 39.62 58.34 12.03 2 82.38 61.50 112.06 41.47 44.41 47.27 61.61 38.81 56.31 11.38 3 78.88 61.44 116.00 41.81 44.88 46.98 64.22 39.44 56.91 12.03 4 82.25 60.00 114.62 39.38 46.28 47.65 64.75 38.88 55.00 11.97 5 82.56 62.81 113.31 41.00 47.88 48.52 70.97 39.75 55.72 11.97 6 84.38 63.19 118.44 42.88 47.03 47.69 68.89 40.06 56.12 12.17 DJI = data_stocks$DJI DJI = na.omit(DJI) #remove NAs as it will affect the calculations # Arithmetic mean mean(DJI) [1] 11098.12 # Geometric mean exp(mean(log(DJI))) [1] 10953.39 # median median(DJI) [1] 10748.8 # variance &amp; standard deviation var(DJI) [1] 3280347 sd(DJI) [1] 1811.173 # interquantile range and few quantiles IQR(DJI) [1] 2276.25 quantile(DJI) 0% 25% 50% 75% 100% 6547.10 10063.25 10748.80 12339.50 16576.66 # skewness and kurtosis skewness and kurtosis functions are not available in R # core library but in library e1071 (there are other packages which have # functions for skewness and kurtosis try ??kurtosis or search for the function # on RSearch. library(e1071) skewness(DJI) [1] 0.4777828 kurtosis(DJI) [1] 0.08404185 The function \\(\\mathtt{summary}\\) in R provides some basic summary viz., minimum value, maximum value, median value and quartiles for one variable or a dataset. The function \\(\\mathtt{summary}\\) can be used as follows # summary of one column/variable in a dataframe summary(DJI) Min. 1st Qu. Median Mean 3rd Qu. Max. 6547 10063 10749 11098 12340 16577 # summary of whole dataset excluding the time column summary(data_stocks[, c(2:21)]) DJI AXP MMM ATT Min. : 6547 Min. :10.26 Min. : 39.50 Min. :19.34 1st Qu.:10063 1st Qu.:38.31 1st Qu.: 62.55 1st Qu.:25.54 Median :10749 Median :47.49 Median : 77.67 Median :29.65 Mean :11098 Mean :46.77 Mean : 75.78 Mean :31.77 3rd Qu.:12340 3rd Qu.:54.44 3rd Qu.: 85.55 3rd Qu.:37.22 Max. :16577 Max. :90.73 Max. :140.25 Max. :58.50 NA&#39;s :27 NA&#39;s :12 NA&#39;s :12 NA&#39;s :12 BA CAT CISCO DD Min. : 25.06 Min. : 14.91 Min. : 8.60 Min. :16.14 1st Qu.: 44.00 1st Qu.: 28.64 1st Qu.:17.68 1st Qu.:41.13 Median : 63.56 Median : 57.10 Median :20.43 Median :44.53 Mean : 62.95 Mean : 56.01 Mean :23.41 Mean :44.37 3rd Qu.: 74.90 3rd Qu.: 79.36 3rd Qu.:24.17 3rd Qu.:48.92 Max. :138.36 Max. :116.20 Max. :80.06 Max. :71.62 NA&#39;s :12 NA&#39;s :12 NA&#39;s :13 NA&#39;s :12 XOM GE GS HD Min. : 30.27 Min. : 6.66 Min. : 52.0 Min. :18.00 1st Qu.: 42.46 1st Qu.:20.00 1st Qu.: 92.2 1st Qu.:31.00 Median : 64.77 Median :30.33 Median :116.2 Median :37.37 Mean : 63.20 Mean :29.56 Mean :126.9 Mean :40.20 3rd Qu.: 81.62 3rd Qu.:36.03 3rd Qu.:159.5 3rd Qu.:46.23 Max. :101.51 Max. :59.94 Max. :247.9 Max. :82.34 NA&#39;s :12 NA&#39;s :12 NA&#39;s :12 NA&#39;s :12 IBM INTC JNJ JPM Min. : 55.07 Min. :12.08 Min. :33.69 Min. :15.45 1st Qu.: 87.82 1st Qu.:20.17 1st Qu.:55.27 1st Qu.:35.66 Median :106.48 Median :22.76 Median :61.30 Median :40.20 Mean :118.83 Mean :25.21 Mean :61.07 Mean :40.36 3rd Qu.:130.00 3rd Qu.:26.77 3rd Qu.:65.20 3rd Qu.:45.71 Max. :215.80 Max. :74.88 Max. :95.63 Max. :65.70 NA&#39;s :12 NA&#39;s :13 NA&#39;s :12 NA&#39;s :12 MRK MCD MSFT NKE Min. :20.99 Min. : 12.38 Min. :15.15 Min. : 6.64 1st Qu.:34.53 1st Qu.: 29.19 1st Qu.:25.67 1st Qu.:14.56 Median :43.63 Median : 43.78 Median :27.59 Median :23.25 Mean :44.65 Mean : 51.07 Mean :28.38 Mean :28.01 3rd Qu.:51.55 3rd Qu.: 70.36 3rd Qu.:30.19 3rd Qu.:36.90 Max. :89.85 Max. :103.59 Max. :58.34 Max. :79.86 NA&#39;s :13 NA&#39;s :12 NA&#39;s :13 NA&#39;s :12 4.3.1 Example-Descriptive Statistics of Stock Returns In this example we will use R to calculate descriptive statistics for the returns of 10 stocks in the data file \\(\\mathtt{data\\_fin.csv}\\).  We will first import the dataset into R using the \\(\\mathtt{read.csv}\\) function. data_cs1 = read.csv(&quot;data/data_fin.csv&quot;) head(data_cs1) #check the imported data Date DJI AXP MMM ATT BA CAT CISCO DD XOM GE 1 3/01/2000 11357.5 45.82 47.19 47.19 40.12 24.31 54.05 65.00 39.09 49.95 2 4/01/2000 10997.9 44.09 45.31 44.25 40.12 24.00 51.00 65.00 38.41 48.06 3 5/01/2000 11122.7 42.96 46.62 44.94 42.62 24.56 51.19 67.75 40.50 47.70 4 6/01/2000 11253.3 43.78 50.62 43.75 43.06 25.81 50.00 71.50 42.59 48.51 5 7/01/2000 11522.6 44.42 51.47 44.12 44.12 26.66 52.94 71.62 42.31 50.28 6 10/01/2000 11572.2 45.04 51.12 44.75 43.69 25.78 54.91 70.00 41.88 50.37 GS HD IBM INTC JNJ JPM MRK MCD MSFT NKE 1 88.31 65.50 115.56 43.47 46.09 48.69 64.04 39.62 58.34 12.03 2 82.38 61.50 112.06 41.47 44.41 47.27 61.61 38.81 56.31 11.38 3 78.88 61.44 116.00 41.81 44.88 46.98 64.22 39.44 56.91 12.03 4 82.25 60.00 114.62 39.38 46.28 47.65 64.75 38.88 55.00 11.97 5 82.56 62.81 113.31 41.00 47.88 48.52 70.97 39.75 55.72 11.97 6 84.38 63.19 118.44 42.88 47.03 47.69 68.89 40.06 56.12 12.17 Apply function to with dates as character and then after converting dates to Date class. # selecting first 10 price series including the data column data_cs1.1 = data_cs1[, c(1:11)] # data cleaning-remove NAs data_cs1.1 = na.omit(data_cs1.1) colnames(data_cs1.1) # see the columns present in the data [1] &quot;Date&quot; &quot;DJI&quot; &quot;AXP&quot; &quot;MMM&quot; &quot;ATT&quot; &quot;BA&quot; &quot;CAT&quot; &quot;CISCO&quot; &quot;DD&quot; [10] &quot;XOM&quot; &quot;GE&quot; summary(data_cs1.1) #notice the Date variable Date DJI AXP MMM Length:3523 Min. : 6547 Min. :10.26 Min. : 39.50 Class :character 1st Qu.:10063 1st Qu.:38.38 1st Qu.: 62.55 Mode :character Median :10749 Median :47.60 Median : 77.67 Mean :11098 Mean :46.83 Mean : 75.80 3rd Qu.:12340 3rd Qu.:54.50 3rd Qu.: 85.61 Max. :16577 Max. :90.73 Max. :140.25 ATT BA CAT CISCO Min. :19.34 Min. : 25.06 Min. : 14.91 Min. : 8.60 1st Qu.:25.54 1st Qu.: 44.02 1st Qu.: 28.48 1st Qu.:17.68 Median :29.76 Median : 63.61 Median : 57.11 Median :20.39 Mean :31.79 Mean : 62.99 Mean : 56.03 Mean :23.42 3rd Qu.:37.23 3rd Qu.: 74.95 3rd Qu.: 79.50 3rd Qu.:24.18 Max. :58.50 Max. :138.36 Max. :116.20 Max. :80.06 DD XOM GE Min. :16.14 Min. : 30.27 Min. : 6.66 1st Qu.:41.17 1st Qu.: 42.41 1st Qu.:20.04 Median :44.58 Median : 64.70 Median :30.37 Mean :44.43 Mean : 63.18 Mean :29.63 3rd Qu.:48.93 3rd Qu.: 81.70 3rd Qu.:36.05 Max. :71.62 Max. :101.51 Max. :59.94 # check class of dates which will be factor ( treated as factor by default)\\t class(data_cs1.1$Date) [1] &quot;character&quot; # convert dates to class Date data_cs1.1$Date = as.Date(data_cs1.1$Date, format = &quot;%d/%m/%Y&quot;) class(data_cs1.1$Date) [1] &quot;Date&quot; summary(data_cs1.1) #notice the Date variable Date DJI AXP MMM Min. :2000-01-03 Min. : 6547 Min. :10.26 Min. : 39.50 1st Qu.:2003-07-08 1st Qu.:10063 1st Qu.:38.38 1st Qu.: 62.55 Median :2007-01-05 Median :10749 Median :47.60 Median : 77.67 Mean :2007-01-03 Mean :11098 Mean :46.83 Mean : 75.80 3rd Qu.:2010-07-06 3rd Qu.:12340 3rd Qu.:54.50 3rd Qu.: 85.61 Max. :2014-01-03 Max. :16577 Max. :90.73 Max. :140.25 ATT BA CAT CISCO Min. :19.34 Min. : 25.06 Min. : 14.91 Min. : 8.60 1st Qu.:25.54 1st Qu.: 44.02 1st Qu.: 28.48 1st Qu.:17.68 Median :29.76 Median : 63.61 Median : 57.11 Median :20.39 Mean :31.79 Mean : 62.99 Mean : 56.03 Mean :23.42 3rd Qu.:37.23 3rd Qu.: 74.95 3rd Qu.: 79.50 3rd Qu.:24.18 Max. :58.50 Max. :138.36 Max. :116.20 Max. :80.06 DD XOM GE Min. :16.14 Min. : 30.27 Min. : 6.66 1st Qu.:41.17 1st Qu.: 42.41 1st Qu.:20.04 Median :44.58 Median : 64.70 Median :30.37 Mean :44.43 Mean : 63.18 Mean :29.63 3rd Qu.:48.93 3rd Qu.: 81.70 3rd Qu.:36.05 Max. :71.62 Max. :101.51 Max. :59.94 Convert prices to returns d2 = as.data.frame(sapply(data_cs1.1[2:11], function(x) diff(log(x)) * 100)) #note it will be one less # create a different dataframe with returns data_stocks_ret = as.data.frame(cbind(Date = data_cs1.1$Date[2:length(data_cs1.1$Date)], d2), stringsAsFactors = FALSE, row.names = NULL) # visual inspection head(data_stocks_ret) Date DJI AXP MMM ATT BA CAT 1 2000-01-04 -3.2173973 -3.8487678 -4.0654247 -6.4326634 0.0000000 -1.283396 2 2000-01-05 1.1283720 -2.5963549 2.8501875 1.5472895 6.0448664 2.306527 3 2000-01-06 1.1673354 1.8907642 8.2317122 -2.6836654 1.0270865 4.964291 4 2000-01-07 2.3648905 1.4512726 1.6652359 0.8421582 2.4318702 3.240230 5 2000-01-10 0.4295346 1.3861165 -0.6823304 1.4178250 -0.9793951 -3.356532 6 2000-01-11 -0.5293883 0.9061837 -1.7165263 -1.4178250 -1.8713726 -1.563754 CISCO DD XOM GE 1 -5.8083911 0.0000000 -1.7548837 -3.8572275 2 0.3718568 4.1437190 5.2984132 -0.7518832 3 -2.3521195 5.3872990 5.0317510 1.6838564 4 5.7136191 0.1676915 -0.6596019 3.5837421 5 3.6536284 -2.2879123 -1.0215079 0.1788376 6 -3.0697677 -1.8018506 1.1868167 0.2379537 4.3.1.1 Using the \\(\\mathtt{describe}\\) function The package psych comes with a function called \\(\\mathtt{describe}\\) which generated the descriptive statistics for all the data vectors (columns) in a data frame, matrix or a vector. library(psych) #load the required package args(describe) #arguments for describe function function (x, na.rm = TRUE, interp = FALSE, skew = TRUE, ranges = TRUE, trim = 0.1, type = 3, check = TRUE, fast = NULL, quant = NULL, IQR = FALSE, omit = FALSE, data = NULL) NULL # use describe to calculate descriptive stats for data_cs1.1r desc1 = describe(data_stocks_ret[, 2:11]) #note we dont pass the date column # check the output head(desc1) vars n mean sd median trimmed mad min max range skew kurtosis DJI 1 3522 0.01 1.23 0.04 0.03 0.82 -8.20 10.51 18.71 -0.06 7.71 AXP 2 3522 0.02 2.89 0.02 0.03 1.55 -19.35 18.77 38.12 -0.01 9.14 MMM 3 3522 0.03 1.55 0.03 0.03 1.10 -9.38 10.39 19.78 0.06 4.87 ATT 4 3522 -0.01 1.80 0.03 0.01 1.22 -13.54 15.08 28.62 0.02 6.26 BA 5 3522 0.03 2.01 0.05 0.06 1.57 -19.39 14.38 33.77 -0.26 5.39 CAT 6 3522 0.04 2.14 0.04 0.05 1.65 -15.69 13.73 29.42 -0.08 4.08 se DJI 0.02 AXP 0.05 MMM 0.03 ATT 0.03 BA 0.03 CAT 0.04 # the above output is in long format, we can transpose it get column format desc1.t = t(desc1) head(desc1.t) DJI AXP MMM ATT BA vars 1.000000e+00 2.000000e+00 3.000000e+00 4.000000e+00 5.000000e+00 n 3.522000e+03 3.522000e+03 3.522000e+03 3.522000e+03 3.522000e+03 mean 1.055257e-02 1.908563e-02 3.056011e-02 -8.647491e-03 3.499777e-02 sd 1.226702e+00 2.892586e+00 1.551706e+00 1.799180e+00 2.013123e+00 median 4.442671e-02 1.723604e-02 3.233787e-02 3.018428e-02 5.279680e-02 trimmed 2.597511e-02 3.152635e-02 3.241946e-02 9.134423e-03 5.567097e-02 CAT CISCO DD XOM GE vars 6.000000e+00 7.000000e+00 8.000000e+00 9.000000e+00 1.000000e+01 n 3.522000e+03 3.522000e+03 3.522000e+03 3.522000e+03 3.522000e+03 mean 3.710732e-02 -2.554732e-02 -5.379787e-04 2.653014e-02 -1.696661e-02 sd 2.143040e+00 2.744068e+00 1.881993e+00 1.629181e+00 2.057353e+00 median 4.291300e-02 3.800312e-02 0.000000e+00 5.437650e-02 0.000000e+00 trimmed 4.678016e-02 -1.339579e-04 1.469270e-03 4.883990e-02 9.841473e-05 The descriptive statistics generated above gives mean, median, standard deviation, trimmed mean(trimmed), median, mad (median absolute deviation from the median), minimum (min), maximum (max), skewness (skew), kurtosis and standard error (se) . This can easily be transferred to a CSV file or a text file. The following single line of code transfers the descriptive statistics to a CSV file which then can be imported into a word or latex file as required. The pastecs package provides the function \\(\\mathtt{stat.desc}\\) which generated descriptive statistics for a data frame, matrix or a timeseries. Skewness and Kurtosis are not calculated by default in \\(\\mathtt{stat.desc}\\) but the argument \\(\\mathtt{norm}\\) can be set to TRUE to get these measures along with their standard errors. require(pastecs) # note library and require can both be used to include a package # detach the package pastecs its useful to avoid any conflicts (e.g psych and # Hmisc have &#39;describe&#39; function with two different behaviours detach(&quot;package:psych&quot;, unload = TRUE) # use stat.desc in with default arguments desc2 = stat.desc(data_stocks_ret[, 2:11], norm = TRUE) desc2 #note skewness/kurtosis DJI AXP MMM ATT nbr.val 3.522000e+03 3.522000e+03 3.522000e+03 3.522000e+03 nbr.null 2.000000e+00 2.400000e+01 3.000000e+01 5.000000e+01 nbr.na 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 min -8.200737e+00 -1.935233e+01 -9.383688e+00 -1.353821e+01 max 1.050812e+01 1.877116e+01 1.039309e+01 1.508318e+01 range 1.870886e+01 3.812349e+01 1.977678e+01 2.862139e+01 sum 3.716616e+01 6.721959e+01 1.076327e+02 -3.045646e+01 median 4.442671e-02 1.723604e-02 3.233787e-02 3.018428e-02 mean 1.055257e-02 1.908563e-02 3.056011e-02 -8.647491e-03 SE.mean 2.067018e-02 4.874068e-02 2.614656e-02 3.031656e-02 CI.mean.0.95 4.052675e-02 9.556282e-02 5.126395e-02 5.943979e-02 var 1.504798e+00 8.367052e+00 2.407790e+00 3.237048e+00 std.dev 1.226702e+00 2.892586e+00 1.551706e+00 1.799180e+00 coef.var 1.162467e+02 1.515583e+02 5.077551e+01 -2.080580e+02 skewness -5.829983e-02 -6.689750e-03 5.927112e-02 1.620418e-02 skew.2SE -7.065472e-01 -8.107441e-02 7.183185e-01 1.963817e-01 kurtosis 7.714304e+00 9.141053e+00 4.865294e+00 6.257155e+00 kurt.2SE 4.675883e+01 5.540681e+01 2.949008e+01 3.792659e+01 normtest.W 9.187712e-01 8.496717e-01 9.384591e-01 9.298653e-01 normtest.p 5.671566e-40 1.020339e-49 6.131053e-36 8.237017e-38 BA CAT CISCO DD nbr.val 3.522000e+03 3.522000e+03 3.522000e+03 3.522000e+03 nbr.null 1.500000e+01 2.400000e+01 5.000000e+01 2.900000e+01 nbr.na 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 min -1.938931e+01 -1.568589e+01 -1.768649e+01 -1.202802e+01 max 1.437774e+01 1.373497e+01 2.182386e+01 1.085590e+01 range 3.376704e+01 2.942086e+01 3.951034e+01 2.288392e+01 sum 1.232621e+02 1.306920e+02 -8.997766e+01 -1.894761e+00 median 5.279680e-02 4.291300e-02 3.800312e-02 0.000000e+00 mean 3.499777e-02 3.710732e-02 -2.554732e-02 -5.379787e-04 SE.mean 3.392155e-02 3.611067e-02 4.623812e-02 3.171197e-02 CI.mean.0.95 6.650788e-02 7.079996e-02 9.065621e-02 6.217570e-02 var 4.052665e+00 4.592620e+00 7.529907e+00 3.541897e+00 std.dev 2.013123e+00 2.143040e+00 2.744068e+00 1.881993e+00 coef.var 5.752148e+01 5.775248e+01 -1.074112e+02 -3.498266e+03 skewness -2.605203e-01 -8.369997e-02 1.547891e-01 -1.523353e-01 skew.2SE -3.157298e+00 -1.014377e+00 1.875920e+00 -1.846181e+00 kurtosis 5.392404e+00 4.077013e+00 7.329406e+00 5.061290e+00 kurt.2SE 3.268507e+01 2.471206e+01 4.442585e+01 3.067808e+01 normtest.W 9.550329e-01 9.583075e-01 9.104004e-01 9.381393e-01 normtest.p 1.196739e-31 1.172506e-30 1.882675e-41 5.179385e-36 XOM GE nbr.val 3.522000e+03 3.522000e+03 nbr.null 2.900000e+01 6.300000e+01 nbr.na 0.000000e+00 0.000000e+00 min -1.502710e+01 -1.368410e+01 max 1.586307e+01 1.798444e+01 range 3.089017e+01 3.166854e+01 sum 9.343915e+01 -5.975640e+01 median 5.437650e-02 0.000000e+00 mean 2.653014e-02 -1.696661e-02 SE.mean 2.745205e-02 3.466683e-02 CI.mean.0.95 5.382353e-02 6.796911e-02 var 2.654232e+00 4.232702e+00 std.dev 1.629181e+00 2.057353e+00 coef.var 6.140870e+01 -1.212589e+02 skewness 4.651513e-02 1.102593e-02 skew.2SE 5.637262e-01 1.336254e-01 kurtosis 1.043194e+01 7.781017e+00 kurt.2SE 6.323129e+01 4.716320e+01 normtest.W 9.160764e-01 9.046828e-01 normtest.p 1.839459e-40 2.126284e-42 "],["5-graphics-in-r-part-i.html", "Topic 5 Graphics in R (Part-I) ", " Topic 5 Graphics in R (Part-I) "],["5.1-basic-plots-in-r.html", "5.1 Basic Plots in R", " 5.1 Basic Plots in R 5.1.1 Scatter Plot One of the most popular and most frequently used functions to build a new plot in R is the \\(\\mathtt{plot}\\) function. Plot is a high level generic graphic function which depends on the class of the first argument (usually the data object). For example if the first argument is of the class zoo which is a timeseries object the \\(\\mathtt{plot}\\) function will call \\(\\mathtt{plot.zoo}\\) from the R package zoo. This time series plot can be a single timeseries line plot or multiple time series stacked plot. # Generate two random normal vectors x = rnorm(100) y = rnorm(100) # plot x and y using the plot() function plot(x, y) Figure 5.1: Simple Scatter Plot There are various other arguments which can be modified to change the overall presentation of the plot and even the type of plot. args(plot.default) function (x, y = NULL, type = &quot;p&quot;, xlim = NULL, ylim = NULL, log = &quot;&quot;, main = NULL, sub = NULL, xlab = NULL, ylab = NULL, ann = par(&quot;ann&quot;), axes = TRUE, frame.plot = axes, panel.first = NULL, panel.last = NULL, asp = NA, xgap.axis = NA, ygap.axis = NA, ...) NULL The following example uses the argument \\(\\mathtt{main}\\) in the \\(\\mathtt{plot}\\) function to include a title on the plot along with axis tiles using the arguments \\(\\mathtt{xlab}\\) and \\(\\mathtt{ylab}\\). plot(x, y, main = &quot;Figure-2&quot;, xlab = &quot;Normal X&quot;, ylab = &quot;Normal Y&quot;) Figure 5.2: Simple Scatter Plot with Title 5.1.2 Line Plot The following example demonstrates how to create a line plot using Microsoft prices given in the data file \\(\\mathtt{data\\_fin.RData}\\). # change the working directory to the folder containing data_fin.csv or provide # the full path with the filename load(&quot;data/data_fin.RData&quot;) # column names colnames(FinData) [1] &quot;Date&quot; &quot;DJI&quot; &quot;AXP&quot; &quot;MMM&quot; &quot;ATT&quot; &quot;BA&quot; &quot;CAT&quot; &quot;CISCO&quot; &quot;DD&quot; [10] &quot;XOM&quot; &quot;GE&quot; &quot;GS&quot; &quot;HD&quot; &quot;IBM&quot; &quot;INTC&quot; &quot;JNJ&quot; &quot;JPM&quot; &quot;MRK&quot; [19] &quot;MCD&quot; &quot;MSFT&quot; &quot;NKE&quot; # plot a line plot for Dow Jones stock index prices plot(FinData$MSFT, type = &quot;l&quot;, main = &quot;Microsoft Prices&quot;, ylab = &quot;Prices&quot;) Figure 5.3: Line Chart Plotting it as Time Series data Various R packages provide functionality to plot specific data types. For example, \\(\\mathtt{zoo}\\) can be used to plot time series data. library(zoo) # convert data to class zoo FinData.ts = zoo(FinData[, 2:5], order.by = FinData$Date) # plot multiple stacked plot plot(FinData.ts, col = gray.colors(4)) #figure-4 Figure 5.4: Time Series Plot 5.1.3 Bar Plot The function \\(\\mathtt{barplot}\\) creates bar graphs in R. The main data argument in this function is \\(\\mathtt{height}\\) which can be a vector or a matrix of values describing the bars which make up the plot. If \\(\\mathtt{height}\\)is a matrix the bar graph can be a stacked graph or a juxtaposed graph with \\(\\mathtt{besides=TRUE}\\) load(&quot;data/GDP_Yearly.RData&quot;) par1 = par() par(ask = F) barplot(height = GDP$Australia, names.arg = GDP$Year, ylab = &quot;GDP Per Capita&quot;) #figure-5 Figure 5.5: Bar Graph with argument height as vector It is also possible to create a yearly vertical stacked or yearly horizontal grouped bar plot for the GDP data. The data has to be first converted into a matrix to create stacked or grouped barplots. In this example the argument \\(\\mathtt{legend}\\) specifies the names (Years) to appear in the legend and the argument \\(\\mathtt{args.legend}\\) specifies the position (x=top), alignment (horiz=TRUE) and distance from the margin (inset=-0.1). # convert data to matrix data = as.matrix(GDP[, 2:12]) # create row names rownames(data) = GDP$Year # plot a stacked bar plot with legend showing the years barplot(height = data[1:5, ], beside = FALSE, col = rainbow(5), legend = rownames(data[1:5, ]), args.legend = list(x = &quot;top&quot;, horiz = TRUE, inset = -0.1), cex.names = 0.6) Figure 5.6: Vertical Stacked Barplot par(par1) 5.1.4 Pie Chart R provides the function \\(\\mathtt{pie}\\) to create pie graphs. \\(\\mathtt{labels}\\) in 5.7 pie(x = data[1, ], labels = colnames(data)) Figure 5.7: Pie Chart 5.1.5 Scatter Plot The basic \\(\\mathtt{plot}\\) function plots a scatter plot for bivariate or univariate data. It is also possible to create a scatterplot for multivariate data using the \\(\\mathtt{pairs}\\) function. pairs(data[, 1:5]) Figure 5.8: Scatterplot A subset can also be selected using formula method, for example the following R code will generate a scatterplot with only Australia, UK and USA. pairs(~Australia + UK + USA, data = data) Figure 5.9: Scatterplot (subset) "],["5.2-r-graphical-parameters.html", "5.2 R Graphical Parameters", " 5.2 R Graphical Parameters The \\(\\mathtt{par}\\) function facilitates access and modification of a large list of parameters such as color, margin, number of rows and columns on a graphic device etc (see \\(\\mathtt{help(par)}\\) for a list of such parameters). R provides various margin parameters to tweak inner and outer margins of a graphical device. Figure 5.10: Graph Margins A modification to par always changes the global values of graphic parameters and hence its is a good practice to first store the default parameters in a separate object (variable) which can be later used to restore default graphic parameters. These margins can be altered using or parameters with the first setting the margins in inches and the second in unit of text lines. Setting one of these will adjust the other accordingly. The following example (output not shown here) changes the margins to \\(\\mathtt{c(5,4,7,2)}\\) from the default of \\(\\mathtt{c(5,4,4,2)+0.1}\\) to accommodate a title on top of the figure. # first save the default parameters par.old = par() # change the margins par(mar = c(5, 4, 7, 2)) # plot the bargraph barplot(height = data[1:5, ], beside = FALSE, col = rainbow(5), legend = rownames(data[1:5, ]), args.legend = list(x = &quot;top&quot;, horiz = TRUE, inset = -0.1), cex.names = 0.6) title(&quot;Bar Plot \\n(with custom margins)&quot;) # set parameters to default par(par.old) A multiple plot grid can be created by altering \\(\\mathtt{mfrow}\\) or \\(\\mathtt{mfcol}\\) parameter which specifies the number of rows and columns in a grid. # first save the default parameters par.old = par() # creat a 2X2 grid par(mfrow = c(2, 2)) # scatterplot plot(x, y, xlab = &quot;Normal X&quot;, ylab = &quot;Normal Y&quot;) # time series plot plot(FinData.ts[, 1]) # bar plot barplot(height = GDP$Australia, names.arg = GDP$Year, ylab = &quot;GDP Per Capita (Australia)&quot;) # pie chart pie(x = data[1:11, 1], labels = rownames(data[1:11, ])) # set parameters to default par(par.old) Figure 5.11: Multiple Plots in a Grid "],["5.3-introduction-to-ggplot2.html", "5.3 Introduction to ggplot2", " 5.3 Introduction to ggplot2 ggplot2 (Wickham 2009) is an R package which provides a large variety of plotting functionality to enable better and highly customisable graphs. These functions in ggplot2 are based on the grammar of graphics (Wickham 2010) which is a more formal and structured way to plotting, for a list of various possible graphs, customisation settings and procedures see https://ggplot2.tidyverse.org/ 5.3.1 \\(\\mathtt{qplot}\\) \\(\\mathtt{qplot}\\) stands for quick plot and it makes is easy to produce plots which may often require several lines of codes using base R graphics system. \\(\\mathtt{qplot}\\) is particularly useful for beginners as they are just getting used to the \\(\\mathtt{plot}\\) function from the base package also the data arguments in \\(\\mathtt{qplot}\\) are same as in the \\(\\mathtt{plot}\\) function (see \\(\\mathtt{help(qplot)}\\) for other arguments to the function). x = rnorm(100) y = rnorm(100) # load the library library(ggplot2) # simple scatterplot using qplot qplot(x, y) Figure 5.12: Scatterplot using qplot The argument geom, which stands for geometric objects drawn to represent data has to be changed to line create this line plot. Similarly there is an option to plot histograms using the argument geom=histogram load(&quot;data/data_fin.RData&quot;) # line plot using qplot qplot(x = FinData$Date, y = FinData$DJI, geom = &quot;line&quot;, xlab = &quot;Dates&quot;, ylab = &quot;Prices&quot;, main = &quot;DJIA Price Timeseries&quot;) Figure 5.13: Line Plot with lables using qplot 5.3.2 Layered graphics using \\(\\mathtt{ggplot}\\) The \\(\\mathtt{qplot}\\) function is just sufficient for creating various plots with better presentation compared to base R plots but the true capabilities of ggplot2 are realised by the function \\(\\mathtt{ggplot}\\). It is important to note that \\(\\mathtt{ggplot}\\) function requires the data in long format and hence it is required to first transform the dataset to long from wide format as in ggplot2, groups are identified by rows, not by columns. # Read &#39;long&#39; format data load(&quot;data/GDP_l.RData&quot;) # data snapshot head(GDP_l) Year Country GDP 1 1990 Australia 18247.39 2 1991 Australia 18837.19 3 1992 Australia 18599.00 4 1993 Australia 17658.08 5 1994 Australia 18080.70 6 1995 Australia 20375.30 # creating the aesthetics using ggplot p1 = ggplot(GDP_l, aes(Country, GDP, fill = Year)) A plot can be created by adding another layer to p1 # figure p1 + geom_bar(stat = &quot;identity&quot;) Figure 5.14: Bar Chart Using ggplot function To draw a line chart using \\(\\mathtt{ggplot}\\), \\(\\mathtt{geom\\_line()}\\) # change the aesthetics to show time on X-axis and GDP values on Y-axis the # colour line fill be according to the country p2 = ggplot(GDP_l, aes(Year, GDP, colour = Country, group = Country)) p2 + geom_line() Figure 5.15: Line Chart Using ggplot These lines can also be drawn in separate panels using faceting. Faceting creates a subplot for each group side by side. Faceting can be used to either to split the data into vertical groups using \\(\\mathtt{facet\\_grid}\\) or horizontal groups using \\(\\mathtt{facet\\_wrap}\\). Figure plots GDP for each country in a separate subplot using grid faceting. # change the aesthetics to show time on X-axis and GDP values on Y-axis the # colour line fill be according to the country p2 = ggplot(GDP_l, aes(Year, GDP, colour = Country, group = Country)) p2 + geom_line() + facet_grid(Country ~ .) Figure 5.16: Faceting in ggplot (Line Chart) 5.3.3 Arranging plots using gridExtra There are a few pacakges which allow to arrange ggplots in a grid or a speacific order. gridExtra is one of them and is quite useful in arranging the plots. Look at the Vignette for egg package for more options. https://cran.r-project.org/web/packages/egg/vignettes/Ecosystem.html Lets create three ggplots p1.1 = ggplot(GDP_l, aes(x = Year, y = GDP)) p2.1 = p1.1 + geom_bar(aes(fill = Country), stat = &quot;identity&quot;, position = &quot;dodge&quot;) Stacked bar chart (previous example) p1.2 = ggplot(GDP_l[GDP_l$Country %in% c(&quot;Australia&quot;, &quot;UK&quot;, &quot;USA&quot;), ], aes(Year, GDP)) p2.2 = p1.2 + geom_col(aes(fill = Country)) + labs(title = &quot;GDP for Aus, US and UK&quot;) #using labs to modify title p2.2 Figure 5.17: Bar Chart with Selected Data Stock data p1.3 = ggplot(FinData, aes(x = Date, y = DJI)) p2.3 = p1.3 + geom_path(colour = &quot;darkblue&quot;) + geom_smooth(colour = &quot;black&quot;) + theme_linedraw() #changing theme p2.3 Figure 5.18: Stock Series Plot with Smooth Curve Now use gridExtra to put these together library(gridExtra) fig1 = grid.arrange(p2.1, p2.2, p2.3, nrow = 3, heights = c(20, 12, 12), top = &quot;Combined plots in three rows&quot;) Figure 5.19: Combined plots The plots can be saved using the \\(\\mathtt{ggsave}\\) function ggsave(filename = &quot;combined_plot.pdf&quot;, plot = fig1) References "],["6-graphics-in-r-part-ii.html", "Topic 6 Graphics in R (Part-II) ", " Topic 6 Graphics in R (Part-II) "],["6.1-interactive-plots-using-plotly.html", "6.1 Interactive Plots using Plotly", " 6.1 Interactive Plots using Plotly Here we will use the COVID-19 data provided by John Hopkins University library(ggplot2) library(maps) library(ggthemes) library(plotly) library(scales) library(dplyr) library(tidyr) # download data d1 = read.csv(&quot;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv&quot;, check.names = FALSE) # head(d1) Data is in wide format lets convert in long format for visualisation # rename Provice/State and Country columns colnames(d1)[1:2] = c(&quot;State&quot;, &quot;Country&quot;) d1.2 = pivot_longer(d1, cols = -c(State, Country, Lat, Long), names_to = &quot;Date&quot;, values_to = &quot;Cases&quot;) # convert dates d1.2$Date = as.Date(d1.2$Date, format = &quot;%m/%e/%y&quot;) Aggregate cases by day (dropping the state) d2 = aggregate(d1.2$Cases, by = list(Lat = d1.2$Lat, Long = d1.2$Long, Country = d1.2$Country, Date = d1.2$Date), FUN = sum) colnames(d2)[5] = &quot;Cases&quot; # reorder d2 = d2[, c(4, 1, 2, 3, 5)] Lets find top 10 by case numbers Using aggregate to find sum by country to find top 10 top1 = aggregate(d2$Cases, by = list(Date = d2$Date, Country = d2$Country), FUN = sum) # select the last date to get overall total top10 = top1[top1$Date == &quot;2021-08-11&quot;, ] # select top 10 top10 = top10[order(-top10$x), ][1:10, ] # let&#39;s include Aus top10_country = c(top10$Country, &quot;Australia&quot;) Use ggplot to create a line chart 6.1 colnames(top1)[3] = &quot;Cases&quot; data_p = top1[top1$Country %in% c(as.character(top10_country)), ] p1 = ggplot(data = data_p, aes(Date, log(Cases), color = Country, group = Country)) + geom_line(stat = &quot;identity&quot;, size = 1) + scale_x_date(labels = date_format(&quot;%m/%y&quot;), breaks = &quot;2 months&quot;) + theme_wsj() p1 Figure 6.1: Line Chart with Custom Theme Create custom color vector and a line chart with basic theme to convert to plotly 6.2 myCol2 = c(&quot;slateblue1&quot;, &quot;purple3&quot;, &quot;turquoise2&quot;, &quot;skyblue&quot;, &quot;steelblue&quot;, &quot;blue2&quot;, &quot;navyblue&quot;, &quot;orange&quot;, &quot;tomato&quot;, &quot;coral2&quot;, &quot;palevioletred&quot;, &quot;violetred&quot;, &quot;red2&quot;, &quot;springgreen2&quot;, &quot;yellowgreen&quot;, &quot;palegreen4&quot;, &quot;wheat2&quot;, &quot;tan&quot;, &quot;tan2&quot;, &quot;tan3&quot;, &quot;brown&quot;, &quot;grey70&quot;, &quot;grey50&quot;, &quot;grey30&quot;) p2 = ggplot(data_p, aes(Date, Cases, group = Country, color = Country)) + geom_line(size = 1.5) + geom_point(size = 1.5) + scale_colour_manual(values = myCol2, &quot;Countries&quot;) + geom_text(data = data_p[data_p$Date == max(data_p$Date), ], aes(x = as.Date(max(data_p$Date) + 4), label = Country), hjust = -0.01, nudge_y = 0.01, show.legend = FALSE) + expand_limits(x = as.Date(c(min(data_p$Date), max(data_p$Date) + 5))) + scale_x_date(breaks = seq(as.Date(min(data_p$Date)), as.Date(max(data_p$Date) + 5), by = &quot;30 days&quot;), date_labels = &quot;%m/%y&quot;) + scale_y_continuous(labels = comma) + theme_classic() + theme(axis.title = element_text(size = 15, face = &quot;bold&quot;)) p2 Figure 6.2: Line Chart Convert to plotly for interactive graphics 6.3 fig_p2 = ggplotly(p2) fig_p2 Figure 6.3: Interactive Line Chart "],["6.2-animation-using-gganimate.html", "6.2 Animation using gganimate", " 6.2 Animation using gganimate We can also use the gganimate package to convert the image into a gif. library(gganimate) p1.anim = p1 + transition_reveal(Date) anim_p1 = animate(p1.anim, fps = 10, start_pause = 2, end_pause = 5, rewind = FALSE, width = 800, height = 1000) anim_save(filename = &quot;covid_cases_log_2021aug.gif&quot;, animation = anim_p1) Figure 6.4: Animated Graph We can also use further customisations to visualise other information on the plot The following example uses data from the 2021 COVID-19 spread in Australia The plot shows the number of Covid-19 cases along with the Date in the animation The data is the last 3 months data (accessed: 30-08-2021), downloaded from https://www.covid19data.com.au/states-and-territories The code below imports the data and selects NSW, VIC and ACT, then creates EMA and SMA using last 7 Days of data. library(tidyverse) #using tidyverse here to easily create grouped statistics d_au_3m = read.csv(&quot;data/Last 3 months.csv&quot;) # the date column name doesnt look ok so lets rename it colnames(d_au_3m)[1] = &quot;Date&quot; # convert to date type d_au_3m$Date = as.Date(d_au_3m$Date, format = &quot;%d/%m/%y&quot;) d_au_3m = d_au_3m[, c(1:3, 9)] data_p2 = d_au_3m %&gt;% pivot_longer(cols = -Date, names_to = &quot;State&quot;, values_to = &quot;Cases&quot;) rolling1 = data_p2 %&gt;% arrange(Date, State, Cases) %&gt;% group_by(State) %&gt;% mutate(EMA = TTR::EMA(Cases, n = 7)) #add EMA rolling1 = rolling1 %&gt;% arrange(Date, State, Cases) %&gt;% group_by(State) %&gt;% mutate(SMA = TTR::SMA(Cases, n = 7)) #add SMA rolling1 = rolling1[rolling1$Date &gt; as.Date(&quot;2021-07-15&quot;), ] #selecting from 15 July 2021 rolling1$State = factor(rolling1$State, levels = c(&quot;NSW&quot;, &quot;VIC&quot;, &quot;ACT&quot;)) Create plots then animate Plot The plot first layer has points and lines for the EMA (can replace for SMA or plot both) The line has an arrow at the end The scale is changed to display the dates better and some changes are made to theme elements Animation A transition_manual is used to access current_frame, cumulative=TRUE keeps the previous data Title used elements from the ggtext to modify the colors of the group variables Notice the use of element_markdown() in the ggplot library(ggthemes) library(ggtext) p2 = ggplot(rolling1, aes(Date, Cases)) + geom_point(alpha = 0.7, aes(color = State, group = seq_along(Date), frame = Cases)) + geom_path(aes(y = EMA, color = State), arrow = arrow(ends = &quot;last&quot;, type = &quot;closed&quot;, length = unit(0.05, &quot;inches&quot;)), size = 1.05, show.legend = FALSE) p3 = p2 + scale_x_date(breaks = c(seq(min(rolling1$Date), max(rolling1$Date), by = &quot;5 days&quot;)), date_labels = &quot;%d/%m&quot;) + scale_color_discrete(name = &quot;&quot;) + labs(x = &quot;Date&quot;, y = &quot;Cases&quot;) + theme_minimal() + theme(title = element_text(face = &quot;bold&quot;), legend.position = &quot;none&quot;, axis.title = element_text(size = 8), strip.text.x = element_text(face = &quot;bold&quot;), axis.text.x = element_text(size = 5, face = &quot;bold&quot;), axis.text.y = element_text(size = 6, face = &quot;bold&quot;), legend.title = element_text(size = 10), plot.subtitle = element_markdown()) + labs(x = &quot;Date&quot;, y = &quot;Cases/EMA (7 Day)&quot;) p2.anim = p3 + transition_manual(Date, cumulative = TRUE) + ggtitle(&quot;NSW, VIC &amp; ACT Daily Cases/EMA(7 Days) &quot;, subtitle = &quot;Date:{current_frame}&lt;br&gt;&lt;span style=&#39;color:#F8766D;&#39;&gt;NSW:{rolling1[rolling1$Date==as.Date(current_frame),3][2,1]} &lt;/span&gt; | &lt;span style=&#39;color:#00BA38;&#39;&gt;VIC:{rolling1[rolling1$Date==as.Date(current_frame),3][3,1]}&lt;/span&gt; | &lt;span style=&#39;color:#619CFF;&#39;&gt; ACT:{rolling1[rolling1$Date==as.Date(current_frame),3][1,1]}&lt;/span&gt;&quot;) anim_p2 = animate(p2.anim, fps = 8, start_pause = 1, end_pause = 30, detail = 2, rewind = FALSE, width = 720, height = 720, res = 140, renderer = gifski_renderer()) anim_save(filename = &quot;covid_aus_cases_EMA_aug30_.gif&quot;, animation = anim_p2) ` Figure 6.5: Animated Graph (COVID-19 AU) "],["6.3-plot-maps.html", "6.3 Plot Maps", " 6.3 Plot Maps We can also plot the data on a map 6.6 # take last day&#39;s data data from d2 d2 = d2[d2$Date == max(d2$Date), ] world = map_data(&quot;world&quot;) w1 = ggplot() + geom_polygon(data = world, aes(color = region, x = long, y = lat, group = group), fill = &quot;white&quot;) + theme_map() + theme(legend.position = &quot;none&quot;) + scale_fill_brewer(palette = &quot;Blues&quot;) map1 = w1 + geom_point(aes(x = Long, y = Lat, size = Cases, colour = Country), data = d2) + labs(title = paste(&quot;COVID-19 Cases as of &quot;, as.character(unique(d2$Date)))) # static version map1 Figure 6.6: Map Interactive version using plotly 6.7 # interactive version map2 = ggplotly(map1, originalData = FALSE, tooltip = c(&quot;colour&quot;, &quot;size&quot;), width = 750) map2 Figure 6.7: Interactive Map # To save htmlwidgets::saveWidget(map2,file=&#39;map2.html&#39;) "],["7-linear-regression.html", "Topic 7 Linear Regression", " Topic 7 Linear Regression Introduction Regression analysis is one of the most widely used tool in quantitative research which is used to analyse the relationship between variables. One or more variables are considered to be explanatory variables, and the other is considered to be the dependent variable. In general linear regression is used to predict a continuous dependent variable (regressand) from a number of independent variables (regressors) assuming that the relationship between the dependent and independent variables is linear. If we have a dependent (or response) variable Y which is related to a predictor variables \\(X_{i}\\). The simple regression model is given by \\[\\begin{equation} Y=\\alpha+\\beta X_{i}+\\epsilon_{i} \\tag{7.1} \\end{equation}\\] R has the function \\(\\mathtt{lm}\\) (linear model) for linear regression. The main arguments to the function \\(\\mathtt{lm}\\) are a formula and the data. \\(\\mathtt{lm}\\) takes the defining model input as a formula A formula object is also used in other statistical function like \\(\\mathtt{glm,\\,nls,\\,rq}\\) etc, which is from a formula class. "],["7.1-investment-beta-using-r-single-index-model.html", "7.1 Investment \\(\\beta\\) using R (Single Index Model)", " 7.1 Investment \\(\\beta\\) using R (Single Index Model) The market model regression can be represented as the following regression. \\[\\begin{equation} R_{i}=\\alpha+\\beta_{i}R_{M}+\\epsilon \\tag{7.2} \\end{equation}\\] "],["7.2-data-preprocessing-1.html", "7.2 Data preprocessing", " 7.2 Data preprocessing Download stock data using Rs quantmod package Convert data to returns Generate some descriptive statistics Some plots Data # Run the following to download and save the data, this should be # done once and when updating the time period library(quantmod) library(pander) library(xts) library(TTR) # download stock BHP = getSymbols(&quot;BHP.AX&quot;, from = &quot;2019-01-01&quot;, to = &quot;2021-07-31&quot;, auto.assign = FALSE) # download index ASX = getSymbols(&quot;^AXJO&quot;, from = &quot;2019-01-01&quot;, to = &quot;2021-07-31&quot;, auto.assign = FALSE) # save both in rds (to be used in the TA chapter) saveRDS(BHP, file = &quot;data/bhp_prices.rds&quot;) saveRDS(ASX, file = &quot;data/asx200.rds&quot;) Convert to returns library(quantmod) library(pander) library(xts) library(TTR) # load data from the saved files (not required if we execute the # chunk above) BHP = readRDS(&quot;data/bhp_prices.rds&quot;) ASX = readRDS(&quot;data/asx200.rds&quot;) # using close prices bhp2 = BHP$BHP.AX.Close asx2 = ASX$AXJO.Close # covert to returns bhp_ret = dailyReturn(bhp2, type = &quot;log&quot;) asx_ret = dailyReturn(asx2, type = &quot;log&quot;) # merge the two with &#39;inner&#39; join to get the same dates data_lm1 = merge.xts(bhp_ret, asx_ret, join = &quot;inner&quot;) # convert to data frame data_lm2 = data.frame(index(data_lm1), data_lm1$daily.returns, data_lm1$daily.returns.1) # change column names colnames(data_lm2) = c(&quot;Date&quot;, &quot;bhp&quot;, &quot;asx&quot;) head(data_lm2) #there are row names which can be removed if required Date bhp asx 2019-01-02 2019-01-02 0.000000000 0.000000000 2019-01-03 2019-01-03 0.000000000 0.013510839 2019-01-04 2019-01-04 -0.008947241 -0.002488271 2019-01-07 2019-01-07 0.029808847 0.011289609 2019-01-08 2019-01-08 0.001162482 0.006873792 2019-01-09 2019-01-09 -0.003782952 0.009721207 library(pastecs) desc_stat1 = stat.desc(data_lm2[, 2:3], norm = TRUE) pander(desc_stat1, caption = &quot;Descriptive Statistics&quot;, split.table = Inf) Descriptive Statistics   bhp asx nbr.val 653 653 nbr.null 9 1 nbr.na 0 0 min -0.1557 -0.102 max 0.1128 0.06766 range 0.2685 0.1697 sum 0.4948 0.2853 median 0 0.001163 mean 0.0007577 0.0004369 SE.mean 0.0007444 0.0005075 CI.mean.0.95 0.001462 0.0009966 var 0.0003618 0.0001682 std.dev 0.01902 0.01297 coef.var 25.1 29.69 skewness -0.4883 -1.443 skew.2SE -2.553 -7.546 kurtosis 9.991 13.32 kurt.2SE 26.16 34.86 normtest.W 0.9198 0.8266 normtest.p 3.885e-18 5.497e-26 "],["7.3-visualisation.html", "7.3 Visualisation", " 7.3 Visualisation library(ggplot2) library(tidyr) p1 = ggplot(data_lm2, aes(asx, bhp)) p1 + geom_point(colour = &quot;brown&quot;) + geom_smooth(method = &quot;lm&quot;) + theme_minimal() + labs(title = &quot;Scatter plot of BHPvsASX and Linear Fit&quot;) p2 = ggplot(data_lm2, aes(Date)) p2 + geom_line(aes(y = bhp, color = &quot;bhp&quot;), size = 1, lty = 1) + geom_line(aes(y = asx, color = &quot;asx&quot;), size = 1, lty = 2) + scale_color_discrete(&quot;Asset&quot;) + theme_minimal() + labs(&quot;Line Chart of Returns&quot;) "],["7.4-regression-analysis-using-lm.html", "7.4 Regression analysis using lm", " 7.4 Regression analysis using lm Use lm to model the SIM lreg1 = lm(formula = bhp ~ asx, data = data_lm2) summary(lreg1) #to generate main results Call: lm(formula = bhp ~ asx, data = data_lm2) Residuals: Min 1Q Median 3Q Max -0.077007 -0.008153 -0.000162 0.007500 0.060409 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.0003046 0.0005270 0.578 0.564 asx 1.0372279 0.0406422 25.521 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.01346 on 651 degrees of freedom Multiple R-squared: 0.5001, Adjusted R-squared: 0.4994 F-statistic: 651.3 on 1 and 651 DF, p-value: &lt; 2.2e-16 pander(lreg1, add.significance.stars = T) #to tabulate Fitting linear model: bhp ~ asx   Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.0003046 0.000527 0.5779 0.5635 asx 1.037 0.04064 25.52 4.215e-100 * * * Using stargazer to print the output library(stargazer) stargazer(lreg1, type = &quot;html&quot;, title = &quot;Regression Results&quot;) Regression Results Dependent variable: bhp asx 1.037*** (0.041) Constant 0.0003 (0.001) Observations 653 R2 0.500 Adjusted R2 0.499 Residual Std. Error 0.013 (df = 651) F Statistic 651.320*** (df = 1; 651) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 Diagnostic Plots par(mfrow = c(2, 2)) plot(lreg1) "],["8-multiple-regression.html", "Topic 8 Multiple Regression", " Topic 8 Multiple Regression Multiple regression extends simple linear regression with more than one (1) predictor variables We have one response variable and multiple independent predictor variable. \\[\\begin{equation} y_{i}=\\alpha+\\beta_{1}x_{1}+\\beta_{2}x_{2}+\\ldots+\\varepsilon_{i} \\tag{8.1} \\end{equation}\\] The estimation process is similar to the univariate case where the additional predictors are added with + operator A multifactor example demonstrates in the next section "],["8.1-fama-french-three-factor-model.html", "8.1 Fama-French Three Factor Model", " 8.1 Fama-French Three Factor Model Fama and French (1992);Fama and French (1993) extended the basic CAPM to include size and book-to-market effects as explanatory factors in explaining the cross-section of stock returns. SMB (Small minus Big) gives the size premium which is the additional return received by investors from investing in companies having a low market capitalization. HML (High minus Low), gives the value premium which is the return provided to investors for investing in companies having high book-to-market values. The three factor Fama-French model is written as: \\[\\begin{equation} r_{A}-r_{F}=+\\beta_{A}(r_{M}-r_{F})+s_{A}SMB+h_{A}HML+\\alpha+e \\#eq:ff1) \\end{equation}\\] Where \\(s_{A}\\) and \\(h_{A}\\) capture the securitys sensitivity to these two additional factors. 8.1.1 Data Preprocessing The three factors daily data is downloaded as a CSV file from the Kenneth French website. Link: https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html AAPL stock prices are downloaded using the quantmod package The following code snippets will pre-process three factor data and stock return data and then combine it in one single # use read.table for text file ff_data = read.csv(&quot;data/F-F_Research_Data_Factors_daily.CSV&quot;, skip = 3) ff_data = na.omit(ff_data) #remove missing values head(ff_data) #date is missing column names X Mkt.RF SMB HML RF 1 19260701 0.10 -0.23 -0.28 0.009 2 19260702 0.45 -0.34 -0.03 0.009 3 19260706 0.17 0.29 -0.38 0.009 4 19260707 0.09 -0.59 0.00 0.009 5 19260708 0.21 -0.38 0.18 0.009 6 19260709 -0.71 0.44 0.58 0.009 colnames(ff_data)[1] = &quot;Date&quot; # convert dates in R date format ff_data$Date = as.Date(strptime(ff_data$Date, format = &quot;%Y%m%d&quot;)) head(ff_data) Date Mkt.RF SMB HML RF 1 1926-07-01 0.10 -0.23 -0.28 0.009 2 1926-07-02 0.45 -0.34 -0.03 0.009 3 1926-07-06 0.17 0.29 -0.38 0.009 4 1926-07-07 0.09 -0.59 0.00 0.009 5 1926-07-08 0.21 -0.38 0.18 0.009 6 1926-07-09 -0.71 0.44 0.58 0.009 Download the data and convert to returns d_aapl = getSymbols(&quot;AAPL&quot;, from = &quot;2019-01-01&quot;, to = &quot;2021-09-30&quot;, auto.assign = F) # select closing prices and covert to log returns aapl = d_aapl$AAPL.Close aapl_ret = dailyReturn(aapl, type = &quot;log&quot;) # convert to data frame aapl_ret2 = fortify.zoo(aapl_ret) #Dates column will be named Index # rename colnames(aapl_ret2) = c(&quot;Date&quot;, &quot;AAPL&quot;) # use merge (can use left_join from dplyr as well) to combine the # stock returns and factor data data_ffex = merge(aapl_ret2, ff_data, by = &quot;Date&quot;) 8.1.2 Regression Analysis The Fama-French regression uses excess returns so first convert Apple returns to excess returns and then fit the model using the lm function # create another column with AAPL-RF data_ffex$AAPL.Rf = data_ffex$AAPL - data_ffex$RF ff_lreg = lm(AAPL.Rf ~ Mkt.RF + SMB + HML, data = data_ffex) A plot function can be used to plot the four regression plots similar to simple regression. par(mfrow = c(2, 2)) plot(ff_lreg) Figure 8.1: Linear Regression Plots There are packages in R which provide functions to export the summary output of a regression model in a LaTeX, HTML or ASCII files. The following code uses function to print the OLS results in ASCII format. The same output can also be exported to an HTML or LaTeX file which can be later used in a word/LaTeX document. stargazer(ff_lreg, summary = T, title = &quot;Fama-French Regression OLS&quot;, type = &quot;html&quot;) Fama-French Regression OLS Dependent variable: AAPL.Rf Mkt.RF 0.013*** (0.0003) SMB -0.003*** (0.001) HML -0.004*** (0.0004) Constant -0.003*** (0.0005) Observations 692 R2 0.677 Adjusted R2 0.676 Residual Std. Error 0.013 (df = 688) F Statistic 481.374*** (df = 3; 688) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 8.1.3 Visualisation The ggeffects package provides good functionality on visualising the marginal effects and adjusted predictions. The predictions generated by a model by varying one independent variable and keeping the others constant. The following example visualises the predictions based on the SMB factor library(ggeffects) mydf = ggpredict(ff_lreg, terms = c(&quot;SMB&quot;)) (p_ff = plot(mydf) + geom_point(data = data_ffex, aes(x = SMB, y = AAPL.Rf), color = &quot;darkblue&quot;)) Figure 8.2: Marginal Effect References "],["9-panel-regression.html", "Topic 9 Panel Regression", " Topic 9 Panel Regression Panel data or longitudinal data is a data structure which contains individuals/variables (e.g., persons, firms, countries, cities etc) observed at several points in time (days, months, years, quarters etc). The dataset GDP_l.RData is an example of panel data where each countrys GDP is recorded over several years in time. load(&quot;data/GDP_l.RData&quot;) # data snapshot GDP_l[c(1:5, 25:29, 241:245), ] Year Country GDP 1 1990 Australia 18247.3946 2 1991 Australia 18837.1893 3 1992 Australia 18599.0012 4 1993 Australia 17658.0794 5 1994 Australia 18080.6975 25 1990 China 314.4310 26 1991 China 329.7491 27 1992 China 362.8081 28 1993 China 373.8003 29 1994 China 469.2128 241 1990 World 4220.6460 242 1991 World 4357.3096 243 1992 World 4591.0928 244 1993 World 4604.2533 245 1994 World 4882.0794 Some visualisation Line Chart library(ggplot2) p1 = ggplot(GDP_l, aes(Year, GDP, group = Country)) p1 + geom_path(aes(color = Country)) + theme_minimal() + theme(legend.position = &quot;top&quot;) Figure 9.1: Panel Data Line Chart Bar Chart p1 + geom_col(aes(fill = Country)) + theme_minimal() + theme(legend.position = &quot;top&quot;) Figure 9.2: Panel Data Bar Chart Bar Chart for each country p1 + geom_col(aes(fill = Country)) + facet_grid(Country ~ .) + theme_minimal() + theme(legend.position = &quot;top&quot;) Figure 9.3: Panel Data Bar Chart Box plot p2 = ggplot(GDP_l, aes(Country, GDP)) p2 + geom_boxplot(aes(fill = Country)) + theme_minimal() + theme(legend.position = &quot;top&quot;) Figure 9.4: Panel Data Box Plot The GDP data here has a balanced panel structure, where all the variables have values for all points in time. This chapter discussed the two basic panel regression models viz, Fixed Effect Model and Random Effect Model for balanced panel data. For an extensive discussion see econometrics textbooks including Baltagi (2005),Wooldridge (2010),Greene (2008) and Stock and Watson (2012). The package plm Croissant and Millo (2008) provides methods for calculating these models, which will be used for in illustrative code. We will use the very popular Grunfeld panel dataset Grunfeld (1958) available in the plm package for demostration which are based on similar examples in Croissant and Millo (2008) and Kleiber and Zeileis (2008). References "],["9.1-fixed-and-random-effects-using-the-plm-package.html", "9.1 Fixed and Random effects using the plm package", " 9.1 Fixed and Random effects using the plm package The argument in is set to for fixed effects model and for a random effects model. The data can be converted to the required panel format using function, which transforms a regular data frame object into a panel data structure. is the main argument in the function which specifies the panel structure, i.e, columns with individual and time variables. # Grunfeld Data representation as per pdata.frame function library(plm) data(Grunfeld) #load data head(Grunfeld) #data snapshot firm year inv value capital 1 1 1935 317.6 3078.5 2.8 2 1 1936 391.8 4661.7 52.6 3 1 1937 410.6 5387.1 156.9 4 1 1938 257.7 2792.2 209.2 5 1 1939 330.8 4313.2 203.4 6 1 1940 461.2 4643.9 207.2 pdata1 = pdata.frame(Grunfeld, index = c(&quot;firm&quot;, &quot;year&quot;)) head(pdata1) firm year inv value capital 1-1935 1 1935 317.6 3078.5 2.8 1-1936 1 1936 391.8 4661.7 52.6 1-1937 1 1937 410.6 5387.1 156.9 1-1938 1 1938 257.7 2792.2 209.2 1-1939 1 1939 330.8 4313.2 203.4 1-1940 1 1940 461.2 4643.9 207.2 "],["9.2-fixed-effects-model.html", "9.2 Fixed Effects Model", " 9.2 Fixed Effects Model We can use the plm package to replicate the following investment equation as considered by Grunfeld (Grunfeld, 1958). \\[\\begin{equation} I_{it}=\\alpha+\\beta_{1}F_{it}+\\beta_{2}C_{it}+\\varepsilon_{it} \\tag{9.1} \\end{equation}\\] The model in equation-1 is a one-way panel regression model which attempts to quantify the dependence of real gross investment (\\(I_{it}\\)) on the real value of the company (\\(F_{it}\\)) and real value of its capital stock (\\(C_{it}\\)). Grunfeld (1958) studied 10 large manufacturing firms from the United States over 20 years (1935-954). A fixed effect estimation can be obtained with the following code # using plm with &#39;within&#39; estimator for fixed effects fe1 = plm(inv ~ value + capital, data = pdata1, model = &quot;within&quot;) # the output can be summarised with summary summary(fe1) Oneway (individual) effect Within Model Call: plm(formula = inv ~ value + capital, data = pdata1, model = &quot;within&quot;) Balanced Panel: n = 10, T = 20, N = 200 Residuals: Min. 1st Qu. Median 3rd Qu. Max. -184.00857 -17.64316 0.56337 19.19222 250.70974 Coefficients: Estimate Std. Error t-value Pr(&gt;|t|) value 0.110124 0.011857 9.2879 &lt; 2.2e-16 *** capital 0.310065 0.017355 17.8666 &lt; 2.2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Total Sum of Squares: 2244400 Residual Sum of Squares: 523480 R-Squared: 0.76676 Adj. R-Squared: 0.75311 F-statistic: 309.014 on 2 and 188 DF, p-value: &lt; 2.22e-16 The summary output for the fitted model object gives details about the fitted object. The individual fixed effects can be obtained using the function , a summary method is also available as shown next # individual fixed effects fixef(fe1) 1 2 3 4 5 6 7 8 -70.2967 101.9058 -235.5718 -27.8093 -114.6168 -23.1613 -66.5535 -57.5457 9 10 -87.2223 -6.5678 # summary summary(fixef(fe1)) Estimate Std. Error t-value Pr(&gt;|t|) 1 -70.2967 49.7080 -1.4142 0.15896 2 101.9058 24.9383 4.0863 6.485e-05 *** 3 -235.5718 24.4316 -9.6421 &lt; 2.2e-16 *** 4 -27.8093 14.0778 -1.9754 0.04969 * 5 -114.6168 14.1654 -8.0913 7.141e-14 *** 6 -23.1613 12.6687 -1.8282 0.06910 . 7 -66.5535 12.8430 -5.1821 5.629e-07 *** 8 -57.5457 13.9931 -4.1124 5.848e-05 *** 9 -87.2223 12.8919 -6.7657 1.635e-10 *** 10 -6.5678 11.8269 -0.5553 0.57933 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 References "],["9.3-random-effects-model.html", "9.3 Random Effects Model", " 9.3 Random Effects Model A random effect model can be estimated by setting the argument to . There are five different methods available for estimation of the variance component (Baltagi (2005)) which can be selected using the argument.  The following output is obtained using the default Swamy-Arora (Swamy and Arora (1972)) random method. # random effect model re1 = plm(inv ~ value + capital, data = pdata1, model = &quot;random&quot;) # summary summary(re1) Oneway (individual) effect Random Effect Model (Swamy-Arora&#39;s transformation) Call: plm(formula = inv ~ value + capital, data = pdata1, model = &quot;random&quot;) Balanced Panel: n = 10, T = 20, N = 200 Effects: var std.dev share idiosyncratic 2784.46 52.77 0.282 individual 7089.80 84.20 0.718 theta: 0.8612 Residuals: Min. 1st Qu. Median 3rd Qu. Max. -177.6063 -19.7350 4.6851 19.5105 252.8743 Coefficients: Estimate Std. Error z-value Pr(&gt;|z|) (Intercept) -57.834415 28.898935 -2.0013 0.04536 * value 0.109781 0.010493 10.4627 &lt; 2e-16 *** capital 0.308113 0.017180 17.9339 &lt; 2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Total Sum of Squares: 2381400 Residual Sum of Squares: 548900 R-Squared: 0.7695 Adj. R-Squared: 0.76716 Chisq: 657.674 on 2 DF, p-value: &lt; 2.22e-16 References "],["9.4-testing.html", "9.4 Testing", " 9.4 Testing 9.4.1 Panel or OLS It is important to test if the panel regression model is signficantly different from the OLS model. In other words, do we need a panel model or OLS model is good enough? The function in the plm package can be used to test a fitted fixed effect model against a fitted OLS model to check which regression is a better choice. # Simple OLS (without the intercept) using pooling ols1 = plm(inv ~ value + capital - 1, data = pdata1, model = &quot;pooling&quot;) # summary of results summary(ols1) Pooling Model Call: plm(formula = inv ~ value + capital - 1, data = pdata1, model = &quot;pooling&quot;) Balanced Panel: n = 10, T = 20, N = 200 Residuals: Min. 1st Qu. Median Mean 3rd Qu. Max. -270.32 -51.32 -23.69 -21.04 -4.51 476.74 Coefficients: Estimate Std. Error t-value Pr(&gt;|t|) value 0.1076384 0.0058256 18.4769 &lt; 2.2e-16 *** capital 0.1832062 0.0242750 7.5471 1.587e-12 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Total Sum of Squares: 9359900 Residual Sum of Squares: 1935600 R-Squared: 0.8113 Adj. R-Squared: 0.81035 F-statistic: 597.659 on 2 and 198 DF, p-value: &lt; 2.22e-16 The above OLS model can now be tested against the fixed effect model to check for the best fit. # Testing for the better model, null: OLS is a better pFtest(fe1, ols1) F test for individual effects data: inv ~ value + capital F = 50.714, df1 = 10, df2 = 188, p-value &lt; 2.2e-16 alternative hypothesis: significant effects Similar to the fixed effect and OLS comparison one can also check if the random effects are needed using one of the available Langrange multiplier tests (Breusch and Pagan (1980) test here) test in function as illustrated below # plmtest using the Breuch-Pagan method plmtest(ols1, type = c(&quot;bp&quot;)) Lagrange Multiplier Test - (Breusch-Pagan) for balanced panels data: inv ~ value + capital - 1 chisq = 727.84, df = 1, p-value &lt; 2.2e-16 alternative hypothesis: significant effects A p-value&lt;0.05 in the above test indicates that the Random Effect model is required. 9.4.2 Fixed Effect or Random Effect The Hausman test (Hausman (1978)) is the standard approach to test for model specification which can be computed using the function in the plm package. # phtest using the fitted models in fe1 and re1 phtest(fe1, re1) Hausman Test data: inv ~ value + capital chisq = 2.3304, df = 2, p-value = 0.3119 alternative hypothesis: one model is inconsistent A p-value&lt;0.05 suggests that the fixed effect model is appropriate so in this case the random effect model should be used. References "],["10-technical-analysis-using-r.html", "Topic 10 Technical Analysis using R", " Topic 10 Technical Analysis using R "],["10.1-technical-analysis-ta-using-r.html", "10.1 Technical Analysis (TA) using R", " 10.1 Technical Analysis (TA) using R Introduction to TA A good resource https://bookdown.org/kochiuyu/Technical-Analysis-with-R/ Technical Analysis is the science of recording, usually in graphic form, the actual history of trading (price changes, volume of transactions, etc.) in a certain stock or in the averages and then deducing from that pictured history the probable future trend(Edwards and Magee (1966)) TA is considered as one of the first Data Science techniques in Finance References "],["10.2-technical-charts.html", "10.2 Technical Charts", " 10.2 Technical Charts Here we will use the quantmod and TTR package to generate some Technical Charts. The data will be downloaded from Yahoo Finance. 10.2.1 Candlesticks and OHLC chart As demonstrated in the previous chapter, R package quantmod can be used to download daily OHLC data (Open, High, Low and Close) with volume. quantmod (https://www.quantmod.com/) also allows to import data from csv files. # Run the following to download and save the data, same as the # regression chapter. Only run if data requires update of different # assets are used library(quantmod) library(pander) # download stock BHP = getSymbols(&quot;BHP.AX&quot;, from = &quot;2019-01-01&quot;, to = &quot;2021-07-31&quot;, auto.assign = FALSE) # download index ASX = getSymbols(&quot;^AXJO&quot;, from = &quot;2019-01-01&quot;, to = &quot;2021-07-31&quot;, auto.assign = FALSE) # save both in rds (to be used in the TA chapter) saveRDS(BHP, file = &quot;data/bhp_prices.rds&quot;) saveRDS(ASX, file = &quot;data/asx200.rds&quot;) library(quantmod) library(pander) BHP = readRDS(&quot;data/bhp_prices.rds&quot;) ASX = readRDS(&quot;data/asx200.rds&quot;) pander(head(BHP), caption = &quot;OHLC Data&quot;, split.table = Inf) OHLC Data Period BHP.AX.Open BHP.AX.High BHP.AX.Low BHP.AX.Close BHP.AX.Volume BHP.AX.Adjusted 2019/01/02 12:00:00 AM 34.28 34.55 33.65 33.68 7290354 26.68 2019/01/03 12:00:00 AM 33.88 34.13 33.56 33.68 8473568 26.68 2019/01/04 12:00:00 AM 33.1 33.39 32.96 33.38 9314513 26.44 2019/01/07 12:00:00 AM 34.5 34.55 34.28 34.39 8553510 27.24 2019/01/08 12:00:00 AM 34.6 34.68 34.43 34.43 8685056 27.28 2019/01/09 12:00:00 AM 34.5 34.59 34.19 34.3 9459525 27.17 chartSeries(BHP, theme = &quot;white&quot;) Figure 10.1: BHP Line Chart with Volume addTA(ASX$AXJO.Close, col = &quot;darkblue&quot;) Figure 10.2: BHP and ASX 10.2.2 Line chart chartSeries(BHP, type = &quot;line&quot;, subset = &quot;2020&quot;, theme = &quot;white&quot;) Figure 5.2: Line Chart 10.2.3 Candlestick chart chartSeries(BHP, type = &quot;candle&quot;, subset = &quot;last 1 month&quot;, theme = &quot;white&quot;) Figure 5.3: Candle Stick chart 10.2.4 Add technical indicators chartSeries(BHP, type = &quot;candle&quot;, subset = &quot;last 6 month&quot;, theme = &quot;white&quot;, TA = c(addSMA(5), addEMA(10))) Figure 5.4: SMA and EMA 10.2.5 Adding indicators sequentially chartSeries(BHP, type = &quot;candle&quot;, subset = &quot;last 6 months&quot;, theme = &quot;white&quot;) Figure 10.3: CandleStick addBBands() Figure 10.4: BOllinger Bands addMomentum(n = 5) Figure 10.5: Momentum "],["11-forecasting-var-using-garch-models.html", "Topic 11 Forecasting VaR using GARCH Models", " Topic 11 Forecasting VaR using GARCH Models "],["11.1-value-at-risk.html", "11.1 Value at Risk", " 11.1 Value at Risk Value at Risk (VaR) is the most widely used market risk measure in financial risk management and it is also used by practitioners such as portfolio managers to account for future market risk. VaR can be defined as loss in market value of an asset over a given time period that is exceeded with a probability \\(\\theta\\). For a time series of returns \\(r_{t}\\) , \\(VaR_{t}\\) would be such that \\[\\begin{equation} P[r_{t}&lt;-VaR_{t}[I_{t-1}]=\\theta \\tag{11.1} \\end{equation}\\] where \\(I_{t-1}\\) represents the information set at time t-1. Despite the appealing simplicity of VaR in its offering of a simple summary of the downside risk of an asset portfolio, there is no single way to calculate it (see Manganelli and Engle (2001) ) for an overview on VaR methods in finance). 1% VaR Convert the prices to returns library(ggplot2) # calculate normal density for the returns # prices to returns bhp2 = BHP$BHP.AX.Close # covert to returns bhp_ret = dailyReturn(bhp2, type = &quot;log&quot;) den1_r = coredata(bhp_ret) den1_bhp = dnorm(x = den1_r, mean = mean(den1_r), sd = sd(den1_r)) data_rd = data.frame(den1_r, den1_bhp) # change column names colnames(data_rd) = c(&quot;x&quot;, &quot;y&quot;) # normal quantile var1 = quantile(den1_r, 0.01) p3 = ggplot(data_rd, aes(x = x, y = y)) + geom_line(size = 2) + geom_vline(xintercept = var1, lty = 2, col = &quot;red&quot;, size = 2) + theme_bw() + labs(title = &quot;Normal Distribution and 1% (Empirical) VaR&quot;) p3 Figure 11.1: 1% VaR In distribution terms, for a distribution F, VaR can be defined as its p-th quantile given by \\[\\begin{equation} VaR_{p}(V_{p})=F^{-1}(1-p) \\tag{11.2} \\end{equation}\\] where \\(F^{-1}\\) is the inverse of the distribution function also called as the quantile function. Hence VaR is easy to calculate once a distribution for the return series can be defined. VaR is the q-th quantile of the distribution of over a time horizon t, which is a well accepted measure of risk in financial management. References "],["11.2-volatility-modelling-forecasting-using-garch.html", "11.2 Volatility Modelling &amp; Forecasting using GARCH", " 11.2 Volatility Modelling &amp; Forecasting using GARCH The Generalised Autoregressive Conditional Heteroskedasticity (GARCH) models (Bollerslev (1986), R. F. Engle (1982); R. Engle (2001)), most popular time series models used for forecasting conditional volatiltiy. These models are conditional heteroskedastic as they take into account the conditional variance in a time series. GARCH models are one of the most widely used models for forecasting financial risk measures like VaR and Conditional VaR in financial risk modelling and management. The GARCH models are a generalised version of ARCH models. A standard ARCH(p) process with p lag terms designed to capture volatility clustering can be written as follows \\[\\begin{equation} \\sigma_{t}^{2}=\\omega+\\sum_{i=1}^{p}\\alpha_{i}Y_{t-i}^{2} \\tag{11.3} \\end{equation}\\] where the return on day t, is \\(Y_{t}=\\sigma_{t}Z_{t}\\) and \\(Z_{t}\\sim i.i.d(0,1)\\), i.e., the innovation in returns are driven by random shocks The GARCH(p,q) model include lagged volatility in an ARCH(p) model to incorporate the impact of historical returns which can be written as follows \\[\\begin{equation} \\sigma_{t}^{2}=\\omega+\\sum_{i=1}^{p}\\alpha_{i}Y_{t-i}^{2}+\\sum_{j=1}^{q}\\beta_{j}\\sigma_{t-j}^{2} \\tag{11.4} \\end{equation}\\] GARCH(1,1) which employs only one lag per order, is the most common version used in empirical research and analysis. References "],["11.3-garch11-to-forecast-var.html", "11.3 GARCH(1,1) to forecast VaR", " 11.3 GARCH(1,1) to forecast VaR One of the most versatile and capable of them is the rugarch package. Here we use previously introduced asx_ret.RData dataset to demonstrate modelling GARCH using the functions and methods av ailable in the rugarch package. Fitting a GARCH model using the rugarch package requires setting the model specification using the ugarchspec function. A GARCH(1,1) model with a contant mean equation mean.model=list(armaOrder=c(0,0) can be specified as follows: library(rugarch) garch_spec = ugarchspec(variance.model = list(model = &quot;sGARCH&quot;, garchOrder = c(1, 1)), mean.model = list(armaOrder = c(0, 0))) The above specification stored in garch_spec can now be used to fit the GARCH(1,1) model to our data. The following code fits the GARCH(1,1) model to BHP log returns using the function and shows the results. fit_garch = ugarchfit(spec = garch_spec, data = bhp_ret) # show the estimates and other diagnostic tests fit_garch *---------------------------------* * GARCH Model Fit * *---------------------------------* Conditional Variance Dynamics ----------------------------------- GARCH Model : sGARCH(1,1) Mean Model : ARFIMA(0,0,0) Distribution : norm Optimal Parameters ------------------------------------ Estimate Std. Error t value Pr(&gt;|t|) mu 0.000929 0.000590 1.5733 0.115648 omega 0.000022 0.000011 1.9495 0.051231 alpha1 0.190305 0.061330 3.1029 0.001916 beta1 0.754359 0.082123 9.1857 0.000000 Robust Standard Errors: Estimate Std. Error t value Pr(&gt;|t|) mu 0.000929 0.000606 1.5338 0.125072 omega 0.000022 0.000018 1.2469 0.212446 alpha1 0.190305 0.142428 1.3361 0.181501 beta1 0.754359 0.157071 4.8027 0.000002 LogLikelihood : 1747.092 Information Criteria ------------------------------------ Akaike -5.3306 Bayes -5.3031 Shibata -5.3306 Hannan-Quinn -5.3199 Weighted Ljung-Box Test on Standardized Residuals ------------------------------------ statistic p-value Lag[1] 0.03488 0.8518 Lag[2*(p+q)+(p+q)-1][2] 0.62736 0.6367 Lag[4*(p+q)+(p+q)-1][5] 1.62615 0.7089 d.o.f=0 H0 : No serial correlation Weighted Ljung-Box Test on Standardized Squared Residuals ------------------------------------ statistic p-value Lag[1] 0.5304 0.4665 Lag[2*(p+q)+(p+q)-1][5] 2.1141 0.5917 Lag[4*(p+q)+(p+q)-1][9] 6.4221 0.2526 d.o.f=2 Weighted ARCH LM Tests ------------------------------------ Statistic Shape Scale P-Value ARCH Lag[3] 0.8395 0.500 2.000 0.3595 ARCH Lag[5] 1.3927 1.440 1.667 0.6210 ARCH Lag[7] 6.1375 2.315 1.543 0.1323 Nyblom stability test ------------------------------------ Joint Statistic: 0.5677 Individual Statistics: mu 0.06882 omega 0.28626 alpha1 0.15743 beta1 0.19222 Asymptotic Critical Values (10% 5% 1%) Joint Statistic: 1.07 1.24 1.6 Individual Statistic: 0.35 0.47 0.75 Sign Bias Test ------------------------------------ t-value prob sig Sign Bias 0.2418 0.8090 Negative Sign Bias 1.3289 0.1844 Positive Sign Bias 0.1904 0.8491 Joint Effect 2.9075 0.4061 Adjusted Pearson Goodness-of-Fit Test: ------------------------------------ group statistic p-value(g-1) 1 20 22.45 0.2623 2 30 31.69 0.3337 3 40 48.63 0.1388 4 50 54.41 0.2761 Elapsed time : 0.1190131 Selected fitted statistics can be obtained using various methods available to the ugarchfit object class par1 = par() #save graphic parameters par(mfrow = c(1, 2)) # generate plots using the which argument Figure-12 1. ACF of # standardised residuals plot(fit_garch, which = 10) # 2. Conditional SD (vs |returns|) plot(fit_garch, which = 3) Figure 11.2: Two Informative Plots for GARCH(1,1) "],["11.4-var-forecasts-using-out-of-sample.html", "11.4 VaR forecasts using out of sample", " 11.4 VaR forecasts using out of sample Lets use Student-t distribution as Financial Returns dont always follow normal distribution # spec2 with student-t distribution spec2 = ugarchspec(variance.model = list(model = &quot;sGARCH&quot;, garchOrder = c(1, 1)), mean.model = list(armaOrder = c(0, 0)), distribution.model = &quot;std&quot;) The rugarch package has a very useful function for estimating moving window models and forecasting VaR. The function provides method for creating rolling forecasts from GARCH models and has various arguments to specify the forecast length (), window size (), model refitting frequency (), a rolling or recursive estimation window () etc. var.t = ugarchroll(spec2, data = bhp_ret, n.ahead = 1, forecast.length = ndays(bhp_ret) - 500, refit.every = 5, window.size = 500, refit.window = &quot;rolling&quot;, calculate.VaR = TRUE, VaR.alpha = c(0.01, 0.05)) We can plot the 1% and 5% VaR forecasts against the actual returns using the following routine. # note the plot method provides four plots with option-4 for the VaR # forecasts 1% Student-t GARCH VaR par(mfrow = c(1, 2)) plot(var.t, which = 4, VaR.alpha = 0.01) # 5% Student-t GARCH VaR plot(var.t, which = 4, VaR.alpha = 0.05) Figure 11.3: Actual Return Vs 1% VaR forecasts Finally backtesting can be obtained using the report method # backtest for VaR forecasts report(var.t, VaR.alpha = 0.05) #default value of alpha is 0.01 VaR Backtest Report =========================================== Model: sGARCH-std Backtest Length: 154 Data: ========================================== alpha: 5% Expected Exceed: 7.7 Actual VaR Exceed: 6 Actual %: 3.9% Unconditional Coverage (Kupiec) Null-Hypothesis: Correct Exceedances LR.uc Statistic: 0.426 LR.uc Critical: 3.841 LR.uc p-value: 0.514 Reject Null: NO Conditional Coverage (Christoffersen) Null-Hypothesis: Correct Exceedances and Independence of Failures LR.cc Statistic: 0.916 LR.cc Critical: 5.991 LR.cc p-value: 0.633 Reject Null: NO "],["12-portfolio-modelling-using-r.html", "Topic 12 Portfolio Modelling using R", " Topic 12 Portfolio Modelling using R This topic provides an introduction to using R for Financial Portfolio Modelling. The chapter will discuss using R for creating multi asset Mean-Variance portfolios. "],["12.1-portfolio-analysis-quick-intro.html", "12.1 Portfolio Analysis (Quick Intro)", " 12.1 Portfolio Analysis (Quick Intro) Investment is a process of maximising wealth. We want to maximise expected return We want to minimise the risk Risk Premium: Difference between returns of a risky asset and risk free asset \\(R_{p}-R_{f}\\) Diversification: Process of diversifying risk in a portfolio. See Chatper-16 from Ruppert (2015) for further details. References "],["12.2-mean-variance-portfolio-important-concepts.html", "12.2 Mean Variance Portfolio: Important concepts", " 12.2 Mean Variance Portfolio: Important concepts Expected Return Expected return for a group of n assets is calculated as \\[\\begin{equation} E(R)=\\sum_{i=1}^{n}p_{i}r_{i} \\tag{12.1} \\end{equation}\\] Portfolio construction should use discrete returns and not logarithmic returns Risk Risk is generally defined as the dispersion of outcomes around the expected value Variance and Standard Deviation measure dispersion The variance of a random variable X is \\[\\begin{equation} \\sigma_{X}^{2}=E(X-E(X)]^{2}=\\sum_{i=1}^{n}p_{i}[x_{i}-E(X)]^{2} \\tag{12.2} \\end{equation}\\] where E(X) is the expected value of a random variable X. This will be financial returns of a security. \\(p_{i}\\) is the probability of the occurrence of \\(x_{i}\\) expression applies to the variance of a single asset where \\(x_{i}=r_{i}\\) Covariance of Returns One of the measures of association between two (or more) random variables. The covariance is positive if the variables tend to move in the same direction, while it is negative if they tend to move in opposite directions. Denoted by \\(\\sigma_{i,j}\\) or \\(Cov(r_{i},r_{j})\\) \\[\\begin{equation} Cov(r_{i},r_{j})=E\\{[r_{i}-E(r_{i})][r_{j}-E(r_{j})]\\}=\\sum_{s=1}^{n}p_{s}\\{[r_{is}-E(r_{i})][r_{js}-E(r_{j})]\\} \\tag{12.3} \\end{equation}\\] Correlation Standardised by standard deviation lies between -1 and 1 \\[\\begin{equation} \\rho_{XY}=\\frac{Cov(r_{i},r_{j})}{\\sigma_{i}\\sigma_{j}} \\tag{12.4} \\end{equation}\\] Within the context of portfolio analysis, diversification can be defined as combining securities with less than perfectly positively correlated returns. In order for the portfolio analyst to construct a diversified portfolio, the analyst must know the correlation coefficients between all securities under consideration. Portfolio Return Its a weighted average \\[\\begin{equation} R_{p}=\\sum_{i=1}^{n}w_{i}r_{i} \\tag{12.5} \\end{equation}\\] Portfolio Risk Not a simple weighted average, correlation has to be accounted for. \\[\\begin{equation} \\sigma_{p}^{2}=E[r_{p}-E(r_{p})]^{2} \\tag{12.6} \\end{equation}\\] where \\(r_{p}=(w_{1}r_{1}+w_{2}r_{2})\\) for a two asset portfolio. After substitution. \\[\\begin{equation} \\sigma_{p}^{2}=E\\{w_{1}r_{1}+w_{2}r_{2}-[w_{1}E(r_{1})+w_{2}E(r_{2})]\\}^{2} \\tag{12.7} \\end{equation}\\] After rearranging and expansion \\[\\begin{equation} \\sigma_{p}^{2}=E\\{w_{1}^{2}[r_{1}-E(r_{1})]^{2}+w_{2}^{2}[r_{2}-E(r_{2})]^{2}+2w_{1}w_{2}[r_{1}-E(r_{1})][r_{2}-E(r_{2})]\\}\\tag{12.8} \\end{equation}\\] \\[\\begin{equation} \\sigma_{p}^{2}=w_{1}^{2}\\sigma_{1}^{2}+w_{2}^{2}\\sigma_{2}^{2}+2w_{1}w_{2}\\sigma_{12} \\tag{12.9} \\end{equation}\\] where \\(\\sigma_{12}=Cov(r_{1},r_{2})\\) Portfolio Risk for N Assets Here we consider 3 assets but the method is generalisable to N assets. Matrix of mean returns \\[\\begin{equation} \\mu=\\left(\\begin{array}{c} \\mu_{a}\\\\ \\mu_{b}\\\\ \\mu_{c} \\end{array}\\right)\\tag{12.10} \\end{equation}\\] Matrix of weights \\[\\begin{equation} w=\\left(\\begin{array}{c} w_{a}\\\\ w_{b}\\\\ w_{c} \\end{array}\\right) \\tag{12.11} \\end{equation}\\] \\(\\sum w=1\\) Variance-covariance matrix \\[\\begin{equation} \\Sigma=\\left(\\begin{array}{ccc} \\sigma_{a}^{2} &amp; \\sigma_{ab} &amp; \\sigma_{ac}\\\\ \\sigma_{ab} &amp; \\sigma_{b}^{2} &amp; \\sigma_{bc}\\\\ \\sigma_{ac} &amp; \\sigma_{bc} &amp; \\sigma_{c}^{2} \\end{array}\\right) \\tag{12.12} \\end{equation}\\] Portfolio expected return \\[\\begin{equation} \\mu_{p}=w^{\\prime}\\mu \\tag{12.12} \\end{equation}\\] Portfolio variance \\[\\begin{equation} \\sigma_{p}^{2}=w^{\\prime}\\Sigma w \\tag{12.13} \\end{equation}\\] See chapter-2 and 3 of Francis and Kim (2013) for further details References "],["12.3-diversification-markowitz-minimum-variance-portfolio.html", "12.3 Diversification &amp; Markowitz Minimum Variance Portfolio", " 12.3 Diversification &amp; Markowitz Minimum Variance Portfolio Chapter-4 from Szylar (2013) provides an easy to follow introduction and futher details. Objective: Rational investors; maximise the expected return for a given risk, and they minimise the risk for a given return. A minimum variance set is the set of all portfolios that have the least volatility for each level of possible expected return. An efficient set (frontier) is the part of the minimum variance frontier that offers the highest expected return for each level of standard deviation. Given the level of risk or standard deviation, investors prefer positions with higher expected return and given the expected return, they prefer the positions of lower risk. Taking this into account, we can determine the minimum variance set. The point where the standard deviation is at its lowest is the global minimum variance portfolio (GMV). Portfolios that lie from the GMV portfolio upwards provide investors with the best riskreturn combinations and thus are the candidates for the optimal portfolio. These portfolios are called the efficient set (frontier); and in order to be on the efficient frontier, the portfolios have to satisfy the following criterion: Given a particular level of standard deviation, the portfolios in the efficient set have the highest attainable expected rate of return. Figure 12.1: Efficient Frontier References "],["12.4-minimum-variance-portfolio.html", "12.4 Minimum Variance Portfolio", " 12.4 Minimum Variance Portfolio Markowitz (H. Markowitz (1952), H. M. Markowitz (1991)) seminal work on Modern Portfolio Theory.  A minimum variance (risk) portfolio \\[\\begin{equation} Minimise\\,\\sigma_{p}^{2}=w^{\\prime}\\Sigma w \\tag{12.14} \\end{equation}\\] subject to \\[\\begin{equation} w^{\\prime}\\mu=\\mu_{p} \\tag{12.15} \\end{equation}\\] and \\[\\begin{equation} w^{\\prime}1=1 \\tag{12.16} \\end{equation}\\] The above constraints apply to long only portfolios. Summation of weights can be different for long and short combination Portfolio can be formed for a target return and minimum risk or just for minimum risk or with maximum return with minimum risk (mean variance) With different stocks and asset types, individual weight limits can be imposed. 12.4.1 Efficient Weights for Two Assets For a portfolio with Two Risk Assets, the mean variance portfolio weights are calculated as \\[\\begin{equation} w_{1}=\\frac{\\sigma_{2}^{2}-Cov(r_{1},r_{2})}{\\sigma_{1}^{2}+\\sigma_{2}^{2}-2Cov(r_{1},r_{2})}\\\\\\\\ w_{2}=1-w_{1} \\tag{12.17} \\end{equation}\\] \\[\\begin{equation} w_{2}=1-w_{1} \\tag{12.18} \\end{equation}\\] The above is derived after taking the first derivative of the portfolio variance w.r.t the weight of asset 1,\\(w_{1}\\). Set that derivative equal to zero and solve for \\(w_{1}\\). \\[\\begin{equation} \\frac{\\partial\\sigma_{p}^{2}}{\\partial w_{1}}=\\frac{\\partial}{\\partial w_{1}}\\left[w_{1}^{2}\\sigma_{1}^{2}+(1-w_{1})^{2}\\sigma_{2}^{2}+2w_{1}(1-w_{1})\\sigma_{12}\\right] \\tag{12.19} \\end{equation}\\] 12.4.2 Portfolio with N Risky Assets Optimisation using Quadratic Programming-Brief overview To recap: Given a target return \\(\\mu_{p}\\), the efficient portfolio minimises \\[\\begin{equation} Minimise\\,\\sigma_{p}^{2}=w^{\\prime}\\Sigma w \\tag{12.20} \\end{equation}\\] subject to \\[w^{\\prime}\\mu=\\mu_{p}\\] and \\[w^{\\prime}1=1\\] Quadratic Programming is used to minimise a quadratic objective function subject to linear constraints.We focus on the implementation on the technique for optimisation not the mathematical details. Students are expected to learn how to use the method in R In applications to portfolio optimization, the objective function is the variance of the portfolio return. The objective function is a function of N variables, such as the weights of N assets, that are denoted by an N × 1 vector x. Suppose that the quadratic objective function to be minimized is \\[\\begin{equation} \\frac{1}{2}x^{\\prime}Dx-d^{\\,\\prime}x \\tag{12.21} \\end{equation}\\] where D is an \\(N\\mathtt{X}N\\) matrix and d is an \\(N\\mathtt{x}1\\) vector. The factor of 1/2 is used make the optimisation consistent with R Two types of linear constraints on x ; inequality and equality Inequality constraint \\[A_{neq}^{\\prime}x\\geq b_{neq}\\] Equality \\[A_{eq}^{\\prime}x=b_{eq}\\] To apply quadratic programming \\(x=w\\) \\(D=2\\Sigma\\) d is an \\(N\\mathtt{x}1\\) vector of zeros so that (12.21) is \\(w^{\\prime}\\Sigma w\\), the return variance of the portfolio.  R package quadprog has a function called solve.QP to achieve this.  R package tseries has portfolio.optim function using the solve.QP to make it easier References "],["12.5-using-r-to-construct-multi-asset-portfolio.html", "12.5 Using R to Construct Multi-Asset Portfolio", " 12.5 Using R to Construct Multi-Asset Portfolio This section will contruct a five (5) stock portfolio using prices downloaded from Yahoo finance The exercise first creates random weight portfolios then uses two packages PortfolioAnalytics and fPortfolio for risk-return efficient portfolios. 12.5.1 Data Download 2 year daily prices for BHP.AX, ANZ.AX, WOW.AX, TLS.AX and CSL.AX Stocks are chosen from different sectors library(quantmod) library(TTR) # create a vector of stocks s1 = c(&quot;BHP.AX&quot;, &quot;ANZ.AX&quot;, &quot;WOW.AX&quot;, &quot;TLS.AX&quot;, &quot;CSL.AX&quot;) # download prices and create returns from Adjusted Prices data1 = lapply(s1, FUN = function(x) { ROC(Ad(getSymbols(x, from = &quot;2019-07-01&quot;, to = &quot;2021-06-30&quot;, auto.assign = FALSE)), type = &quot;discrete&quot;) * 100 }) #%returns # convert to data frame ret1 = as.data.frame(do.call(merge, data1)) # change columna names colnames(ret1) = gsub(&quot;.AX.Adjusted&quot;, &quot;&quot;, colnames(ret1)) # remove the first row of missing values ret1 = ret1[-1, ] # add dates column ret1 = data.frame(Date = as.Date(row.names(ret1)), ret1) row.names(ret1) = NULL # save the dataframe (not necessarily required, only for # reproducibility) saveRDS(ret1, file = &quot;data/port_ret.Rds&quot;) Plot the data library(pander) library(ggplot2) library(tidyr) library(ggthemes) ret1 = readRDS(&quot;data/port_ret.Rds&quot;) # overview pander(head(ret1), split.table = Inf) Date BHP ANZ WOW TLS CSL 2019-07-02 0.8637 -1.45 -0.2726 -0.5222 0.6696 2019-07-03 0.09515 -0.287 2.703 1.312 -0.3486 2019-07-04 -0.5466 1.331 0.7096 0 1.781 2019-07-05 -1.338 -0.03551 0.734 0.5181 1.596 2019-07-08 -1.768 -0.9591 -1.137 -0.7732 -1.491 2019-07-09 1.159 -0.6815 0.6486 0.7792 -0.5874 # convert to long ret_long = pivot_longer(ret1, cols = -c(Date), values_to = &quot;Return&quot;, names_to = &quot;Stock&quot;) # plot port_p1 = ggplot(ret_long, aes(Date, Return, color = Stock)) + geom_path(stat = &quot;identity&quot;) + facet_grid(Stock ~ .) + theme_minimal() + labs(x = &quot;Date&quot;, y = &quot;Returns&quot;) port_p1 #covid crisis period is evident in the plot Figure 12.2: Simple returns 12.5.2 Portfolios with random weights Calculate the mean returns Calculate variance-covariance matrix Create series of random weights Create series of rerturns and risks Create plot np1 = 200 #number of portfolios ret2 = ret1[, -1] #excluding dates mu1 = colMeans(ret2) #mean returns na1 = ncol(ret2) #number of assets varc1 = cov(ret2) riskp1 = NULL #vector to store risk retp1 = NULL #vector to store returns # using loops here (not aiming for efficiency but demonstration) for (i in 1:np1) { w = diff(c(0, sort(runif(na1 - 1)), 1)) # random weights r1 = t(w) %*% mu1 #matrix multiplication sd1 = t(w) %*% varc1 %*% w retp1 = rbind(retp1, r1) riskp1 = rbind(riskp1, sd1) } # create a data frame of risk and return d_p1 = data.frame(Ret = retp1, Risk = riskp1) # simple plot plot(d_p1$Risk, d_p1$Ret, xlab = &quot;Risk&quot;, ylab = &quot;Return&quot;, main = &quot;Frontier Portfolios&quot;, col = &quot;blue&quot;) Figure 12.3: Random Portfolios Use ggplot2 library(ggplot2) # first layer p1 = ggplot(d_p1, aes(Risk, Ret, colour = Ret)) # scatter plot p1 = p1 + geom_point() # scatter plot with density and identified port risk return (highest # lowest returns and min risk) p1 + geom_point() + geom_hline(yintercept = c(max(d_p1$Ret), median(d_p1$Ret), min(d_p1$Ret)), colour = c(&quot;darkgreen&quot;, &quot;darkgray&quot;, &quot;darkred&quot;), size = 1) + geom_vline(xintercept = d_p1[(d_p1$Risk == min(d_p1$Risk)), ][, 2]) + labs(colour = &quot;Portfolio Return&quot;, x = &quot;Portfolio Risk&quot;, y = &quot;Portfolio Return&quot;, title = &quot;Random Feasible Portfolios&quot;) + theme_bw() Figure 12.4: Random Portfolios "],["12.6-efficient-minimum-variance-portfolio-using-r-packages.html", "12.6 Efficient (Minimum Variance) Portfolio using R packages", " 12.6 Efficient (Minimum Variance) Portfolio using R packages Rs PortfolioAnalytics package provides various tools for portfolio anlaytics including minimum variance portfolio optimisation Create random portfolios based on risk (Std. Dev.) and reward (mean return) Full investment: Allocate in all the assets (minimum 0 and maximum 1 weight) Long only: Only buy, no short position (positive weights only) library(PortfolioAnalytics) # initialise with asset names uses time series data data_p2 = zoo(ret1[, -1], order.by = as.Date(ret1$Date)) # create specification port = portfolio.spec(assets = c(colnames(data_p2))) # add long only constraint port = add.constraint(portfolio = port, type = &quot;long_only&quot;) # add full investment contraint port = add.constraint(portfolio = port, type = &quot;full_investment&quot;) # objective: manimise risk port_rnd = add.objective(portfolio = port, type = &quot;risk&quot;, name = &quot;StdDev&quot;) # objective: maximise return port_rnd = add.objective(portfolio = port_rnd, type = &quot;return&quot;, name = &quot;mean&quot;) # 1. optimise random portfolios rand_p = optimize.portfolio(R = data_p2, portfolio = port_rnd, optimize_method = &quot;random&quot;, trace = TRUE, search_size = 1000) # plot chart.RiskReward(rand_p, risk.col = &quot;StdDev&quot;, return.col = &quot;mean&quot;, chart.assets = TRUE) #also plots the equally weighted portfolio Figure 12.5: Mean-Variance Portfolios Optimise for minimum risk Minimise the level of Standard Deviation for the portfolio port_msd = add.objective(portfolio = port, type = &quot;risk&quot;, name = &quot;StdDev&quot;) minvar1 = optimize.portfolio(R = data_p2, portfolio = port_msd, optimize_method = &quot;ROI&quot;, trace = TRUE) minvar1 *********************************** PortfolioAnalytics Optimization *********************************** Call: optimize.portfolio(R = data_p2, portfolio = port_msd, optimize_method = &quot;ROI&quot;, trace = TRUE) Optimal Weights: BHP ANZ WOW TLS CSL 0.1437 0.0735 0.3122 0.4543 0.0164 Objective Measure: StdDev 1.32 # plot plot(minvar1, risk.col = &quot;StdDev&quot;, main = &quot;Mean Variance Portfolio&quot;, chart.assets = TRUE) Figure 12.6: Mean Variance Portfolio Risk/Return # efficient frontier minvar_ef = create.EfficientFrontier(R = data_p2, portfolio = port_msd, type = &quot;mean-StdDev&quot;) chart.EfficientFrontier(minvar_ef, match.col = &quot;StdDev&quot;, type = &quot;l&quot;, tangent.line = FALSE, chart.assets = TRUE) Figure 12.7: Efficient Frontier The fPortfolio package also provides functions to conduct the optimisation An ebook is available with further details on the fPortfolio package see here https://www.rmetrics.org/ebooks-portfolio library(fPortfolio) data_p2 = as.timeSeries(data_p2) pspec = portfolioSpec() #initial specification setNFrontierPoints(pspec) = 500 #random portfolios for the efficient frontier eff_front2 = portfolioFrontier(data_p2, constraints = &quot;LongOnly&quot;) #strategy plot(eff_front2, c(1, 2, 4, 5, 6)) Figure 12.8: Efficient frontier plot (fPortfolio) Another function (allows to add different points and lines from other portfolios) Also plots of the weights in different portfolios tailoredFrontierPlot(eff_front2, sharpeRatio = FALSE, risk = &quot;Sigma&quot;) Figure 12.9: Efficient Frontier # weights weightsPlot(eff_front2) Figure 12.10: Weights Plot 12.6.1 Minimum Variance and portfolio for a given level of return Minimum variance Uses previous constraints (Long Only) (minvar2 = minvariancePortfolio(data_p2)) Title: MV Minimum Variance Portfolio Estimator: covEstimator Solver: solveRquadprog Optimize: minRisk Constraints: LongOnly Portfolio Weights: BHP ANZ WOW TLS CSL 0.1437 0.0735 0.3122 0.4543 0.0164 Covariance Risk Budgets: BHP ANZ WOW TLS CSL 0.1437 0.0735 0.3122 0.4543 0.0164 Target Returns and Risks: mean Cov CVaR VaR 0.0382 1.3204 3.2805 2.0550 Description: Mon Oct 25 17:30:39 2021 by user: RMachine Target Return mu = mean(colMeans(data_p2)) #target return setTargetReturn(pspec) = mu (eff_port2 = efficientPortfolio(data_p2, pspec)) Title: MV Efficient Portfolio Estimator: covEstimator Solver: solveRquadprog Optimize: minRisk Constraints: LongOnly Portfolio Weights: BHP ANZ WOW TLS CSL 0.2364 0.0342 0.3425 0.2360 0.1510 Covariance Risk Budgets: BHP ANZ WOW TLS CSL 0.2563 0.0326 0.3439 0.1994 0.1677 Target Returns and Risks: mean Cov CVaR VaR 0.0521 1.3608 3.4011 1.9698 Description: Mon Oct 25 17:30:39 2021 by user: RMachine 12.6.2 Portfolio with Box Constraints We can add box constraints to setup minimum weights so that we invest in each stock in the portfolio. This can be achieved using both packages. Here we demonstrate using the fPortfolio package. See help(box_constraints) for PortfolioAnalytics package pspec2 = portfolioSpec() setNFrontierPoints(pspec) = 500 boxconstraints = c(&quot;minW[1:5]=0.1&quot;, &quot;maxW[1:5]=1&quot;) #for minimum asset weights and maximum asset weights eff_front3 = portfolioFrontier(data_p2, spec = pspec2, constraints = boxconstraints) eff_front3 Title: MV Portfolio Frontier Estimator: covEstimator Solver: solveRquadprog Optimize: minRisk Constraints: minW maxW Portfolio Points: 5 of 24 Portfolio Weights: BHP ANZ WOW TLS CSL 1 0.1000 0.1000 0.1248 0.5752 0.1000 6 0.1160 0.1000 0.2713 0.4127 0.1000 12 0.1900 0.1000 0.3253 0.2611 0.1236 18 0.2323 0.1000 0.3382 0.1292 0.2003 24 0.2558 0.1000 0.1000 0.1000 0.4442 Covariance Risk Budgets: BHP ANZ WOW TLS CSL 1 0.0922 0.0983 0.1022 0.6022 0.1052 6 0.1136 0.1040 0.2662 0.4063 0.1099 12 0.2038 0.1069 0.3268 0.2270 0.1354 18 0.2495 0.1038 0.3270 0.0972 0.2225 24 0.2495 0.0917 0.0711 0.0669 0.5209 Target Returns and Risks: mean Cov CVaR VaR 1 0.0346 1.3525 3.2604 2.0459 6 0.0410 1.3280 3.3011 2.0827 12 0.0488 1.3482 3.3697 1.9793 18 0.0566 1.3977 3.4827 2.0566 24 0.0644 1.5223 3.7541 2.2943 Description: Mon Oct 25 17:30:39 2021 by user: RMachine plot(eff_front2, c(1, 2, 4, 5, 6)) Figure 12.11: Frontier Plot (box constraints) (minvar3 = minvariancePortfolio(data = data_p2, spec = pspec2, constraints = boxconstraints)) Title: MV Minimum Variance Portfolio Estimator: covEstimator Solver: solveRquadprog Optimize: minRisk Constraints: minW maxW Portfolio Weights: BHP ANZ WOW TLS CSL 0.1179 0.1000 0.2728 0.4093 0.1000 Covariance Risk Budgets: BHP ANZ WOW TLS CSL 0.1158 0.1041 0.2680 0.4021 0.1099 Target Returns and Risks: mean Cov CVaR VaR 0.0412 1.3280 3.3025 2.0801 Description: Mon Oct 25 17:30:41 2021 by user: RMachine "],["12.7-evaluating-portfoios-risk-adjusted-performance.html", "12.7 Evaluating Portfoios: Risk Adjusted Performance", " 12.7 Evaluating Portfoios: Risk Adjusted Performance 12.7.1 Sharpe Ratio The Sharpe Ratio compares the excess return of a portfolio relative to the risk-free rate with the portfolios standard deviation. \\[\\begin{equation} Sharpe_{p}=\\frac{E(r_{p})-r_{f}}{\\sigma_{p}} \\tag{12.22} \\end{equation}\\] The portfolio with highest Sharpe Ratio is known as the Tangency Portfolio 12.7.2 Roys Safety Ratio Roys Safety First (SF) Ratio makes a slight modification to the Sharpe Ratio. Specifically, instead of using the risk-free rate, Roys SF Ratio instead uses a target or minimum acceptable return to calculate the excess return \\[\\begin{equation} RoysSF_{p}=\\frac{E(r_{p})-MAR}{\\sigma_{p}} \\tag{12.23} \\end{equation}\\] 12.7.3 Treynors Ratio The Treynor Ratio modifies the denominator in the Sharpe Ratio to use beta instead of standard deviation in the denominator. That is \\[\\begin{equation} Treynor_{p}=\\frac{E(r_{p})-r_{f}}{\\beta_{p}} \\tag{12.24} \\end{equation}\\] 12.7.4 Sharpe Ratio (Portfolio) in R Assume an rf=0.02 The code uses the fPortfolio package The ratio can also be easily calculated using base R # create specification pspec3 = portfolioSpec() setRiskFreeRate(pspec3) = 0.02 setNFrontierPoints(pspec3) = 50 # create efficient frontier eff_front4 = portfolioFrontier(data_p2, spec = pspec3, constraints = &quot;LongOnly&quot;) # find the tangency port tgport1 = tangencyPortfolio(data = data_p2, spec = pspec3, constraints = &quot;LongOnly&quot;) # create frontier plot frontierPlot(eff_front4, pch = 1, auto = FALSE, xlim = c(0, 2.5), ylim = c(0, 0.085)) #custom x and y limits minvariancePoints(object = eff_front4, auto = FALSE, col = &quot;red&quot;, pch = 20) #add min variance port tangencyPoints(object = eff_front4, col = &quot;blue&quot;, pch = 20) #add tangency port tangencyLines(object = tgport1, col = &quot;darkblue&quot;) #add tangency portfolio line grid() Figure 12.12: Tangency Portfolio (Sharpe Portfolio) print(tgport1) Title: MV Tangency Portfolio Estimator: covEstimator Solver: solveRquadprog Optimize: minRisk Constraints: LongOnly Portfolio Weights: BHP ANZ WOW TLS CSL 0.3996 0.0000 0.0758 0.0000 0.5247 Covariance Risk Budgets: BHP ANZ WOW TLS CSL 0.3853 0.0000 0.0464 0.0000 0.5683 Target Returns and Risks: mean Cov CVaR VaR 0.0741 1.6352 4.0740 2.4228 Description: Mon Oct 25 17:30:41 2021 by user: RMachine # weights plot weightsPie(tgport1,box=FALSE) # weightedReturnsPie(tgport1,box=FALSE) "],["13-machine-learning-using-r-introduction-to-data-splitting-sampling-resampling.html", "Topic 13 Machine Learning using R-Introduction to Data Splitting, Sampling &amp; Resampling", " Topic 13 Machine Learning using R-Introduction to Data Splitting, Sampling &amp; Resampling Some references: Boehmke and Greenwell (2019), Hastie et al. (2013) and Lantz (2019) Data science is a superset of Machine learning, data mining, and related subjects. It extensively covers the complete process starting from data loading until production. Machine learning is a scientific discipline that is concerned with the design and development of algorithms that allow computers to evolve behaviours based on empirical data, such as from sensor data or databases. (Wikipedia) Primary goal of a ML implementation is to develop a general purpose algorithm that solves a practical and focused problem. Important aspects in the process include data, time, and space requirements. The goal of a learning algorithm is to produce a result that is a rule and is as accurate as possible. References "],["13.1-machine-learning-process-quick-intro.html", "13.1 Machine Learning Process (Quick Intro)", " 13.1 Machine Learning Process (Quick Intro) There are three main phases in an ML process Training Phase: Training Data is used to train the model by using expected output with the input. Output is the learning model. Validation/Test Phase: Measuring the validity and fit of the model. How good is the model? Uses validation dataset, which can be a subset of the initial dataset. Application Phase: Run the model with real world data to generate results "],["13.2-data-splitting-sampling.html", "13.2 Data Splitting: Sampling", " 13.2 Data Splitting: Sampling Training set: data examples that are used to learn or build a classifier. Validation set: data examples that are verified against the built classifier and can help tune the accuracy of the output. Testing set: data examples that help assess the performance of the classifier. Machine Learning requires the data to be split in mainly three categories. The first two (training and validation sets) are usually from the portion of the data selected to build the model on. Two most common ways of splitting data Overfitting: Building a model that memorizes the training data, and does not generalize well to new data. Generalisation error &gt; Training error. Simple random sampling Stratified sampling Typical recommendations for splitting your data into training-test splits include 60% (training)40% (testing), 70%30%, or 80%20%. Its is good to keep the following points in mind: Spending too much in training (e.g., &gt;80%) wont allow us to get a good assessment of predictive performance. We may find a model that fits the training data very well, but is not generalizable (overfitting). Sometimes too much spent in testing (&gt;40% ) wont allow us to get a good assessment of model parameters. "],["13.3-random-sampling.html", "13.3 Random Sampling", " 13.3 Random Sampling This section explores some ways to conduct random sampling in R. Simple Random sampling does not control for any data attributes. 13.3.1 Base R The following code uses the BHP close prices to perform a simple random sample using base R sample function. # import data and select the closing prices library(xts) #required as the data was saved as an xts object d_bhp = readRDS(&quot;data/bhp_prices.rds&quot;) d_bhp = d_bhp$BHP.AX.Close #select close prices d_bhp = data.frame(Date = as.Date(index(d_bhp)), Price = coredata(d_bhp)) #convert to data frame (for convenience not necessaily required) head(d_bhp) Date BHP.AX.Close 1 2019-01-02 33.68 2 2019-01-03 33.68 3 2019-01-04 33.38 4 2019-01-07 34.39 5 2019-01-08 34.43 6 2019-01-09 34.30 # use base R function set.seed(999) #seed is set for reproducibility as the random number generator picks a different seed each time unless specified idx1 = sample(1:nrow(d_bhp), round(nrow(d_bhp) * 0.7)) #70% # training set train1 = d_bhp[idx1, ] # testing set (remaining data) test1 = d_bhp[-idx1, ] Note: Sampling is a random process and random number generator produces different results on each execution. Setting a seed in the code keeps it consistent allows for reproducibility. Visualise the distribution of training and testing set library(ggplot2) p1 = ggplot(train1, aes(x = BHP.AX.Close)) + geom_density(trim = TRUE, aes(color = &quot;Training&quot;), size = 1) + geom_density(data = test1, aes(x = BHP.AX.Close, color = &quot;Testing&quot;), trim = TRUE, size = 1, linetype = 2) (p1 = p1 + theme_bw() + labs(color = &quot;Density&quot;, title = &quot;Random Sampling (Base R)&quot;, x = &quot;BHP Prices&quot;, y = &quot;Density&quot;)) Figure 13.1: Training/Testing using Base R 13.3.2 Using the caret package We can use the caret package to create the training and testing samples set.seed(999) library(caret) idx2 = createDataPartition(d_bhp$BHP.AX.Close, p = 0.7, list = FALSE) train2 = d_bhp[idx2, ] test2 = d_bhp[-idx2, ] # plot p2 = ggplot(train2, aes(x = BHP.AX.Close)) + geom_density(trim = TRUE, aes(color = &quot;Training&quot;), size = 1) + geom_density(data = test2, aes(x = BHP.AX.Close, color = &quot;Testing&quot;), trim = TRUE, size = 1, linetype = 2) (p2 = p2 + theme_bw() + labs(color = &quot;Density&quot;, title = &quot;Random Sampling (Caret package)&quot;, x = &quot;BHP Prices&quot;, y = &quot;Density&quot;)) Figure 13.2: Training/Testing using caret 13.3.3 Using the rsample package Provides an easy to use method for sampling which is slightly different but can be more convenient due to the function names set.seed(999) library(rsample) idx3 = initial_split(d_bhp, prop = 0.7) #creates an object to further use for training and testing train3 = training(idx3) test3 = testing(idx3) # plot p3 = ggplot(train3, aes(x = BHP.AX.Close)) + geom_density(trim = TRUE, aes(color = &quot;Training&quot;), size = 1) + geom_density(data = test3, aes(x = BHP.AX.Close, color = &quot;Testing&quot;), trim = TRUE, size = 1, linetype = 2) (p3 = p3 + theme_bw() + labs(color = &quot;Density&quot;, title = &quot;Random Sampling (rsample package)&quot;, x = &quot;BHP Prices&quot;, y = &quot;Density&quot;)) Figure 13.3: Training/Testing using rsample Combine all three plots Notice some differences between the three due to the method used. library(gridExtra) grid.arrange(p1, p2, p3, nrow = 1) Figure 13.4: Splitting using three methods "],["13.4-stratified-sampling.html", "13.4 Stratified Sampling", " 13.4 Stratified Sampling Random sampling does not control for the proportion of the target variables in the sampling process. Machine Learning methods may require similar proportions in the training and testing set to avoid imbalanced response variable. Stratified sampling is able to obtain similar distributions for the response variable. It can be applied to both, classification or regression problems. With a continuous response variable, stratified sampling will segment Y (response variable) into quantiles and randomly sample from each. Consequently, this will help ensure a balanced representation of the response distribution in both the training and test sets. rsample package can be used to create stratified samples. The following code demonstrates that on a dataset suitable for classification. set.seed(999) data(&quot;GermanCredit&quot;) #credit risk data from the caret package idx4 = initial_split(GermanCredit, prop = 0.7, strata = &quot;Class&quot;) #Class is the binary response variable train4 = training(idx4) test4 = testing(idx4) # check the proportion of outcomes prop.table(table(train4$Class)) #training set Bad Good 0.3004292 0.6995708 prop.table(table(test4$Class)) #testing set Bad Good 0.2990033 0.7009967 The above training and testing set will have the same proportion of the class values. "],["13.5-resampling.html", "13.5 Resampling", " 13.5 Resampling The last section demonstrated creation of training and testing dataset for the ML process. Its not adviced to use the test set to assess model performance during the training phase. How do we assess the generalization performance of the model? One option is to assess an error metric based on the training data. Unfortunately, this leads to biased results as some models can perform very well on the training data but not generalize well to a new data set. Use a validation approach, which involves splitting the training set further to create two parts: a training set and a validation set (or holdout set). We can then train our model(s) on the new training set and estimate the performance on the validation set. Validation using single holdout set can be highly variable and produce unreliable results. Solution: Resampling Methods Resampling Methods: Alternative approach, allowing for repeated model fits to parts of the training data and test it on other parts. Two most commonly used methods are: K-fold Cross validation Bootstrapping Lets briefly discuss these two. "],["13.6-k-fold-cross-validation.html", "13.6 K-fold Cross Validation", " 13.6 K-fold Cross Validation k-fold cross-validation (aka k-fold CV) is a resampling method that randomly divides the training data into k groups (aka folds) of approximately equal size. The model is fit on k1 folds and then the remaining fold is used to compute model performance. This procedure is repeated k times; each time, a different fold is treated as the validation set. This process results in k estimates of the generalization error. The k-fold CV estimate is computed by averaging the k test errors, providing us with an approximation of the error we might expect on unseen data. K-fold CV in R rsample and caret package provide functionality to create k-fold CV set.seed(999) # using rsample package cv1 = vfold_cv(d_bhp[2], v = 10) #v is the number of folds cv1 #10 folds # 10-fold cross-validation # A tibble: 10 x 2 splits id &lt;list&gt; &lt;chr&gt; 1 &lt;split [588/66]&gt; Fold01 2 &lt;split [588/66]&gt; Fold02 3 &lt;split [588/66]&gt; Fold03 4 &lt;split [588/66]&gt; Fold04 5 &lt;split [589/65]&gt; Fold05 6 &lt;split [589/65]&gt; Fold06 7 &lt;split [589/65]&gt; Fold07 8 &lt;split [589/65]&gt; Fold08 9 &lt;split [589/65]&gt; Fold09 10 &lt;split [589/65]&gt; Fold10 # using caret package cv2 = createFolds(d_bhp$BHP.AX.Close, k = 10) cv2$Fold01 #gives indices for 10 folds [1] 28 33 38 42 44 52 71 85 97 119 121 122 125 126 135 160 161 168 191 [20] 194 197 201 222 227 231 239 241 246 265 284 292 298 302 310 319 331 336 344 [39] 353 362 368 384 386 387 402 403 406 430 466 471 484 500 532 533 539 554 567 [58] 570 581 585 610 612 633 641 642 "],["13.7-cv-for-time-series-data.html", "13.7 CV for time series data", " 13.7 CV for time series data k-fold cv is random and doesnt preserve the order of the dataset The order is important in time series applications which are common in financial data science. One method is to use Time series cross validation. Hyndman and Athanasopoulos (2019) https://otexts.com/fpp3/ section 5.9 provides detailed introduction to the technique. Basic idea The corresponding training set consists only of observations that occurred prior to the observation that forms the test set. No future observations can be used in constructing the forecast. Since it is not possible to obtain a reliable forecast based on a small training set, the earliest observations are not considered as test sets. CV for time series in R There are several ways to create time series samples in R. The caret package provides a function to accomplish this as well. The following creates time slices with a moving window of 500 days (initial window size) with a test period of 100 days (horizon) The function returns a list with two elements, train and test with training sample and testing sample d_bhp2 = xts(d_bhp$BHP.AX.Close, order.by = d_bhp$Date) cv_ts = createTimeSlices(d_bhp2, initialWindow = 500, horizon = 100, fixedWindow = TRUE) References "],["13.8-bootstrapping.html", "13.8 Bootstrapping", " 13.8 Bootstrapping Bootstrapping is any test or metric that uses random sampling with replacement, and falls under the broader class of resampling methods. Bootstrapping assigns measures of accuracy (bias, variance, confidence intervals, prediction error, etc.) to sample estimates. (Wikipedia) This means that, after a data point is selected for inclusion in the subset, its still available for further selection. A bootstrap sample is the same size as the original data set from which it was constructed. Since observations are replicated in bootstrapping, there tends to be less variability in the error measure compared with k-fold CV. Bootsrapping in R rsample package provides bootstraps function for bootstrapping a sample boot1 = bootstraps(d_bhp[2], times = 10) boot1 # Bootstrap sampling # A tibble: 10 x 2 splits id &lt;list&gt; &lt;chr&gt; 1 &lt;split [654/241]&gt; Bootstrap01 2 &lt;split [654/253]&gt; Bootstrap02 3 &lt;split [654/240]&gt; Bootstrap03 4 &lt;split [654/226]&gt; Bootstrap04 5 &lt;split [654/223]&gt; Bootstrap05 6 &lt;split [654/242]&gt; Bootstrap06 7 &lt;split [654/241]&gt; Bootstrap07 8 &lt;split [654/240]&gt; Bootstrap08 9 &lt;split [654/258]&gt; Bootstrap09 10 &lt;split [654/249]&gt; Bootstrap10 "],["14-logistic-regression-k-nearest-neighbour-knn-for-classification.html", "Topic 14 Logistic Regression &amp; K-Nearest Neighbour (kNN) for Classification", " Topic 14 Logistic Regression &amp; K-Nearest Neighbour (kNN) for Classification This topic provides a brief introduction to two ML methods used for classification; Logistic Regression and K-Nearest Neighbour. The topic includes an example on predicting the direction of stock price movement to illustrate the application of R for these two models. "],["14.1-logistic-regression.html", "14.1 Logistic Regression", " 14.1 Logistic Regression Logistic Regression belongs to the class of generalised linear models (glms)generalised linear models (glms) Used to model data with a dichotomous response variable. Logistic regression models the conditional probability of the response variable rather than its value. A logit link function, defined as \\(logit\\,p=log[p/(1-p)]\\), is used to transform the output of a linear regression to be suitable for probabilities. A linear model for these transformed probabilities can be setup as \\[\\begin{equation} logit\\,p=\\beta_{0}+\\beta_{1}x_{1}+\\beta_{2}x_{2}+\\ldots\\beta_{k}x_{x} \\tag{14.1} \\end{equation}\\] R provides the glm function for modelling generalised linear models including the logistic regression model We will use the caret package to model logistic regression later in this topic. See Hastie et al. (2013) and Boehmke and Greenwell (2019) for further details on logistic regression. References "],["14.2-k-nearest-neighbour.html", "14.2 K-Nearest Neighbour", " 14.2 K-Nearest Neighbour Nearest Neighbour classification (Quick Introduction) Idea: Things that are alike are likely to have similar properties. ML uses this principle to classify data by placing it in the same category or similar, or nearest neighbours. Computers apply a human like ability to recall past experiences to make conclusions about current circumstances. Nearest neighbour methods are based on a simple idea, yet they can be extremely powerful. In general, well suited for classification tasks. If a concept is difficult to define, but you know it when you see it, then nearest neighbours might be appropriate. If the data is noisy and thus no clear distinction exists among the groups, nearest neighbour algorithms may struggle to identify the class boundaries. k-NN Algorithm K-nearest neighbor (KNN) is a very simple algorithm in which each observation is predicted based on its similarity to other observations. The letter k is a variable term implying that any number of nearest neighbors could be used. After choosing k, the algorithm requires a training dataset made up of examples that have been classified into several categories, as labeled by a nominal variable Unlike most methods in ML, KNN is a memory-based algorithm and cannot be summarized by a closed-form model. Measuring similarity in distance Locating the nearest neighbours require a distance function; measuring similarity between two instances k-NN uses Euclidean distance, a distance between two points. Euclidean distance is measured as the shortest direct route. For p and q instances (to be compared) with n features \\[\\begin{equation} dist(p,q)=\\sqrt{(p_{1}-q_{1})^{2}+(p_{2}-q_{2})^{2}+\\ldots+(p_{n}-q_{n})^{2}} \\tag{14.2} \\end{equation}\\] Choosing k The balance between overfitting and underfitting the training data is a problem known as the bias-variance tradeoff. Choosing a large k reduces the impact or variance caused by noisy data but can bias the learner such that it runs the risk of ignoring small, but important patterns. There is no general rule about the best k as it depends greatly on the nature of the data. A grid search can be performed to assess the best value of k Grid Search: A grid search is an automated approach to searching across many combinations of hyperparameter values. A grid search would predefine a candidate set of values for k (e.g., k=1,2,,j ) and perform a resampling method (e.g., k-fold CV) to estimate which k value generalizes the best to unseen data. The caret package can be used to implement the knn method. The next section demonstrate an application of Logistic Regression and KNN using an example from the applied finance domain. "],["14.3-forecasting-stock-price-movement-using-ml.html", "14.3 Forecasting Stock Price Movement using ML", " 14.3 Forecasting Stock Price Movement using ML Topic-7 provided an introduction to using R for creating Technical Indicators for traded assets. This example uses selected indicators to predict the direction of the price movement, i.e., if the prices will be higher or lower the next day. Various studies have evaluated ML methods to predict the price direction or return, for example, Shynkevich et al. (2017) examined indicators like SMA, EMA etc on various forecast horizons and window lengths. The example will use the closing prices of a stock (BHP in this case) to create the following Technical Indicators Lag of 5 Days Simple Moving Average : Moving averages are the simplest among all indicators: they show the average price level for you on a rolling basis. Lag 5 Days Exponential Moving Average: The difference between SMA and EMA is that SMA weighs all candles equally whereas EMA gives exponential weights  hence the name: it overweights recent days to previous ones. MACD: Moving Average Convergence Divergence: Uses EMA of 26 and 12 days. Designed to reveal changes in strength, direction, momentum and duration of a trend in stock price. Lag of Log Returns Lag of RSI: Momentum oscillator that measures the speed and change of price movements. Ranges from zero to 100. Can be used to identify general trends. The response (dependent) variable is constructed based on the previous day(s) prices. Here we use the following indicator: \\[\\begin{equation} 1 \\rightarrow P_{t}\\geq P_{t-5}\\\\ 0\\, otherswise \\tag{14.3} \\end{equation}\\] 14.3.1 Data &amp; Indicators library(quantmod) library(TTR) library(xts) #required as the data was saved as an xts object d_bhp = readRDS(&quot;data/bhp_prices.rds&quot;) d_bhp = d_bhp$BHP.AX.Close #select close prices colnames(d_bhp) = &quot;Price&quot; # SMA sma5 = lag(SMA(d_bhp, n = 5)) #notice the use of the lag function to take lagged values # EMA ema5 = lag(EMA(d_bhp, n = 5)) # MACD macd1 = lag(MACD(d_bhp)) # RSI rsi1 = lag(RSI(d_bhp, 5)) # log returns ret1 = lag(dailyReturn(d_bhp, type = &quot;log&quot;)) # price director indicator dir = ifelse(d_bhp$Price &gt;= lag(d_bhp$Price, 5), 1, 0) #direction variable compared to 5 day before price Combine all the indicators and response variable in a data frame d_ex1 = cbind(dir, ret1, sma5, ema5, macd1, rsi1) # change column names colnames(d_ex1) = c(&quot;Direction&quot;, &quot;Ret&quot;, &quot;SMA&quot;, &quot;EMA&quot;, &quot;MACD&quot;, &quot;Signal&quot;, &quot;RSI&quot;) 14.3.2 Visualise the data Using the quantmod package chartSeries(d_bhp, theme = &quot;white&quot;, name = &quot;BHP Closing Prices and Indicators&quot;) addTA(d_ex1[, 1], col = 1, legend = &quot;Direction&quot;) #Direction addTA(d_ex1[, -1], on = NA, col = rainbow(6), legend = as.character(colnames(d_ex1[, -1]))) Figure 14.1: Direction and Technical Indicators The above graph can be improved using ggplot2 library(tidyr) library(ggplot2) # create a dataset and convert data to long d_plot = merge.xts(d_bhp, d_ex1) # remove NAs and then convert to long d_plot = na.omit(d_plot) # convert to dataframe d_plot = data.frame(Date = index(d_plot), coredata(d_plot)) d_plot_long = pivot_longer(d_plot, -c(Date, Direction), values_to = &quot;value&quot;, names_to = &quot;Indicator&quot;) # change direction to a factor d_plot_long$Direction = as.factor(d_plot_long$Direction) (p2_ex = ggplot(d_plot_long, aes(Date, value, color = Indicator)) + geom_path(stat = &quot;identity&quot;) + facet_grid(Indicator ~ ., scale = &quot;free&quot;) + theme_minimal()) Figure 14.2: Indicators and Prices (ggplot2) The above is a line chart of indicators and prices. We can also create a box plot to visualise the variance in the values relative to the direction p2_ex = ggplot(d_plot_long, aes(value, Indicator, fill = Direction)) + geom_boxplot() p2_ex + theme_minimal() + labs(title = &quot;TA Indicators vs Price Direction&quot;) + scale_fill_manual(name = &quot;Price Direction&quot;, values = c(&quot;orange&quot;, &quot;lightblue&quot;)) Figure 14.3: Box Plot of Indicators Some differences per category can be noticed 14.3.3 Using Logistic Regression 70:30 data split Time series sampling Stratified sampling will not keep the time order and hence its avoided # remove NAs d_ex1 = na.omit(d_ex1) # convert to data frame d_ex1 = as.data.frame(d_ex1) # convert direction to a factor for classification d_ex1$Direction = as.factor(d_ex1$Direction) idx1 = c(1:round(nrow(d_ex1) * 0.7)) #create index for first 70% values to be in the testing set d_train1 = d_ex1[idx1, ] #training set d_test1 = d_ex1[-idx1, ] #testing set Training Setup Time series cross validate for resampling: 250 day window 30 days for prediction (this can be changed) Data preprocessing is conducted to normalise the scale of the values glm method with binomial family for the binary classification library(caret) set.seed(999) # control cntrl1 = trainControl(method = &quot;timeslice&quot;, initialWindow = 250, horizon = 30, fixedWindow = TRUE) # preprocesing prep1 = c(&quot;center&quot;, &quot;scale&quot;) # logistic regression logit_ex1 = train(Direction ~ ., data = d_train1, method = &quot;glm&quot;, family = &quot;binomial&quot;, trControl = cntrl1, preProcess = prep1) logit_ex1 #final model accuracy Generalized Linear Model 434 samples 6 predictor 2 classes: &#39;0&#39;, &#39;1&#39; Pre-processing: centered (6), scaled (6) Resampling: Rolling Forecasting Origin Resampling (30 held-out with a fixed window) Summary of sample sizes: 250, 250, 250, 250, 250, 250, ... Resampling results: Accuracy Kappa 0.7324731 0.4658076 summary(logit_ex1$finalModel) #summary of the final model Call: NULL Deviance Residuals: Min 1Q Median 3Q Max -2.43299 -0.63007 -0.04745 0.57603 2.49565 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -0.05772 0.13429 -0.430 0.667 Ret -0.82494 0.18720 -4.407 1.05e-05 *** SMA -20.20397 3.70217 -5.457 4.83e-08 *** EMA 20.05707 3.67374 5.460 4.77e-08 *** MACD 0.50610 0.73203 0.691 0.489 Signal -0.94914 0.68541 -1.385 0.166 RSI 2.14092 0.30037 7.128 1.02e-12 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 601.61 on 433 degrees of freedom Residual deviance: 348.17 on 427 degrees of freedom AIC: 362.17 Number of Fisher Scoring iterations: 5 Variable importance In the case of multiple predictor variables, we want to understand which variable is the most influential in predicting the response variable. library(vip) vip(logit_ex1, geom = &quot;point&quot;) + theme_minimal() Figure 14.4: Variable Importance Predictive Accuracy ML models should be checked for predictive accuracy on the test set caret provides predict function to create predictions These predictions can be assessed based on the confusion matrix Confusion Matrix - When applying classification models, we often use a confusion matrix to evaluate certain performance measures. A confusion matrix is simply a matrix that compares actual categorical levels (or events) to the predicted categorical levels. Prediction of the right level; refer to this as a true positive. Prediction of a level or event that did not happen this is called a false positive (i.e. we predicted a customer would redeem a coupon and they did not). Alternatively, when we do not predict a level or event and it does happen that this is called a false negative (i.e. a customer that we did not predict to redeem a coupon does). Lets predict using the final model and assess using the confusion matrix. pred1 = predict(logit_ex1, newdata = d_test1) #prediction on the test data confusionMatrix(data = pred1, reference = d_test1$Direction) Confusion Matrix and Statistics Reference Prediction 0 1 0 52 26 1 16 92 Accuracy : 0.7742 95% CI : (0.7073, 0.8321) No Information Rate : 0.6344 P-Value [Acc &gt; NIR] : 2.959e-05 Kappa : 0.5279 Mcnemar&#39;s Test P-Value : 0.1649 Sensitivity : 0.7647 Specificity : 0.7797 Pos Pred Value : 0.6667 Neg Pred Value : 0.8519 Prevalence : 0.3656 Detection Rate : 0.2796 Detection Prevalence : 0.4194 Balanced Accuracy : 0.7722 &#39;Positive&#39; Class : 0 The model provides decent accuracy which is higher than the No Information Rate and also statistically significant. There are other analyses which can be conducted on logistic regression, for example, ROC (Receiver Operating Characteristic) curve analysis for the Area Under the Curve. 14.3.4 Using K-NN This section will implement the previous model using k-nearest neigbour algorithm using the caret package Then we will use the same split as the logistic regression We are selecting the model based on Accuracy &amp; Kappa Accuracy: Accuracy measures the overall correctness of the classifier \\(\\frac{(TN+TP)}{Total}\\). Objective: maximise Kappa (Cohens Kappa): Its like Accuracy but it is normalised at the baseline of random chance on the dataset. It compares observed accuracy to expected accuracy. A grid search is used to search for an optimal value for k set.seed(999) grid1 = expand.grid(k = seq(1, 10, by = 2)) #to search from k=1 to k=10 knn_ex1 = train(Direction ~ ., data = d_train1, method = &quot;knn&quot;, tuneGrid = grid1, trControl = cntrl1, preProcess = prep1) View the fit plot(knn_ex1) #may suggest using a wider grid Figure 14.5: Grid plot knn_ex1 k-Nearest Neighbors 434 samples 6 predictor 2 classes: &#39;0&#39;, &#39;1&#39; Pre-processing: centered (6), scaled (6) Resampling: Rolling Forecasting Origin Resampling (30 held-out with a fixed window) Summary of sample sizes: 250, 250, 250, 250, 250, 250, ... Resampling results across tuning parameters: k Accuracy Kappa 1 0.5898925 0.1554741 3 0.6066667 0.1834679 5 0.6419355 0.2500894 7 0.6690323 0.3194890 9 0.6686022 0.3209926 Accuracy was used to select the optimal model using the largest value. The final value used for the model was k = 7. Variable importance cant be obtained here as KNN does not produce a model Predictive Accuracy Lets predict using the final model and assess using the confusion matrix. pred2 = predict(knn_ex1, newdata = d_test1) #prediction on the test data confusionMatrix(data = pred2, reference = d_test1$Direction) Confusion Matrix and Statistics Reference Prediction 0 1 0 50 50 1 18 68 Accuracy : 0.6344 95% CI : (0.5608, 0.7037) No Information Rate : 0.6344 P-Value [Acc &gt; NIR] : 0.5330295 Kappa : 0.2833 Mcnemar&#39;s Test P-Value : 0.0001704 Sensitivity : 0.7353 Specificity : 0.5763 Pos Pred Value : 0.5000 Neg Pred Value : 0.7907 Prevalence : 0.3656 Detection Rate : 0.2688 Detection Prevalence : 0.5376 Balanced Accuracy : 0.6558 &#39;Positive&#39; Class : 0 A comparison betweek Logistic Regression and KNN can be made based on their predictive accuracy. The two models can also be compared based on their resampling results. The random number seeds should be initialised to the same value for resampling comparison. The following code compares the model fits. resamp1 = resamples(list(logit = logit_ex1, knn = knn_ex1)) summary(resamp1) Call: summary.resamples(object = resamp1) Models: logit, knn Number of resamples: 155 Accuracy Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s logit 0.4666667 0.6666667 0.7333333 0.7324731 0.8000000 0.9000000 0 knn 0.4333333 0.5666667 0.6666667 0.6690323 0.7666667 0.8666667 0 Kappa Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s logit 0.1208791 0.3333333 0.4666667 0.4658076 0.5838144 0.7887324 0 knn -0.1860465 0.1558442 0.3243243 0.3194890 0.4666667 0.7235023 0 bwplot(resamp1, metric = &quot;Accuracy&quot;) Figure 14.6: Resample Accuracy comparison The graph provides a comparison of the overall accuracy of the model (not the predictions). References "],["15-decision-trees-using-r.html", "Topic 15 Decision Trees using R", " Topic 15 Decision Trees using R Some references: Boehmke and Greenwell (2019), Hastie et al. (2013) and Lantz (2019) In this section we discuss tree based methods for classification. Tree-based models are a class of non-parametric algorithms that work by partitioning the feature space into a number of smaller (non-overlapping) regions with similar response values using a set of splitting rules. These involve stratifying or segmenting the predictor space into a number of simple regions. Typically use the mean or the mode of the training observations in the region to which it belongs. Since the set of splitting rules used to segment the predictor space can be summarized in a tree, these types of approaches are known as decision tree methods. Classification and Regression Tree (CART) (Breiman et al. (1984)) is the most well-known decision tree algorithm. CART uses binary recursive partitioning: Each split depends on the split above (before) it. A basic decision tree partitions the training data into homogeneous subgroups (i.e., groups with similar response values) and then fits a simple constant in each subgroup (e.g., the mean of the within group response values for regression). The subgroups (also called nodes) are formed recursively using binary partitions formed by asking simple yes-or-no questions about each feature. This is done a number of times until a suitable stopping criteria is satisfied (e.g., a maximum depth of the tree is reached). After all the partitioning has been done, the model predicts the output based on the average response values for all observations that fall in that subgroup (regression problem), or the class that has majority representation (classification problem) Root node: First subgroup Terminal node or Leaf node: Final subgroup Internal node: Other subgroups between root and terminal node Branches: Connection between the nodes We will apply the CART method on a Credit Risk Example Two types of risks are associated with the banks decision If the applicant is a good credit risk, i.e. is likely to repay the loan, then not approving the loan to the person results in a loss of business to the bank If the applicant is a bad credit risk, i.e. is not likely to repay the loan, then approving the loan to the person results in a financial loss to the bank This analysis is an example and is not exhaustive list of methods available for data description, visualisation or ML using R. References "],["15.1-import-data-and-pre-processing.html", "15.1 Import Data and Pre-processing", " 15.1 Import Data and Pre-processing Data discription is here https://onlinecourses.science.psu.edu/stat508/book/export/html/796 20 variables Status of existing checking account. Duration in month Credit history Purpose Credit amount Savings account/bonds Present employment since Installment rate in percentage of disposable income Personal status and sex Other debtors / guarantors Present residence since Property Age in years Other installment plans Housing Number of existing credits at this bank Job Number of people being liable to provide maintenance for Telephone foreign worker data_cr = read.csv(&quot;data/german_credit.csv&quot;) # preliminary analysis descriptive and visual str(data_cr) &#39;data.frame&#39;: 1000 obs. of 21 variables: $ Creditability : int 1 1 1 1 1 1 1 1 1 1 ... $ Account.Balance : int 1 1 2 1 1 1 1 1 4 2 ... $ Duration.of.Credit..month. : int 18 9 12 12 12 10 8 6 18 24 ... $ Payment.Status.of.Previous.Credit: int 4 4 2 4 4 4 4 4 4 2 ... $ Purpose : int 2 0 9 0 0 0 0 0 3 3 ... $ Credit.Amount : int 1049 2799 841 2122 2171 2241 3398 1361 1098 3758 ... $ Value.Savings.Stocks : int 1 1 2 1 1 1 1 1 1 3 ... $ Length.of.current.employment : int 2 3 4 3 3 2 4 2 1 1 ... $ Instalment.per.cent : int 4 2 2 3 4 1 1 2 4 1 ... $ Sex...Marital.Status : int 2 3 2 3 3 3 3 3 2 2 ... $ Guarantors : int 1 1 1 1 1 1 1 1 1 1 ... $ Duration.in.Current.address : int 4 2 4 2 4 3 4 4 4 4 ... $ Most.valuable.available.asset : int 2 1 1 1 2 1 1 1 3 4 ... $ Age..years. : int 21 36 23 39 38 48 39 40 65 23 ... $ Concurrent.Credits : int 3 3 3 3 1 3 3 3 3 3 ... $ Type.of.apartment : int 1 1 1 1 2 1 2 2 2 1 ... $ No.of.Credits.at.this.Bank : int 1 2 1 2 2 2 2 1 2 1 ... $ Occupation : int 3 3 2 2 2 2 2 2 1 1 ... $ No.of.dependents : int 1 2 1 2 1 2 1 2 1 1 ... $ Telephone : int 1 1 1 1 1 1 1 1 1 1 ... $ Foreign.Worker : int 1 1 1 2 2 2 2 2 1 1 ... # remove NA data_cr = na.omit(data_cr) # quick summary summary(data_cr) Creditability Account.Balance Duration.of.Credit..month. Min. :0.0 Min. :1.000 Min. : 4.0 1st Qu.:0.0 1st Qu.:1.000 1st Qu.:12.0 Median :1.0 Median :2.000 Median :18.0 Mean :0.7 Mean :2.577 Mean :20.9 3rd Qu.:1.0 3rd Qu.:4.000 3rd Qu.:24.0 Max. :1.0 Max. :4.000 Max. :72.0 Payment.Status.of.Previous.Credit Purpose Credit.Amount Min. :0.000 Min. : 0.000 Min. : 250 1st Qu.:2.000 1st Qu.: 1.000 1st Qu.: 1366 Median :2.000 Median : 2.000 Median : 2320 Mean :2.545 Mean : 2.828 Mean : 3271 3rd Qu.:4.000 3rd Qu.: 3.000 3rd Qu.: 3972 Max. :4.000 Max. :10.000 Max. :18424 Value.Savings.Stocks Length.of.current.employment Instalment.per.cent Min. :1.000 Min. :1.000 Min. :1.000 1st Qu.:1.000 1st Qu.:3.000 1st Qu.:2.000 Median :1.000 Median :3.000 Median :3.000 Mean :2.105 Mean :3.384 Mean :2.973 3rd Qu.:3.000 3rd Qu.:5.000 3rd Qu.:4.000 Max. :5.000 Max. :5.000 Max. :4.000 Sex...Marital.Status Guarantors Duration.in.Current.address Min. :1.000 Min. :1.000 Min. :1.000 1st Qu.:2.000 1st Qu.:1.000 1st Qu.:2.000 Median :3.000 Median :1.000 Median :3.000 Mean :2.682 Mean :1.145 Mean :2.845 3rd Qu.:3.000 3rd Qu.:1.000 3rd Qu.:4.000 Max. :4.000 Max. :3.000 Max. :4.000 Most.valuable.available.asset Age..years. Concurrent.Credits Min. :1.000 Min. :19.00 Min. :1.000 1st Qu.:1.000 1st Qu.:27.00 1st Qu.:3.000 Median :2.000 Median :33.00 Median :3.000 Mean :2.358 Mean :35.54 Mean :2.675 3rd Qu.:3.000 3rd Qu.:42.00 3rd Qu.:3.000 Max. :4.000 Max. :75.00 Max. :3.000 Type.of.apartment No.of.Credits.at.this.Bank Occupation No.of.dependents Min. :1.000 Min. :1.000 Min. :1.000 Min. :1.000 1st Qu.:2.000 1st Qu.:1.000 1st Qu.:3.000 1st Qu.:1.000 Median :2.000 Median :1.000 Median :3.000 Median :1.000 Mean :1.928 Mean :1.407 Mean :2.904 Mean :1.155 3rd Qu.:2.000 3rd Qu.:2.000 3rd Qu.:3.000 3rd Qu.:1.000 Max. :3.000 Max. :4.000 Max. :4.000 Max. :2.000 Telephone Foreign.Worker Min. :1.000 Min. :1.000 1st Qu.:1.000 1st Qu.:1.000 Median :1.000 Median :1.000 Mean :1.404 Mean :1.037 3rd Qu.:2.000 3rd Qu.:1.000 Max. :2.000 Max. :2.000 Lets convert data types to as all of them are factors but some of them should be used as numeric sapply(data_cr, class) Creditability Account.Balance &quot;integer&quot; &quot;integer&quot; Duration.of.Credit..month. Payment.Status.of.Previous.Credit &quot;integer&quot; &quot;integer&quot; Purpose Credit.Amount &quot;integer&quot; &quot;integer&quot; Value.Savings.Stocks Length.of.current.employment &quot;integer&quot; &quot;integer&quot; Instalment.per.cent Sex...Marital.Status &quot;integer&quot; &quot;integer&quot; Guarantors Duration.in.Current.address &quot;integer&quot; &quot;integer&quot; Most.valuable.available.asset Age..years. &quot;integer&quot; &quot;integer&quot; Concurrent.Credits Type.of.apartment &quot;integer&quot; &quot;integer&quot; No.of.Credits.at.this.Bank Occupation &quot;integer&quot; &quot;integer&quot; No.of.dependents Telephone &quot;integer&quot; &quot;integer&quot; Foreign.Worker &quot;integer&quot; # Keep Duration.of.Credit..month and Credit.Amount as numeric rest to # factors id = c(1, 2, 4, 5, 7:21) data_cr[id] = lapply(data_cr[id], as.factor) "],["15.2-visualisation-some-features.html", "15.2 Visualisation some features", " 15.2 Visualisation some features Create some visualisation to visualise data characteristics Box plots 15.1 for Duration of Credit and Credit Amount library(ggplot2) library(gridExtra) # copy of data for plots data_cr_p = data_cr # set theme for the session theme_set(theme_bw()) p1 = ggplot(data_cr, aes(Duration.of.Credit..month., fill = Creditability)) + geom_boxplot() p2 = ggplot(data_cr_p, aes(Credit.Amount, Creditability, fill = Creditability)) + geom_boxplot() grid.arrange(p1, p2, ncol = 1) Figure 15.1: Box Plots See the relationship between Purpose, Sex/status and Creditability 15.2 p3 = ggplot(data_cr_p, aes(Purpose)) + geom_bar(aes(fill = Creditability), stat = &quot;count&quot;, position = &quot;dodge&quot;) p4 = ggplot(data_cr_p, aes(Sex...Marital.Status)) + geom_bar(aes(fill = Creditability), stat = &quot;count&quot;, position = &quot;dodge&quot;) grid.arrange(p3, p4, ncol = 2) Figure 15.2: Bar plots "],["15.3-creating-training-and-testing-set-and-control.html", "15.3 Creating Training and Testing Set and Control", " 15.3 Creating Training and Testing Set and Control Use rsample package and stratified sampling Using mutliple cross validation for resampling library(rsample) library(caret) set.seed(999) #for reproducibility (can pick your own seed, but keep it consistent) idx = initial_split(data = data_cr, prop = 0.8, strata = &quot;Creditability&quot;) d_train1 = training(idx) d_test1 = testing(idx) prop.table(table(d_train1$Creditability)) 0 1 0.3 0.7 prop.table(table(d_test1$Creditability)) 0 1 0.3 0.7 cntrl1 = trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 2) #using repeated cross validate (repeating twice) "],["15.4-train-the-model-on-the-training-set.html", "15.4 Train the model on the training set", " 15.4 Train the model on the training set Use rpart package and caret package The following block may results in slightly different results on individual computers. The fitted models are in data folder set.seed(999) #for reproducibility cv_tree1 = train(Creditability ~ ., data = d_train1, trControl = cntrl1, method = &quot;rpart&quot;, tuneLength = 20, parms = list(split = &quot;gini&quot;)) #using gini index cv_tree2 = train(Creditability ~ ., data = d_train1, trControl = cntrl1, method = &quot;rpart&quot;, tuneLength = 20, parms = list(split = &quot;information&quot;)) #using information The fitted models were saved afterwards as cv_tree1_gini.Rdata and cv_tree2_info.Rdata Loading the fitted models to analyse set.seed(999) load(&quot;data/cv_tree1_gini.Rdata&quot;) load(&quot;data/cv_tree2_info.Rdata&quot;) # model fit 1 cv_tree1 CART 802 samples 20 predictor 2 classes: &#39;0&#39;, &#39;1&#39; No pre-processing Resampling: Cross-Validated (10 fold, repeated 2 times) Summary of sample sizes: 721, 722, 722, 722, 722, 721, ... Resampling results across tuning parameters: cp Accuracy Kappa 0.000000000 0.6969830 0.2512465 0.001747106 0.6988580 0.2543412 0.003494213 0.7063349 0.2628946 0.005241319 0.7206867 0.2861502 0.006988425 0.7200540 0.2777666 0.008735532 0.7200617 0.2727974 0.010482638 0.7244367 0.2758523 0.012229744 0.7219444 0.2739317 0.013976851 0.7256636 0.2964605 0.015723957 0.7262886 0.2940382 0.017471064 0.7262886 0.2936297 0.019218170 0.7238272 0.2881274 0.020965276 0.7225926 0.2839374 0.022712383 0.7225926 0.2839374 0.024459489 0.7151466 0.2534916 0.026206595 0.7145370 0.2595461 0.027953702 0.7070833 0.2284884 0.029700808 0.7014583 0.2203101 0.031447914 0.6983565 0.1916670 0.033195021 0.6902701 0.1609005 Accuracy was used to select the optimal model using the largest value. The final value used for the model was cp = 0.01747106. # model fit 2 cv_tree2 CART 802 samples 20 predictor 2 classes: &#39;0&#39;, &#39;1&#39; No pre-processing Resampling: Cross-Validated (10 fold, repeated 2 times) Summary of sample sizes: 721, 722, 722, 722, 721, 722, ... Resampling results across tuning parameters: cp Accuracy Kappa 0.000000000 0.6963735 0.2470114 0.001747106 0.7056867 0.2628565 0.003494213 0.7050849 0.2532738 0.005241319 0.7056867 0.2530103 0.006988425 0.7144290 0.2708436 0.008735532 0.7144290 0.2715985 0.010482638 0.7200540 0.2804530 0.012229744 0.7237654 0.2784178 0.013976851 0.7275154 0.2885322 0.015723957 0.7231404 0.2833342 0.017471064 0.7237731 0.2843818 0.019218170 0.7243981 0.2782958 0.020965276 0.7237886 0.2732868 0.022712383 0.7237886 0.2732868 0.024459489 0.7225386 0.2708625 0.026206595 0.7144136 0.2456549 0.027953702 0.7082022 0.2211492 0.029700808 0.7082022 0.2211492 0.031447914 0.7013580 0.1774435 0.033195021 0.6976080 0.1496378 Accuracy was used to select the optimal model using the largest value. The final value used for the model was cp = 0.01397685. plot(cv_tree1) #parameter search using gini plot(cv_tree2) #parameter search using information "],["15.5-prediction-and-accuracy.html", "15.5 Prediction and Accuracy", " 15.5 Prediction and Accuracy Using testing set and confusion matrix to compare accuracies 15.5.1 Model-1 pred1 = predict(cv_tree1, newdata = d_test1) confusionMatrix(pred1, d_test1$Creditability) Confusion Matrix and Statistics Reference Prediction 0 1 0 33 10 1 27 130 Accuracy : 0.815 95% CI : (0.7541, 0.8663) No Information Rate : 0.7 P-Value [Acc &gt; NIR] : 0.0001479 Kappa : 0.5207 Mcnemar&#39;s Test P-Value : 0.0085289 Sensitivity : 0.5500 Specificity : 0.9286 Pos Pred Value : 0.7674 Neg Pred Value : 0.8280 Prevalence : 0.3000 Detection Rate : 0.1650 Detection Prevalence : 0.2150 Balanced Accuracy : 0.7393 &#39;Positive&#39; Class : 0 15.5.2 Model-2 pred2 = predict(cv_tree2, newdata = d_test1) confusionMatrix(pred2, d_test1$Creditability) Confusion Matrix and Statistics Reference Prediction 0 1 0 32 9 1 28 131 Accuracy : 0.815 95% CI : (0.7541, 0.8663) No Information Rate : 0.7 P-Value [Acc &gt; NIR] : 0.0001479 Kappa : 0.5157 Mcnemar&#39;s Test P-Value : 0.0030846 Sensitivity : 0.5333 Specificity : 0.9357 Pos Pred Value : 0.7805 Neg Pred Value : 0.8239 Prevalence : 0.3000 Detection Rate : 0.1600 Detection Prevalence : 0.2050 Balanced Accuracy : 0.7345 &#39;Positive&#39; Class : 0 "],["15.6-feature-interpretation.html", "15.6 Feature Interpretation", " 15.6 Feature Interpretation We can use the vip package to create a graph of feature importance library(vip) vip(cv_tree1, num_features = 20, geom = &quot;point&quot;) Figure 15.3: Variable importance based on the total reduction in MSE "],["15.7-tree-visualisation.html", "15.7 Tree visualisation", " 15.7 Tree visualisation Lets visualise tree using the first model Use rpart.plot library(rpart.plot) rpart.plot(cv_tree1$finalModel, main = &quot;Decision Tree using Gini Index&quot;, tweak = 1.4) Figure 15.4: Decision Tree using rpart.plot "],["16-text-mining-using-r.html", "Topic 16 Text Mining using R", " Topic 16 Text Mining using R We Facebook users have been building a treasure lode of big data that government and corporate researchers have been mining to predict and influence what we buy and for whom we vote. We have been handing over to them vast quantities of information about ourselves and our friends, loved ones and acquaintances-Douglas Rushkoff "],["16.1-introduction-to-text-mining.html", "16.1 Introduction to Text Mining", " 16.1 Introduction to Text Mining Text mining, also referred to as text data mining, roughly equivalent to text analytics, refers to the process of deriving high-quality information from text. High-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning. Welbers et al. (2017) provides a mild introduction to Text Analytics using R Text mining has gained momentum and is used in analytics worldwide Sentiment Analysis Predicting Stock Market and other Financial Applications Customer influence News Analytics Social Network Analysis Customer Service and Help Desk 16.1.1 Text Data Text data is ubiquitous in social media analytics. Traditional media, social media, survey data, and numerous other sources. Twitter, Facebook, Surveys, Reported Data (Incident Reports) Massive quantity of text in the modern information age. The mounting availability of and interest in text data has been the development of a variety of statistical approaches for analysing this data. 16.1.2 Generic Text Mining System Following figure demonstrates a general text mining system (Source: Feldman and Sanger (2007)) knitr::include_graphics(&quot;fig-2.png&quot;) Figure 16.1: Generic Text Mining System 16.1.3 Data pre-processing in Text Mining Following figure summarises main steps in a typical data pre-processing stage of text mining knitr::include_graphics(&quot;fig-3.png&quot;) Figure 16.2: Typical Text Pre-procesing References "],["16.2-mining-twitter-text-data-using-r.html", "16.2 Mining Twitter Text Data using R", " 16.2 Mining Twitter Text Data using R Twitter is one of the most popular social media platform for information sharing. Some examples from rforresearch tweets Follow (rforresearch?) Tweets by rforresearch 16.2.1 Obtaining Twitter Data Twitter provides an API access for their data feed. User is required to create an app and obtain access codes for the API. See here https://cran.r-project.org/web/packages/rtweet/vignettes/auth.html for a detailed introduction on obtaining API credentials. Data access is limited in various ways (days, size of data etc.). See here https://developer.twitter.com/en/docs.html for full documentation. R provides packages like twitteR and rtweet provide programming interface using R to access the data. Twitter doesnt allow data sharing but the API setup is straight forward. Twitter does allow sharing of the tweet IDs (available in the data folder) "],["16.3-download-data.html", "16.3 Download Data", " 16.3 Download Data We will download tweets with #auspol; a popular hashtag used in Australia to talk about current affairs and socio-political issues Example here uses rtweet package to download data After setting up the API Create the token search_tweets can be used without creating the token, it will create a token using the default app for your Twitter account library(rtweet) token &lt;- create_token(app = &quot;your_app_name&quot;, consumer_key = &quot;your_consumer_key&quot;, consumer_secret = &quot;your_consumer_secret&quot;, access_token = &quot;your_access_token&quot;, access_secret = &quot;your_access_secret&quot;) Use the search_tweets function to download the data, convert to data frame and save rt = search_tweets(q = &quot;#auspol&quot;, n = 15000, type = &quot;recent&quot;, include_rts = FALSE, since = &quot;2020-09-14&quot;) rt2 = as.data.frame(rt) saveRDS(rt2, file = &quot;tweets_auspol.rds&quot;) "],["16.4-data-pre-processing.html", "16.4 Data Pre-processing", " 16.4 Data Pre-processing We will conduct data pre-processing in this stage Some steps (depend on the types of problems analysed) Create a corpus Change encoding Convert to lower case Remove hashtags Remove URLs Remove @ mentions Remove punctuations Remove stop words Stemming can also be conducting (avoided in this example) library(tm) rt = readRDS(&quot;tweets_auspol.rds&quot;) # encoding rt$text &lt;- sapply(rt$text, function(row) iconv(row, &quot;latin1&quot;, &quot;ASCII&quot;, sub = &quot;&quot;)) # build a corpus, and specify the source to be character vectors myCorpus = Corpus(VectorSource(rt$text)) # convert to lower case myCorpus = tm_map(myCorpus, content_transformer(tolower)) # remove punctuation myCorpus &lt;- tm_map(myCorpus, removePunctuation) # remove numbers myCorpus &lt;- tm_map(myCorpus, removeNumbers) # remove URLs removeURL = function(x) gsub(&quot;http[^[:space:]]*&quot;, &quot;&quot;, x) removeURLs = function(x) gsub(&quot;https[^[:space:]]*&quot;, &quot;&quot;, x) # remove hashtags removehash = function(x) gsub(&quot;#\\\\S+&quot;, &quot;&quot;, x) # remove @ removeats &lt;- function(x) gsub(&quot;@\\\\w+&quot;, &quot;&quot;, x) #only removes the &#39;@&#39; # remove numbers and punctuations removeNumPunct = function(x) gsub(&quot;[^[:alpha:][:space:]]*&quot;, &quot;&quot;, x) # leading and trailing white spaces wspace1 = function(x) gsub(&quot;^[[:space:]]*&quot;, &quot;&quot;, x) ## Remove leading whitespaces wspace2 = function(x) gsub(&quot;[[:space:]]*$&quot;, &quot;&quot;, x) ## Remove trailing whitespaces wspace3 = function(x) gsub(&quot; +&quot;, &quot; &quot;, x) ## Remove extra whitespaces removeIms &lt;- function(x) gsub(&quot;im&quot;, &quot;&quot;, x) myCorpus = tm_map(myCorpus, content_transformer(removeURL)) #url myCorpus = tm_map(myCorpus, content_transformer(removeURLs)) #url myCorpus &lt;- tm_map(myCorpus, content_transformer(removehash)) #hashtag myCorpus &lt;- tm_map(myCorpus, content_transformer(removeats)) #mentions myCorpus = tm_map(myCorpus, content_transformer(removeNumPunct)) #number and punctuation (just in case some are left over) myCorpus = tm_map(myCorpus, content_transformer(removeIms)) #Ims myCorpus = tm_map(myCorpus, content_transformer(wspace1)) myCorpus = tm_map(myCorpus, content_transformer(wspace2)) myCorpus = tm_map(myCorpus, content_transformer(wspace3)) #other white spaces # remove extra whitespace myCorpus = tm_map(myCorpus, stripWhitespace) # remove extra stopwords myStopwords = c(stopwords(&quot;English&quot;), stopwords(&quot;SMART&quot;), &quot;rt&quot;, &quot;ht&quot;, &quot;via&quot;, &quot;amp&quot;, &quot;the&quot;, &quot;australia&quot;, &quot;australians&quot;, &quot;australian&quot;, &quot;auspol&quot;) myCorpus = tm_map(myCorpus, removeWords, myStopwords) # generally a good idea to save the processed corpus now save(myCorpus, file = &quot;auspol_sep.RData&quot;) data_tw2 = data.frame(text = get(&quot;content&quot;, myCorpus), row.names = NULL) data_tw2 = cbind(data_tw2, ID = rt$status_id) data_tw2 = cbind(Date = as.Date(rt$created_at), data_tw2) # look at the data frame, still some white spaces left so let&#39;s get rid of them data_tw2$text = gsub(&quot;\\r?\\n|\\r&quot;, &quot;&quot;, data_tw2$text) data_tw2$text = gsub(&quot; +&quot;, &quot; &quot;, data_tw2$text) head(data_tw2) # save data saveRDS(data_tw2, file = &quot;processed_data.rds&quot;) "],["16.5-some-visualisation.html", "16.5 Some Visualisation", " 16.5 Some Visualisation Bar Chart of top words library(tm) load(&quot;auspol_sep.RData&quot;) # Build TDM tdm = TermDocumentMatrix(myCorpus, control = list(wordLengths = c(3, Inf))) m = as.matrix(tdm) word.freq = sort(rowSums(m), decreasing = T) # plot term freq term.freq1 = rowSums(as.matrix(tdm)) term.freq = subset(term.freq1, term.freq1 &gt;= 50) df = data.frame(term = names(term.freq), freq = term.freq) df = transform(df, term = reorder(term, freq)) library(ggplot2) library(ggthemes) m2 = ggplot(head(df, n = 20), aes(x = reorder(term, -freq), y = freq)) + geom_bar(stat = &quot;identity&quot;, aes(fill = term)) + theme(legend.position = &quot;none&quot;) + ggtitle(&quot;Top 20 words in tweets #auspol \\n (14 Sep to 20 Sep 2020)&quot;) + theme(axis.text = element_text(size = 12, angle = 90, face = &quot;bold&quot;), axis.title.x = element_blank(), title = element_text(size = 15)) m2 = m2 + xlab(&quot;Words&quot;) + ylab(&quot;Frequency&quot;) + theme_wsj() + theme(legend.position = &quot;none&quot;, text = element_text(face = &quot;bold&quot;, size = 10)) m2 Figure 16.3: Top 20 Words Word Cloud 1 library(wordcloud) library(RColorBrewer) pal = brewer.pal(7, &quot;Dark2&quot;) wordcloud(words = names(word.freq), freq = word.freq, min.freq = 5, max.words = 1000, random.order = F, colors = pal) Figure 16.4: Wordcloud-1 Word Cloud 2 library(wordcloud2) # some data re-arrangement term.freq2 = data.frame(word = names(term.freq1), freq = term.freq1) term.freq2 = term.freq2[term.freq2$freq &gt; 5, ] # figure-3.4 wordcloud2(term.freq2) Figure 16.5: Wordcloud-2 The word cloud reflects the discussion around the Federal Governments new Energy Policy announced during the data time period "],["16.6-sentiment-analysis.html", "16.6 Sentiment Analysis", " 16.6 Sentiment Analysis Classification of Sentiment Analysis Methods 16.6 shows various classification of sentiment analysis methods (Collomb et al. (2014)) Figure 16.6: Classification of Sentiment Analysis Methods 16.6.1 Method Lexicon (or Dictionary) based method used in the following illustration. Sentence level classification Eight emotions to classify: anger, fear, anticipation, trust, surprise, sadness, joy, and disgust Lexicon Used: NRC Emotion Lexicon. See http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm See Mohammad and Turney (2010) for more details on the lexicon. # required libraries sentiment analysis library(syuzhet) library(lubridate) library(ggplot2) library(scales) library(reshape2) library(dplyr) library(qdap) # convert data to dataframe for analysis data_sent = readRDS(&quot;processed_data.rds&quot;) data_sent = data_sent[!apply(data_sent, 1, function(x) any(x == &quot;&quot;)), ] #remove rows with empty values data_sent = data_sent[wc(data_sent$text) &gt; 4, ] #more than 4 words per tweet tw2 = data_sent$text Conduct Sentiment Analysis and Visualise mySentiment = get_nrc_sentiment(tw2) #this can take some time tweets_sentiment = cbind(data_sent, mySentiment) # save the results saveRDS(tweets_sentiment, file = &quot;tw_sentiment.rds&quot;) Plot sentimentTotals = data.frame(colSums(tweets_sentiment[, c(4:13)])) names(sentimentTotals) = &quot;count&quot; sentimentTotals = cbind(sentiment = rownames(sentimentTotals), sentimentTotals) rownames(sentimentTotals) = NULL # plot ggplot(data = sentimentTotals, aes(x = sentiment, y = count)) + geom_bar(aes(fill = sentiment), stat = &quot;identity&quot;) + theme(legend.position = &quot;none&quot;) + xlab(&quot;Sentiment&quot;) + ylab(&quot;Total Count&quot;) + ggtitle(&quot;Sentiment Score for Sample Tweets&quot;) + theme_minimal() + theme(axis.text = element_text(size = 15, face = &quot;bold&quot;)) Figure 16.7: Emotions/Sentiment Scores References "],["16.7-topic-modelling.html", "16.7 Topic Modelling", " 16.7 Topic Modelling This section will use bi-term topic modelling method to demonstrate topic modelling exercise. Biterm Topic Modelling (BTM) (Yan et al. (2013)) is useful for short text like the twitter data we have in this example. A BTM is a word co-occurance based topic model that learns topics by modelling word-word patterns (biterms) BTM models biterm occurences in a corpus More details here https://github.com/xiaohuiyan/xiaohuiyan.github.io/blob/master/paper/BTM-WWW13.pdf A good example here http://www.bnosac.be/index.php/blog/98-biterm-topic-modelling-for-short-texts # load packages and rearrange data library(udpipe) library(data.table) library(stopwords) library(BTM) library(textplot) library(ggraph) # rearrange to get doc id data_tm = data_sent[, c(3, 2)] colnames(data_tm)[1] = &quot;doc_id&quot; # use parts of sentence (Nouns, Adjectives, Verbs for TM) Method is # computationally intensive and can take several minutes. anno &lt;- udpipe(data_tm, &quot;english&quot;, trace = 1000) biterms &lt;- as.data.table(anno) biterms &lt;- biterms[, cooccurrence(x = lemma, relevant = upos %in% c(&quot;NOUN&quot;, &quot;ADJ&quot;, &quot;VERB&quot;) &amp; nchar(lemma) &gt; 2 &amp; !lemma %in% stopwords(&quot;en&quot;), skipgram = 3), by = list(doc_id)] set.seed(999) traindata &lt;- subset(anno, upos %in% c(&quot;NOUN&quot;, &quot;ADJ&quot;, &quot;VERB&quot;) &amp; !lemma %in% stopwords(&quot;en&quot;) &amp; nchar(lemma) &gt; 2) traindata &lt;- traindata[, c(&quot;doc_id&quot;, &quot;lemma&quot;)] # fit 10 topics (other parameters are mostly default) model &lt;- BTM(traindata, biterms = biterms, k = 10, iter = 2000, background = FALSE, trace = 2000) # extract biterms for plotting biterms1 = terms(model, type = &quot;biterms&quot;)$biterms # The model, biterms, biterms1 were saved to create the plot in this markdown # document. Plot the topics with 20 terms and labelled by the proportion plot(model, subtitle = &quot;#auspol 14-20 Sep 2020&quot;, biterms = biterms1, labels = paste(round(model$theta * 100, 2), &quot;%&quot;, sep = &quot;&quot;), top_n = 20) Figure 16.8: BTM Visualisation of #auspol Other analysis which can be conducted may include, clustering analysis, co-word clusters, network analysis etc. Other Topic Modelling methods can also be implemented. References "],["17-work-in-progress.html", "Topic 17 Work in Progress", " Topic 17 Work in Progress We have finished a quick intro to R and some use cases in Data Analytics This is a work in progress and may include more examples later Follow rforresearch "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
